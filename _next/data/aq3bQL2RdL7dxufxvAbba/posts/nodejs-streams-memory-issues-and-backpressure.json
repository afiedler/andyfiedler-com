{"pageProps":{"postData":{"id":"nodejs-streams-memory-issues-and-backpressure","contentHtml":"<p>I was working on an ETL process with NodeJS that ran on Heroku, which limits you to 500MB of\nmemory on a standard dyno. The ETL was quickly running out of memory, even through each\ndocument that it processed was less than 1MB.</p>\n<p>This ETL was made up of a <code>Readable</code> stream from a MongoDB <code>Cursor</code>, then two <code>Transform</code>\nstreams, one using <a href=\"https://github.com/dominictarr/event-stream\"><code>event-stream</code></a> and one\nimplemented as a subclass of <code>Stream.Transform</code>. The last stream was a <code>Writable</code> stream\nthat validated and wrote back documents to MongoDB.</p>\n<p>Clearly the readable stream was reading documents out of Mongo way too quickly for the\ntransform streams to do their work and the writable stream to write the documents back to\nMongoDB. But wasn't NodeJS suppose to manage this automatically, assuming the <code>highWaterMark</code>\nwas set to something reasonable?</p>\n<p>It turns our that, yes, NodeJS does manage this correctly, as long as <em>every stream in your pipe\nis a 0.10.x stream or greater</em>. NodeJS implemented the\n<a href=\"https://nodejs.org/en/blog/feature/streams2/\"><code>highWaterMark</code> in Node 0.10.x</a>.</p>\n<p>The culprit in my case was <code>event-stream</code>, which is an old library that only makes 0.8.x streams.</p>\n<p>Moral of the story: if you are having memory issues with streams that seem like backpressure\nshould be solving, make sure all of the streams in your pipe are 0.10.x streams or greater!</p>\n","layout":"post","title":"Memory Issues with NodeJS Streams? Remove 0.8.x Streams from Your Pipe"}},"__N_SSG":true}
{"pageProps":{"postData":{"id":"the-ultimate-guide-to-dealing-with-high-frequency-data","contentHtml":"<p>I have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!</p>\n<p>I'm currently working on a project to replace the Time Series Database (TSDB) that I wrote a few years ago. By re-writing it, I'm learning a lot about what works and what doesn't when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I've developed while working on this project.</p>\n<p>TSDB attempts to fix this problem by keeping rows in order and creating a sparse index (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I'm not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.</p>\n<p>Another approach is to partition your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union queries across tables.</p>\n<p>Partitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I'm working on will use PyTables to partition time series data by date.</p>\n<p>UTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.</p>\n<p>The most basic way to make the load step revertible is to just make a copy of your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!</p>\n","layout":"post","status":"publish","published":true,"title":"The Ultimate Guide to Dealing with High Frequency Data","author":{"display_name":"afiedler","login":"afiedler","email":"andy@andyfiedler.com","url":""},"author_login":"afiedler","author_email":"andy@andyfiedler.com","wordpress_id":267,"wordpress_url":"http://andyfiedler.com/?p=267","date":"2014-05-07 09:10:24 -0400","date_gmt":"2014-05-07 13:10:24 -0400","categories":["Tech Notes"],"tags":["time series","big data"]}},"__N_SSG":true}
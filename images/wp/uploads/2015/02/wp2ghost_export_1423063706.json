{"data":{"posts":[{"id":5,"title":"Using the Google Nexus S in China","slug":"using-google-nexus-s-china","markdown":"\nI just spent two weeks in China visiting Beijing, Xi\u2019an, and Shanghai. It was a great trip, but not speaking any Chinese made it rather difficult to get around at times. English is not widely spoken, and even in Shanghai, finding English speakers was difficult. Also, even writing things down was not effective because most Chinese only know the spelling of tourist sites in Chinese characters. I knew this before I left, and figured having my cell phone working in China would be very useful for translation and maps. This proved to be true, and there were a few times where it was really invaluable (for example, showing the Chinese taxi driver my destination on Google Maps, which has English and Chinese names, was often the only way to convey where I wanted to go).\n\nHere\u2019s how I got my phone, the Google Nexus S, to work in China. This should work with any other unlocked T-Mobile phone, too. First, there are two major cell phone carriers in China: China Mobile, and China Unicom. You\u2019ll want to use China Unicom because they have 3G on the 2100 MHz band, which is the same as T-Mobile uses in the US. You can buy a SIM card from many convenience stores in China, or you can order one from [3GSolutions](http:\/\/www.3gsolutions.com.cn\/page\/simcard\/). I got package 3S96 from them, which is a SIM card with CNY 50 of credit on it. If you order it from them, they will mail it to your hotel for you to pick up upon arrival. This was helpful, but China Unicom stores are ubiquitous in big cities and many convenience stores had SIM cards too.\n\nAfter you get your SIM card, you\u2019ll want to activate a monthly plan on it. If you don\u2019t use a plan, 3G data charges are astronomical. China Unicom only bills by calendar-month, which is a pain if you are arriving at the end a month like I was. However, you can also buy a half-month plan with half the data and call time allowance for half the price for your first calendar month there. 3GSolutions has a good list of the available plans [here](http:\/\/www.3gsolutions.com.cn\/page\/mthplan\/). If you get your SIM card from 3GSolutions, they will activate a plan for you, or a least they say they will. Unfortunately, for some reason my monthly plan was not activated when I got my SIM card, and I quickly blew through the credits on it using Google Maps.\n\nTo activate a monthly plan yourself, just call 10010. If you are not in the city where you bought the SIM card when you want to activate a plan, you will need to dial the city code first. For example, my SIM card was purchased in Shanghai, which is city code 021. So, to activate a plan while I was in Beijing, I called 021-10010. There are operators who speak English well, at least on the Shanghai hotline.\n\nSo, how much does this all cost? If you go at the end of a month (I arrived on July 24 and was staying until August 4), it will be more expensive since you will need to pay for two months of service. That being said, overall it is not too expensive: CNY 100 for the SIM card with CNY 50 in credits. Then I added CNY 50 by buying a recharge card. I got the CNY 66 monthly plan with 300 MB of data and 50 minutes of calling. For my first week there (in July), I took advantage of the half-price first month deal from China Unicom (CNY 33 for 150 MB and 25 minutes of calling). Then for the few days in August I was there, I was charged CNY 66 for that month. In total, it would have come to CNY 150, and I\u2019d be left with a SIM card with CNY 1 on it when I left.\n\n<div>That\u2019s about $23.30, which given the number of times I needed Google Maps\/Translator, was definitely worth it. I ended up actually paying a bit more since 3GSolutions charges a\u00a0commission\u00a0to get the SIM card and send it to you, and also because somehow my plan wasn\u2019t activated at first and I quickly blew through the included credits.\n\n\n# Getting around the Great Firewall\n\nChina blocks a lot of websites that websites that we take for granted, and if you really need to access Facebook and Twitter from China, you\u2019ll need a VPN. The Nexus S has a built-in VPN client, but it was very flaky for me. If you loose a network connection for any reason (going out or range for wifi or loosing cell service briefly), you\u2019ll loose your connection to the VPN. Also, I\u2019ve heard reports that PPTP VPNs have been blocked in China, but I couldn\u2019t confirm this.\n\nI experimented with [PureVPN](http:\/\/billing.purevpn.com\/aff.php?aff=453), which uses IPSec, over China Unicom and it actually worked well. It did not work at all over any wifi hotspot that I tested in China.\n\n<\/div>\n","html":"<p>I just spent two weeks in China visiting Beijing, Xi\u2019an, and Shanghai. It was a great trip, but not speaking any Chinese made it rather difficult to get around at times. English is not widely spoken, and even in Shanghai, finding English speakers was difficult. Also, even writing things down was not effective because most Chinese only know the spelling of tourist sites in Chinese characters. I knew this before I left, and figured having my cell phone working in China would be very useful for translation and maps. This proved to be true, and there were a few times where it was really invaluable (for example, showing the Chinese taxi driver my destination on Google Maps, which has English and Chinese names, was often the only way to convey where I wanted to go).<\/p>\n<p>Here\u2019s how I got my phone, the Google Nexus S, to work in China. This should work with any other unlocked T-Mobile phone, too. First, there are two major cell phone carriers in China: China Mobile, and China Unicom. You\u2019ll want to use China Unicom because they have 3G on the 2100 MHz band, which is the same as T-Mobile uses in the US. You can buy a SIM card from many convenience stores in China, or you can order one from <a href=\"http:\/\/www.3gsolutions.com.cn\/page\/simcard\/\">3GSolutions<\/a>. I got package 3S96 from them, which is a SIM card with CNY 50 of credit on it. If you order it from them, they will mail it to your hotel for you to pick up upon arrival. This was helpful, but China Unicom stores are ubiquitous in big cities and many convenience stores had SIM cards too.<\/p>\n<p>After you get your SIM card, you\u2019ll want to activate a monthly plan on it. If you don\u2019t use a plan, 3G data charges are astronomical. China Unicom only bills by calendar-month, which is a pain if you are arriving at the end a month like I was. However, you can also buy a half-month plan with half the data and call time allowance for half the price for your first calendar month there. 3GSolutions has a good list of the available plans <a href=\"http:\/\/www.3gsolutions.com.cn\/page\/mthplan\/\">here<\/a>. If you get your SIM card from 3GSolutions, they will activate a plan for you, or a least they say they will. Unfortunately, for some reason my monthly plan was not activated when I got my SIM card, and I quickly blew through the credits on it using Google Maps.<\/p>\n<p>To activate a monthly plan yourself, just call 10010. If you are not in the city where you bought the SIM card when you want to activate a plan, you will need to dial the city code first. For example, my SIM card was purchased in Shanghai, which is city code 021. So, to activate a plan while I was in Beijing, I called 021-10010. There are operators who speak English well, at least on the Shanghai hotline.<\/p>\n<p>So, how much does this all cost? If you go at the end of a month (I arrived on July 24 and was staying until August 4), it will be more expensive since you will need to pay for two months of service. That being said, overall it is not too expensive: CNY 100 for the SIM card with CNY 50 in credits. Then I added CNY 50 by buying a recharge card. I got the CNY 66 monthly plan with 300 MB of data and 50 minutes of calling. For my first week there (in July), I took advantage of the half-price first month deal from China Unicom (CNY 33 for 150 MB and 25 minutes of calling). Then for the few days in August I was there, I was charged CNY 66 for that month. In total, it would have come to CNY 150, and I\u2019d be left with a SIM card with CNY 1 on it when I left.<\/p>\n<div>\n<p>That\u2019s about $23.30, which given the number of times I needed Google Maps\/Translator, was definitely worth it. I ended up actually paying a bit more since 3GSolutions charges a\u00a0commission\u00a0to get the SIM card and send it to you, and also because somehow my plan wasn\u2019t activated at first and I quickly blew through the included credits.<\/p>\n<h1>Getting around the Great Firewall<\/h1>\n<p>China blocks a lot of websites that websites that we take for granted, and if you really need to access Facebook and Twitter from China, you\u2019ll need a VPN. The Nexus S has a built-in VPN client, but it was very flaky for me. If you loose a network connection for any reason (going out or range for wifi or loosing cell service briefly), you\u2019ll loose your connection to the VPN. Also, I\u2019ve heard reports that PPTP VPNs have been blocked in China, but I couldn\u2019t confirm this.<\/p>\n<p>I experimented with <a href=\"http:\/\/billing.purevpn.com\/aff.php?aff=453\">PureVPN<\/a>, which uses IPSec, over China Unicom and it actually worked well. It did not work at all over any wifi hotspot that I tested in China.<\/p>\n<\/div>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 07 Sep 2011 14:57:48 +0000","created_by":1,"updated_at":"Wed, 08 Aug 2012 18:58:28 +0000","updated_by":1,"published_at":"Wed, 07 Sep 2011 14:57:48 +0000","published_by":1},{"id":12,"title":"Heading to Buenos Aires for Februray","slug":"heading-to-buenos-aires-for-februray","markdown":"\n<div class=\"wp-caption alignnone\" id=\"attachment_13\" style=\"width: 610px\">[![Poster at a bus stop in Buenos Aires of tango dancers](http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/01\/2943121826_980b26da61_o.jpg \"2943121826_980b26da61_o\")](http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/01\/2943121826_980b26da61_o.jpg)Credit: oijulia\n\n<\/div>The next four weeks are going to be a whirlwind of travelling: Miami for the Miama Half Marathon, then on to Buenos Aires for most of February.\n\nI\u2019ve wanted to go to Argentina for a while now, but kept putting off the trip for a variety of reasons: no time, no money, the usual. I though about going a few months ago, but at the time decided to put it off, yet again, for the same reasons. Then, this winter, with a fairly flexible work schedule, a friend who can sublet my apartment in DC, and dirt cheap flights from Miami, I realized that now was the time to go!\n\nI\u2019ll be staying in Buenos Aires for most of the time, working during the day and taking Spanish classes at night. I\u2019m planning a few weekend trips to Uruguay (Montevideo) and possibly Mendoza for some hiking. Stay tuned for photos from the trip.\n\n\n","html":"<div id=\"attachment_13\" style=\"width: 610px\" class=\"wp-caption alignnone\"><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/01\/2943121826_980b26da61_o.jpg\"><img class=\"size-full wp-image-13\" title=\"2943121826_980b26da61_o\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/01\/2943121826_980b26da61_o.jpg\" alt=\"Poster at a bus stop in Buenos Aires of tango dancers\" width=\"600\" height=\"800\" \/><\/a><p class=\"wp-caption-text\">Credit: oijulia<\/p><\/div>\n<p>The next four weeks are going to be a whirlwind of travelling: Miami for the Miama Half Marathon, then on to Buenos Aires for most of February.<\/p>\n<p>I\u2019ve wanted to go to Argentina for a while now, but kept putting off the trip for a variety of reasons: no time, no money, the usual. I though about going a few months ago, but at the time decided to put it off, yet again, for the same reasons. Then, this winter, with a fairly flexible work schedule, a friend who can sublet my apartment in DC, and dirt cheap flights from Miami, I realized that now was the time to go!<\/p>\n<p>I\u2019ll be staying in Buenos Aires for most of the time, working during the day and taking Spanish classes at night. I\u2019m planning a few weekend trips to Uruguay (Montevideo) and possibly Mendoza for some hiking. Stay tuned for photos from the trip.<\/p>\n","image":"http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/01\/2943121826_980b26da61_o.jpg","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 23 Jan 2012 14:51:26 +0000","created_by":1,"updated_at":"Mon, 26 Mar 2012 13:43:52 +0000","updated_by":1,"published_at":"Mon, 23 Jan 2012 14:51:26 +0000","published_by":1},{"id":20,"title":"Takeaways from Lean Startup Machine DC","slug":"takeaways-from-lean-startup-machine-dc","markdown":"\nLast weekend, I went to [Lean Startup Machine DC](http:\/\/leanstartupmachine.com\/ \"Lean Startup Machine\"), which is a weekend-long workshop on applying the principles of Eric Ries\u2019 book [The Lean Startup](http:\/\/www.amazon.com\/The-Lean-Startup-Entrepreneurs-Continuous\/dp\/0307887898). The weekend is incredibly intensive, and we made a lot of progress validating and invalidating some of the assumptions we had for our local peer-to-peer lending platform, [Clovest](clovest.com).\n\nFor me the biggest take-away is the importance of talking to your customers early and often. Instead of writing code and building the website, we focused completely on customer interviews, market research, and developing a trial ad campaign to see how interested the community is in this project.\n\nFar too often, technical people (myself included) would rather spend a few hours building something in Rails, or whatever is the current coolest technology, instead of talking with end users. Building software has instant gratification, it feels productive, and the risk is minimal (besides wasting time and getting carpal tunnel syndrome). Talking to people is difficult and uncomfortable, and forces you to consider the possibility that they will reject your idea. However, wouldn\u2019t you rather have your idea rejected while its just a sketch on a napkin and not 20,000 lines of code that took 200 man-hours to write? I\u2019d definitely prefer the former, and that\u2019s probably the biggest takeaway from the whole lean startup methodology.\n\n\n","html":"<p>Last weekend, I went to <a title=\"Lean Startup Machine\" href=\"http:\/\/leanstartupmachine.com\/\">Lean Startup Machine DC<\/a>, which is a weekend-long workshop on applying the principles of Eric Ries&#8217; book <a href=\"http:\/\/www.amazon.com\/The-Lean-Startup-Entrepreneurs-Continuous\/dp\/0307887898\">The Lean Startup<\/a>. The weekend is incredibly intensive, and we made a lot of progress validating and invalidating some of the assumptions we had for our local peer-to-peer lending platform, <a href=\"clovest.com\">Clovest<\/a>.<\/p>\n<p>For me the biggest take-away is the importance of talking to your customers early and often. Instead of writing code and building the website, we focused completely on customer interviews, market research, and developing a trial ad campaign to see how interested the community is in this project.<\/p>\n<p>Far too often, technical people (myself included) would rather spend a few hours building something in Rails, or whatever is the current coolest technology, instead of talking with end users. Building software has instant gratification, it feels productive, and the risk is minimal (besides wasting time and getting carpal tunnel syndrome). Talking to people is difficult and uncomfortable, and forces you to consider the possibility that they will reject your idea. However, wouldn&#8217;t you rather have your idea rejected while its just a sketch on a napkin and not 20,000 lines of code that took 200 man-hours to write? I&#8217;d definitely prefer the former, and that&#8217;s probably the biggest takeaway from the whole lean startup methodology.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 26 Mar 2012 13:40:27 +0000","created_by":1,"updated_at":"Mon, 26 Mar 2012 13:41:02 +0000","updated_by":1,"published_at":"Mon, 26 Mar 2012 13:40:27 +0000","published_by":1},{"id":27,"title":"About","slug":"about","markdown":"\nMy name is Andy Fiedler, and that\u2019s how I cleverly got the name for this site. I\u2019m currently a freelancer based in Washington, DC.\n\nI\u2019m interested in all sorts of ways to use technology to solve problems in business, local and international economic development. I graduated Rutgers University in 2009 with a B.S., double majoring in Biomedical Engineering and Economics. After that, I worked at the Federal Reserve Board in the International Finance research department. There, I developed a variety of financial models in sovereign debt, foreign exchange, and commodities futures. I also wrote a high-frequency timeseries database system, which is in the process of being open sourced here.\n\nOutside of technology, I\u2019m an avid runner, hiker, and attempt to do triathlons without drowning.\n\n\n","html":"<p>My name is Andy Fiedler, and that\u2019s how I cleverly got the name for this site. I\u2019m currently a freelancer based in Washington, DC.<\/p>\n<p>I\u2019m interested in all sorts of ways to use technology to solve problems in business, local and international economic development. I graduated Rutgers University in 2009 with a B.S., double majoring in Biomedical Engineering and Economics. After that, I worked at the Federal Reserve Board in the International Finance research department. There, I developed a variety of financial models in sovereign debt, foreign exchange, and commodities futures. I also wrote a high-frequency timeseries database system, which is in the process of being open sourced here.<\/p>\n<p>Outside of technology, I\u2019m an avid runner, hiker, and attempt to do triathlons without drowning.<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 26 Mar 2012 13:56:12 +0000","created_by":1,"updated_at":"Sun, 08 Jul 2012 21:13:04 +0000","updated_by":1,"published_at":"Mon, 26 Mar 2012 13:56:12 +0000","published_by":1},{"id":35,"title":"Contact","slug":"contact","markdown":"\n[contact-form-7 id=\u201d29\u2033 title=\u201dContact\u201d]\n\n\n","html":"<p>[contact-form-7 id=&#8221;29&#8243; title=&#8221;Contact&#8221;]<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 26 Mar 2012 14:01:08 +0000","created_by":1,"updated_at":"Thu, 05 Jul 2012 21:43:59 +0000","updated_by":1,"published_at":"Mon, 26 Mar 2012 14:01:08 +0000","published_by":1},{"id":40,"title":"Using Gitolite with Redmine","slug":"using-gitolite-with-redmine","markdown":"\nI use Gitolite to self-host multiple git projects and Redmine for time-tracking and project management. Redmine has a cool feature where it can integrate with source control management systems, but getting it to work with Gitolite was a bit of a challenge. Here\u2019s how I did it.\n\nFirst, create a new repository in Gitolite. You\u2019re going to want to mirror a read-only version somewhere for Redmine to access. Then, we\u2019ll keep them synced using a post-receive hook in Git. After you create your new Gitolite repo, login to your server and go to a folder where you\u2019ll keep a mirror for Redmine. In this example, I\u2019ll use `\/var\/gitolite-mirrors\/`. In that folder, type this to create a mirror of your Gitolite repo and, critically, make git change the permissions of new files to 0644 (which gives read access to all users). Note that # means run as root.\n\n# git clone --mirror \/home\/git\/repositories\/my-new-repo.git # cd my-new-repo.git # git config --add core.sharedRepository 0644\n\nNow, change the permissions so that the git user (the user that Gitolite runs as) owns this folder.\n\n# chown -R git:git ..\/my-new-repo.git\n\nFinally, we\u2019ll add a post-receive hook to the Gitolite repo. Go to `\/home\/git\/repositories\/my-new-repo.git\/hooks`. Add a new file called `post-receive`, and enter this into the file:\n\n#!\/bin\/sh \/usr\/bin\/git push --mirror \/var\/gitolite-mirrors\/my-new-repo.git\n\nThen, you\u2019ll want to change the ownership and permissions on that.\n\n# chown git:git post-receive # chmod 700 post-receive\n\nIn Redmine, go to your project you want to associate with this repo. Click the Settings, then Repository tabs. Choose Git as your SCM. In this example, you\u2019d enter `\/var\/gitolite-mirrots\/my-new-repo.git` as your repository path. Click \u201cSave\u201d.\n\nNow, click the Repository tab on the top tab bar. You may get a 404 error at this point. That is because you need to push a master branch to this repository (it is empty right now). Push a few files to the repo and check again.\n\nIf it is still not working, you can log in to your server and navigate to the mirror location (`\/var\/gitolite-mirrors\/my-new-repo.git`). Switch users to the user your Redmine process is running as (probably www-data or something similar). Redmine is running `git log master` to get the changes from your repo to display on the webpage. Try running that command and see if you get any errors, and debug from there.\n\n\n","html":"<p>I use Gitolite to self-host multiple git projects and Redmine for time-tracking and project management. Redmine has a cool feature where it can integrate with source control management systems, but getting it to work with Gitolite was a bit of a challenge. Here&#8217;s how I did it.<\/p>\n<p>First, create a new repository in Gitolite. You&#8217;re going to want to mirror a read-only version somewhere for Redmine to access. Then, we&#8217;ll keep them synced using a post-receive hook in Git. After you create your new Gitolite repo, login to your server and go to a folder where you&#8217;ll keep a mirror for Redmine. In this example, I&#8217;ll use <code>\/var\/gitolite-mirrors\/<\/code>. In that folder, type this to create a mirror of your Gitolite repo and, critically, make git change the permissions of new files to 0644 (which gives read access to all users). Note that # means run as root.<\/p>\n<pre class=\"lang:default highlight:0 decode:true\"># git clone --mirror \/home\/git\/repositories\/my-new-repo.git\r\n# cd my-new-repo.git\r\n# git config --add core.sharedRepository 0644<\/pre>\n<p>Now, change the permissions so that the git user (the user that Gitolite runs as) owns this folder.<\/p>\n<pre class=\"lang:default highlight:0 decode:true\"># chown -R git:git ..\/my-new-repo.git<\/pre>\n<p>Finally, we&#8217;ll add a post-receive hook to the Gitolite repo. Go to <code>\/home\/git\/repositories\/my-new-repo.git\/hooks<\/code>. Add a new file called <code>post-receive<\/code>, and enter this into the file:<\/p>\n<pre class=\"lang:default highlight:0 decode:true\">#!\/bin\/sh\r\n\/usr\/bin\/git push --mirror \/var\/gitolite-mirrors\/my-new-repo.git<\/pre>\n<p>Then, you&#8217;ll want to change the ownership and permissions on that.<\/p>\n<pre class=\"lang:default highlight:0 decode:true crayon-selected\"># chown git:git post-receive\r\n# chmod 700 post-receive<\/pre>\n<p>In Redmine, go to your project you want to associate with this repo. Click the Settings, then Repository tabs. Choose Git as your SCM. In this example, you&#8217;d enter <code>\/var\/gitolite-mirrots\/my-new-repo.git<\/code> as your repository path. Click &#8220;Save&#8221;.<\/p>\n<p>Now, click the Repository tab on the top tab bar. You may get a 404 error at this point. That is because you need to push a master branch to this repository (it is empty right now). Push a few files to the repo and check again.<\/p>\n<p>If it is still not working, you can log in to your server and navigate to the mirror location (<code>\/var\/gitolite-mirrors\/my-new-repo.git<\/code>). Switch users to the user your Redmine process is running as (probably www-data or something similar). Redmine is running <code>git log master<\/code> to get the changes from your repo to display on the webpage. Try running that command and see if you get any errors, and debug from there.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 14 May 2012 11:09:38 +0000","created_by":1,"updated_at":"Thu, 23 Aug 2012 17:36:15 +0000","updated_by":1,"published_at":"Mon, 14 May 2012 11:09:38 +0000","published_by":1},{"id":50,"title":"Projects","slug":"projects","markdown":"\n\n## [TsTables](http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/ \"TsTables \u2013 Store High Frequency Data with PyTables\")\n\nPython library that extends PyTables for storing high frequency data. Replacement for my original Time Series Database.\n\n\n## [Time Series Database](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\")\n\nA flat-file database built on [HDF5](http:\/\/www.hdfgroup.org\/HDF5\/ \"HDFGroup - HDF5\")\u00a0for storing huge amounts of time series data. I created this as a side project to help organize terabytes of high-frequency foreign exchange data while I worked at the Federal Reserve. It is a C++ library with MATLAB and R bindings, a collection of console programs, and a basic desktop application for browsing data. You could use it to store any very large dataset of high-frequency time series data.\n\n\n## [HealthyGeek.me](http:\/\/andyfiedler.com\/projects\/healthygeek-me\/ \"HealthyGeek.me\")\n\nA collection of web apps for health and fitness. Currently there\u2019s a [nutrition calculator](http:\/\/healthygeek.me\/menus\/new) written in Backbone.js and Rails.\n\n\n","html":"<h2><a title=\"TsTables \u2013 Store High Frequency Data with PyTables\" href=\"http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/\">TsTables<\/a><\/h2>\n<p>Python library that extends PyTables for storing high frequency data. Replacement for my original Time Series Database.<\/p>\n<h2><a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">Time Series Database<\/a><\/h2>\n<p>A flat-file database built on <a title=\"HDFGroup - HDF5\" href=\"http:\/\/www.hdfgroup.org\/HDF5\/\">HDF5<\/a>\u00a0for storing huge amounts of time series data. I created this as a side project to help organize terabytes of high-frequency foreign exchange data while I worked at the Federal Reserve. It is a C++ library with MATLAB and R bindings, a collection of console programs, and a basic desktop application for browsing data. You could use it to store any very large dataset of high-frequency time series data.<\/p>\n<h2><a title=\"HealthyGeek.me\" href=\"http:\/\/andyfiedler.com\/projects\/healthygeek-me\/\">HealthyGeek.me<\/a><\/h2>\n<p>A collection of web apps for health and fitness. Currently there&#8217;s a <a href=\"http:\/\/healthygeek.me\/menus\/new\">nutrition calculator<\/a> written in Backbone.js and Rails.<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 23 Jun 2012 11:56:50 +0000","created_by":1,"updated_at":"Mon, 30 Jun 2014 10:37:47 +0000","updated_by":1,"published_at":"Sat, 23 Jun 2012 11:56:50 +0000","published_by":1},{"id":53,"title":"Time Series Database","slug":"time-series-database","markdown":"\n\n## Deprecated\n\nThis project isn\u2019t being maintained any more. Check out my replacement, [TsTables](http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/ \"TsTables \u2013 Store High Frequency Data with PyTables\")!\n\n\n## Overview\n\nThis project was started as a way to organize massive amounts of time series data. We had terabytes of high frequency foreign exchange data and wanted to run models on it in common statistical programming languages like R and MATLAB. However, with that much data, there is no way we could fit it into memory on a normal desktop PC.\n\nBelieve it or not, there is no simple, affordable solution to this problem out there. We tried stuffing it into MySQL (epic fail), SAS (couldn\u2019t figure out a good way to index it, plus we wanted to use MATLAB and R for analysis), and flat text files (sort of worked, but was clumsy and space-inefficient).\n\nWe did, however, find the excellent [HDF5 library](http:\/\/hdfgroup.org), which is used by many scientists to store large matrixes. HDF5 seemed like a good solution, but it has no time series indexing. Time Series Database (TSDB), is a wrapper on HDF5 to provide a nice interface for storing time series data as well as a basic sparse B-Tree index on the timestamp. TSDB takes advantage of the fact that most time series data is already sorted and thus you can save a ton of space by inserting an index point every few thousand records and then sequentially searching to find your desired record.\n\n\n## Features\n\n- Fast subsetting. Able to extract a million rows into MATLAB in a few seconds\n- Compact file size. It uses a sparse index, so no more multi-gigabyte SAS or relational database indexes. ZLib compression is also enabled by default for further space\u00a0savings.\n- Millisecond precision\n- Multiple records with the same timestamp are okay\n- Gaps in data are okay. TSDB does not force you to store your data on an equally-spaced grid, like time series databases like [FAME](http:\/\/www.sungard.com\/fame).\n- A bunch of datatypes - 32-bit integer\n- 8-bit integer\n- 64-bit double\n- 1-byte character\n- 64-bit timestamp (millisecond precision)\n- string (user-specified length)\n\n\n## Limitations\n\n- Time series are **append only**. It\u2019s possible with HDF5 to edit or insert data in the middle of table, but we haven\u2019t written the code to update the indexes in that case. I probably won\u2019t add that feature because this is designed for batch appends of CSV data.\n- When appending, you\u00a0**must** start appending with a row that has a timestamp equal to or after the last timestamp of the existing table. The library will refuse your append if that\u2019s not the case.\n- Any data you append must be sorted from earliest timestamp to latest.\n- You cannot add or remove a column after a time series is created.\n- You cannot delete a time series from a file (you can obviously delete a file, though).\n- You cannot have variable length rows. So, if you include a large string field then that number of byte is allocated for each row no matter the length of the string stored in that row. Compression helps with this problem, though, and changing it would slow down the library.\n- You can\u2019t change the precision of the timestamps (millisecond only).\n- You can\u2019t store timezone info with the timestamp. The library makes no time zone assumptions, though.\n- It does **not** support transactions, so if your program crashes while you are writing to a time series, you could corrupt the file. Keep backups!\n- Its not under active development. But, if there\u2019s a glaring bug, I\u2019ll try to fix it.\n\n\n## Intended usage\n\nTSDB is beta software, at best. You should\u00a0**not** use it to store your only copy of some critical data. Its best use it to store a copy of your raw data in a format that makes it very easy to extract subsets. The idea is to write your time series model in a way that you pull subsets of your data into memory, do some processing, and then write the result to disc.\n\n\n## Get the source code\n\nThe source code is hosted on Github. I\u2019m in the process of cleaning up this library, and right now only the C++ library is ready to use. I\u2019ve only tested the build instructions on a Mac (although Linux should be very similar).\n\n- [Get a copy of the source here](https:\/\/github.com\/afiedler\/tsdb).\n- [Build instructions for Mac OS X are here](https:\/\/github.com\/afiedler\/tsdb\/wiki\/Build-Instructions-(Mac-OSX)).\n\n\n## \n\n\n","html":"<h2>Deprecated<\/h2>\n<p>This project isn&#8217;t being maintained any more. Check out my replacement, <a title=\"TsTables \u2013 Store High Frequency Data with PyTables\" href=\"http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/\">TsTables<\/a>!<\/p>\n<h2>Overview<\/h2>\n<p>This project was started as a way to organize massive amounts of time series data. We had terabytes of high frequency foreign exchange data and wanted to run models on it in common statistical programming languages like R and MATLAB. However, with that much data, there is no way we could fit it into memory on a normal desktop PC.<\/p>\n<p>Believe it or not, there is no simple, affordable solution to this problem out there. We tried stuffing it into MySQL (epic fail), SAS (couldn&#8217;t figure out a good way to index it, plus we wanted to use MATLAB and R for analysis), and flat text files (sort of worked, but was clumsy and space-inefficient).<\/p>\n<p>We did, however, find the excellent <a href=\"http:\/\/hdfgroup.org\">HDF5 library<\/a>, which is used by many scientists to store large matrixes. HDF5 seemed like a good solution, but it has no time series indexing. Time Series Database (TSDB), is a wrapper on HDF5 to provide a nice interface for storing time series data as well as a basic sparse B-Tree index on the timestamp. TSDB takes advantage of the fact that most time series data is already sorted and thus you can save a ton of space by inserting an index point every few thousand records and then sequentially searching to find your desired record.<\/p>\n<h2>Features<\/h2>\n<ul>\n<li>Fast subsetting. Able to extract a million rows into MATLAB in a few seconds<\/li>\n<li>Compact file size. It uses a sparse index, so no more multi-gigabyte SAS or relational database indexes. ZLib compression is also enabled by default for further space\u00a0savings.<\/li>\n<li>Millisecond precision<\/li>\n<li>Multiple records with the same timestamp are okay<\/li>\n<li>Gaps in data are okay. TSDB does not force you to store your data on an equally-spaced grid, like time series databases like <a href=\"http:\/\/www.sungard.com\/fame\">FAME<\/a>.<\/li>\n<li>A bunch of datatypes\n<ul>\n<li>32-bit integer<\/li>\n<li>8-bit integer<\/li>\n<li>64-bit double<\/li>\n<li>1-byte character<\/li>\n<li>64-bit timestamp (millisecond precision)<\/li>\n<li>string (user-specified length)<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>Limitations<\/h2>\n<ul>\n<li>Time series are <strong>append only<\/strong>. It&#8217;s possible with HDF5 to edit or insert data in the middle of table, but we haven&#8217;t written the code to update the indexes in that case. I probably won&#8217;t add that feature because this is designed for batch appends of CSV data.<\/li>\n<li>When appending, you\u00a0<strong>must<\/strong> start appending with a row that has a timestamp equal to or after the last timestamp of the existing table. The library will refuse your append if that&#8217;s not the case.<\/li>\n<li>Any data you append must be sorted from earliest timestamp to latest.<\/li>\n<li>You cannot add or remove a column after a time series is created.<\/li>\n<li>You cannot delete a time series from a file (you can obviously delete a file, though).<\/li>\n<li>You cannot have variable length rows. So, if you include a large string field then that number of byte is allocated for each row no matter the length of the string stored in that row. Compression helps with this problem, though, and changing it would slow down the library.<\/li>\n<li>You can&#8217;t change the precision of the timestamps (millisecond only).<\/li>\n<li>You can&#8217;t store timezone info with the timestamp. The library makes no time zone assumptions, though.<\/li>\n<li>It does <strong>not<\/strong> support transactions, so if your program crashes while you are writing to a time series, you could corrupt the file. Keep backups!<\/li>\n<li>Its not under active development. But, if there&#8217;s a glaring bug, I&#8217;ll try to fix it.<\/li>\n<\/ul>\n<h2>Intended usage<\/h2>\n<p>TSDB is beta software, at best. You should\u00a0<strong>not<\/strong> use it to store your only copy of some critical data. Its best use it to store a copy of your raw data in a format that makes it very easy to extract subsets. The idea is to write your time series model in a way that you pull subsets of your data into memory, do some processing, and then write the result to disc.<\/p>\n<h2>Get the source code<\/h2>\n<p>The source code is hosted on Github. I&#8217;m in the process of cleaning up this library, and right now only the C++ library is ready to use. I&#8217;ve only tested the build instructions on a Mac (although Linux should be very similar).<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/afiedler\/tsdb\">Get a copy of the source here<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/afiedler\/tsdb\/wiki\/Build-Instructions-(Mac-OSX)\">Build instructions for Mac OS X are here<\/a>.<\/li>\n<\/ul>\n<h2><\/h2>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 23 Jun 2012 11:57:43 +0000","created_by":1,"updated_at":"Mon, 30 Jun 2014 10:39:09 +0000","updated_by":1,"published_at":"Sat, 23 Jun 2012 11:57:43 +0000","published_by":1},{"id":67,"title":"Books","slug":"books","markdown":"","html":"","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 25 Jun 2012 12:18:07 +0000","created_by":1,"updated_at":"Mon, 25 Jun 2012 12:18:07 +0000","updated_by":1,"published_at":"Mon, 25 Jun 2012 12:18:07 +0000","published_by":1},{"id":72,"title":"The Personal MBA by Josh Kaufman","slug":"temp-slug-9","markdown":"\n\n## The Personal MBA\n\nRead: July 2012  \n Rating:\n\n### My Notes\n\n#### Introduction\n\nJosh (the author) wrote this book after reading a lot of other books on business, psychology, and marketing. He also has an undergraduate business degree and spent some time working at Proctor and Gamble. Honestly, I was a little skeptical that someone without a ton of experience (as a CEO or high-level executive) was qualified to write a book on business, but Josh does show a great ability to distill business problems and how to\u00a0*think* about business problems down to some really clear and digestible writing.\n\nHe makes a few key points in the introduction that I think are worth noting. First, he repeats the increasingly common refrain that the MBA degree is less and less a smart investment. His main arguments against an MBA are:\n\n- They are too expensive. Its not that they don\u2019t provide enough of a benefit, its that they cost too much for the benefits that they provide. This is pretty ironic, since cost-benefit analysis is one of the things they teach in business school!\n- They teach outdated methods. Business is changing rapidly, and MBA programs are focusing too much on complicated financial and statistical formulas without teaching how to improve a real operating business.\n- An MBA can\u2019t\u00a0guarantee\u00a0you a high-paying job, because the skills for high-paying jobs are learned by experience. An MBA can get you connection with\u00a0recruiters, but actually getting the job is your responsibility.\n\n<div>I tend to agree with Josh, so I didn\u2019t take note of too many of the finer points of his arguments. He does back them up with some studies and the arguments are well-reasoned, so if you are considering business school and are at all\u00a0apprehensive, you should read it.<\/div>Josh also makes a good point about acquiring knowledge (learning). He\u2019s theory is that learning helps us to build better mental models because we are using the experiences of others to calibrate our understanding of the world. This book is a distillation of mental models Josh has learned from other business books, working at P&G, and consulting.\n\n\u00a0\n\n\n","html":"<h2>The Personal MBA<\/h2>\n<p>Read: July 2012<br \/>\nRating:<\/p>\n<h3>My Notes<\/h3>\n<h4>Introduction<\/h4>\n<p>Josh (the author) wrote this book after reading a lot of other books on business, psychology, and marketing. He also has an undergraduate business degree and spent some time working at Proctor and Gamble. Honestly, I was a little skeptical that someone without a ton of experience (as a CEO or high-level executive) was qualified to write a book on business, but Josh does show a great ability to distill business problems and how to\u00a0<em>think<\/em> about business problems down to some really clear and digestible writing.<\/p>\n<p>He makes a few key points in the introduction that I think are worth noting. First, he repeats the increasingly common refrain that the MBA degree is less and less a smart investment. His main arguments against an MBA are:<\/p>\n<ul>\n<li>They are too expensive. Its not that they don&#8217;t provide enough of a benefit, its that they cost too much for the benefits that they provide. This is pretty ironic, since cost-benefit analysis is one of the things they teach in business school!<\/li>\n<li>They teach outdated methods. Business is changing rapidly, and MBA programs are focusing too much on complicated financial and statistical formulas without teaching how to improve a real operating business.<\/li>\n<li>An MBA can&#8217;t\u00a0guarantee\u00a0you a high-paying job, because the skills for high-paying jobs are learned by experience. An MBA can get you connection with\u00a0recruiters, but actually getting the job is your responsibility.<\/li>\n<\/ul>\n<div>I tend to agree with Josh, so I didn&#8217;t take note of too many of the finer points of his arguments. He does back them up with some studies and the arguments are well-reasoned, so if you are considering business school and are at all\u00a0apprehensive, you should read it.<\/div>\n<p>Josh also makes a good point about acquiring knowledge (learning). He&#8217;s theory is that learning helps us to build better mental models because we are using the experiences of others to calibrate our understanding of the world. This book is a distillation of mental models Josh has learned from other business books, working at P&amp;G, and consulting.<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 05 Jul 2012 22:33:30 +0000","created_by":1,"updated_at":"Thu, 05 Jul 2012 22:33:30 +0000","updated_by":1,"published_at":"","published_by":1},{"id":76,"title":"Consulting","slug":"consulting","markdown":"\n\n## Available for consulting\n\nI\u2019ve worked on a diverse array of projects, [get in touch](http:\/\/andyfiedler.com\/contact\/ \"Contact\") to see how I can help you on yours! Here\u2019s a few examples of the types of projects I\u2019ve worked on in the past:\n\n- Javascript front-end development using jQuery and Backbone.js\n- Backend development with Ruby on Rails and PHP\n- Windows desktop application development in C# - High-frequency trading applications using the [ESignal](http:\/\/esignal.com \"ESignal\"), [Interactive Brokers](http:\/\/www.interactivebrokers.com \"Interactive Brokers\"), and [IQFeed](http:\/\/iqfeed.net \"IQFeed\") APIs.\n- Trading strategies with [NinjaTrader](http:\/\/www.ninjatrader.com \"Ninja Trader\").\n- Statistical programming and data-mining with R and MATLAB - Option pricing models\n- Yield curve models\n- Analyzing high frequency data sets. See my open source project to [store high frequency data and access it from R and MATLAB](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\").\n\n\n","html":"<h2>Available for consulting<\/h2>\n<p>I&#8217;ve worked on a diverse array of projects, <a title=\"Contact\" href=\"http:\/\/andyfiedler.com\/contact\/\">get in touch<\/a> to see how I can help you on yours! Here&#8217;s a few examples of the types of projects I&#8217;ve worked on in the past:<\/p>\n<ul>\n<li>Javascript front-end development using jQuery and Backbone.js<\/li>\n<li>Backend development with Ruby on Rails and PHP<\/li>\n<li>Windows desktop application development in C#\n<ul>\n<li>High-frequency trading applications using the <a title=\"ESignal\" href=\"http:\/\/esignal.com\">ESignal<\/a>, <a title=\"Interactive Brokers\" href=\"http:\/\/www.interactivebrokers.com\">Interactive Brokers<\/a>, and <a title=\"IQFeed\" href=\"http:\/\/iqfeed.net\">IQFeed<\/a> APIs.<\/li>\n<li>Trading strategies with <a title=\"Ninja Trader\" href=\"http:\/\/www.ninjatrader.com\">NinjaTrader<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>Statistical programming and data-mining with R and MATLAB\n<ul>\n<li>Option pricing models<\/li>\n<li>Yield curve models<\/li>\n<li>Analyzing high frequency data sets. See my open source project to <a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">store high frequency data and access it from R and MATLAB<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 06 Jul 2012 11:43:58 +0000","created_by":1,"updated_at":"Mon, 24 Sep 2012 17:48:29 +0000","updated_by":1,"published_at":"Fri, 06 Jul 2012 11:43:58 +0000","published_by":1},{"id":81,"title":"Using ","slug":"temp-slug-11","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 06 Jul 2012 14:14:04 +0000","created_by":1,"updated_at":"Fri, 06 Jul 2012 14:14:04 +0000","updated_by":1,"published_at":"","published_by":1},{"id":85,"title":"Using TSDB to store U.S. Equities data from TickData","slug":"using-tsdb-to-store-u-s-equities-data-from-tickdata","markdown":"\nIn this article, I\u2019ll show you how to use my\u00a0[Time Series Database](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\") to import some sample U.S. equities data from [TickData](http:\/\/www.tickdata.com), a data provider of high-frequency historical financial data. First, get the [sample data](http:\/\/www.tickdata.com\/downloads\/SampleEquityData_US.zip) and unzip it on your computer. Look in the Quotes folder for a file called GLP_27667.asc. This is file has quotes for Global Partners LP, a petroleum company. If you are running on Mac OS X or Linux, you can use the `head` command to see the first few lines, which in my download look like this:\n\n11\/01\/2011,04:00:00.301,P,14.5,21.36,5,1,R,,175,P,P,1,2,,C 11\/01\/2011,04:56:38.930,P,14.5,21.36,5,2,R,,18491,P,P,1,2,,C 11\/01\/2011,06:02:00.115,M,0,0,0,0,R,,40307,M,M,0,2,,C 11\/01\/2011,07:15:35.501,P,16.67,21.36,15,2,R,,73573,P,P,1,2,,C 11\/01\/2011,07:28:41.053,P,18.4,21.36,10,2,R,,80351,P,P,1,2,,C 11\/01\/2011,07:35:03.401,T,15.18,0,1,0,R,,88691,T,T,0,2,,C 11\/01\/2011,07:35:03.402,T,15.18,25.1,1,1,R,,88692,T,T,0,2,,C 11\/01\/2011,07:50:24.960,T,15.18,21.22,1,1,R,,100945,T,T,6,2,,C 11\/01\/2011,07:50:29.064,P,14.5,21.36,5,2,R,,100980,P,P,6,2,,C 11\/01\/2011,07:50:29.072,T,0,21.22,0,1,R,,100981,T,T,6,2,,C\n\nIf you look on TickData\u2019s website, you can find a [document describing the format](http:\/\/www.tickdata.com\/pdf\/TickData_File_Format_Overview_US_Equities.pdf \"U.S. Equities TickData Format Definition\") of these files in detail, but for this example, we\u2019re only going to look at the first 7 fields.\n\n- Date (MM\/DD\/YYYY)\n- Time\n- Exchange indicator (P means ARCA, M means Chicago, T means NASD)\n- Bid Price\n- Ask Price\n- Bid Amount (100 share lots)\n- Ask Amount (100 share lots)\n\nLet\u2019s create a new TSDB file to store this data. To do this, use the tsdbcreate command.\n\ntsdbcreate glp.tsdb quotes char exchange double bid_price double ask_price int32 bid_size int32 ask_size\n\nThis command is creating a new tsdb database called glp.tsdb with a series called \u201cquotes\u201d. That series has 5 columns: exchange (a character), bid_price (floating point double), ask_price (floating point double), bid_size and ask_size (both 32-bit integers).\n\nYou can add more than one series to a TSDB file. If you were to call the tsdbcreate command and glp.tsdb already existed, then it would add another series to the file.\n\nNow that you have an empty TSDB file, we\u2019ll create some import instructions and import the data. TSDB has a program called TSDB import that uses an XML file to describe how to parse the delimited data in a file like a CSV file and append it to the end of a time series.\u00a0 Whenever you import data, you are always appending it to the end of a time series.**\u00a0**You can import from more than one file into one series in a TSDB file, but you need to start with the earliest file. You also must make sure each of the files you import are in chronological order from oldest timestamp to newest (repeated timestamps are okay).\n\nHere\u2019s an example XML file that shows how you could import the TickData quotes files.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <dataimport> <delimparser field_delim=\",\"> <fieldparser name=\"_TDSB_timestamp\" type=\"timestamp\" tokens=\"0,1\" format_string=\"%m\/%d\/%Y %H:%M:%S%F\" \/> <fieldparser name=\"exchange\" type=\"char\" tokens=\"2\" \/> <fieldparser name=\"bid_price\" type=\"double\" tokens=\"3\" \/> <fieldparser name=\"ask_price\" type=\"double\" tokens=\"4\" \/> <fieldparser name=\"bid_amount\" type=\"int32\" tokens=\"5\" \/> <fieldparser name=\"ask_amount\" type=\"int32\" tokens=\"6\" \/> <\/delimparser> <\/dataimport>\n\nThe XML import tells the tsdbimport command how to map the comma-separated values in each line into columns in the database. Notice in the timestamp `fieldparser` block, we specify to use tokens 0 and 1 to make the timestamp. This tells the parser to join tokens zero and one with a space and then to parse the combined string as a timestamp.\n\nYou can save that XML as a text file called \u201cinstructions.xml\u201d and then import the\u00a0GLP_27667.asc file like this:\n\ntsdbimport instructions.xml GLP_27667.asc glp.tsdb quotes\n\nAfter you run this command, you\u2019ll have the complete GLP time series imported. Now, you can access this data quickly via either the R or MATLAB TSDB bindings. Stay tuned for a article on how to do that within the next few days.\n\n\n","html":"<p>In this article, I&#8217;ll show you how to use my\u00a0<a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">Time Series Database<\/a> to import some sample U.S. equities data from <a href=\"http:\/\/www.tickdata.com\">TickData<\/a>, a data provider of high-frequency historical financial data. First, get the <a href=\"http:\/\/www.tickdata.com\/downloads\/SampleEquityData_US.zip\">sample data<\/a> and unzip it on your computer. Look in the Quotes folder for a file called GLP_27667.asc. This is file has quotes for Global Partners LP, a petroleum company. If you are running on Mac OS X or Linux, you can use the <code>head<\/code> command to see the first few lines, which in my download look like this:<\/p>\n<pre class=\"lang:default highlight:0 decode:true\">11\/01\/2011,04:00:00.301,P,14.5,21.36,5,1,R,,175,P,P,1,2,,C\r\n11\/01\/2011,04:56:38.930,P,14.5,21.36,5,2,R,,18491,P,P,1,2,,C\r\n11\/01\/2011,06:02:00.115,M,0,0,0,0,R,,40307,M,M,0,2,,C\r\n11\/01\/2011,07:15:35.501,P,16.67,21.36,15,2,R,,73573,P,P,1,2,,C\r\n11\/01\/2011,07:28:41.053,P,18.4,21.36,10,2,R,,80351,P,P,1,2,,C\r\n11\/01\/2011,07:35:03.401,T,15.18,0,1,0,R,,88691,T,T,0,2,,C\r\n11\/01\/2011,07:35:03.402,T,15.18,25.1,1,1,R,,88692,T,T,0,2,,C\r\n11\/01\/2011,07:50:24.960,T,15.18,21.22,1,1,R,,100945,T,T,6,2,,C\r\n11\/01\/2011,07:50:29.064,P,14.5,21.36,5,2,R,,100980,P,P,6,2,,C\r\n11\/01\/2011,07:50:29.072,T,0,21.22,0,1,R,,100981,T,T,6,2,,C<\/pre>\n<p>If you look on TickData&#8217;s website, you can find a <a title=\"U.S. Equities TickData Format Definition\" href=\"http:\/\/www.tickdata.com\/pdf\/TickData_File_Format_Overview_US_Equities.pdf\">document describing the format<\/a> of these files in detail, but for this example, we&#8217;re only going to look at the first 7 fields.<\/p>\n<ul>\n<li>Date (MM\/DD\/YYYY)<\/li>\n<li>Time<\/li>\n<li>Exchange indicator (P means ARCA, M means Chicago, T means NASD)<\/li>\n<li>Bid Price<\/li>\n<li>Ask Price<\/li>\n<li>Bid Amount (100 share lots)<\/li>\n<li>Ask Amount (100 share lots)<\/li>\n<\/ul>\n<p>Let&#8217;s create a new TSDB file to store this data. To do this, use the tsdbcreate command.<\/p>\n<pre class=\"lang:sh decode:true\">tsdbcreate glp.tsdb quotes char exchange double bid_price double ask_price int32 bid_size int32 ask_size<\/pre>\n<p>This command is creating a new tsdb database called glp.tsdb with a series called &#8220;quotes&#8221;. That series has 5 columns: exchange (a character), bid_price (floating point double), ask_price (floating point double), bid_size and ask_size (both 32-bit integers).<\/p>\n<p>You can add more than one series to a TSDB file. If you were to call the tsdbcreate command and glp.tsdb already existed, then it would add another series to the file.<\/p>\n<p>Now that you have an empty TSDB file, we&#8217;ll create some import instructions and import the data. TSDB has a program called TSDB import that uses an XML file to describe how to parse the delimited data in a file like a CSV file and append it to the end of a time series.\u00a0 Whenever you import data, you are always appending it to the end of a time series.<strong>\u00a0<\/strong>You can import from more than one file into one series in a TSDB file, but you need to start with the earliest file. You also must make sure each of the files you import are in chronological order from oldest timestamp to newest (repeated timestamps are okay).<\/p>\n<p>Here&#8217;s an example XML file that shows how you could import the TickData quotes files.<\/p>\n<pre class=\"lang:xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\r\n   &lt;dataimport&gt;\r\n      &lt;delimparser field_delim=\",\"&gt;\r\n         &lt;fieldparser name=\"_TDSB_timestamp\" type=\"timestamp\"\r\n            tokens=\"0,1\" format_string=\"%m\/%d\/%Y %H:%M:%S%F\" \/&gt;\r\n         &lt;fieldparser name=\"exchange\" type=\"char\" tokens=\"2\" \/&gt;\r\n         &lt;fieldparser name=\"bid_price\" type=\"double\" tokens=\"3\" \/&gt;\r\n         &lt;fieldparser name=\"ask_price\" type=\"double\" tokens=\"4\" \/&gt;\r\n         &lt;fieldparser name=\"bid_amount\" type=\"int32\" tokens=\"5\" \/&gt;\r\n         &lt;fieldparser name=\"ask_amount\" type=\"int32\" tokens=\"6\" \/&gt;\r\n      &lt;\/delimparser&gt;\r\n   &lt;\/dataimport&gt;<\/pre>\n<p>The XML import tells the tsdbimport command how to map the comma-separated values in each line into columns in the database. Notice in the timestamp <code>fieldparser<\/code> block, we specify to use tokens 0 and 1 to make the timestamp. This tells the parser to join tokens zero and one with a space and then to parse the combined string as a timestamp.<\/p>\n<p>You can save that XML as a text file called &#8220;instructions.xml&#8221; and then import the\u00a0GLP_27667.asc file like this:<\/p>\n<pre class=\"lang:sh\">tsdbimport instructions.xml GLP_27667.asc glp.tsdb quotes<\/pre>\n<p>After you run this command, you&#8217;ll have the complete GLP time series imported. Now, you can access this data quickly via either the R or MATLAB TSDB bindings. Stay tuned for a article on how to do that within the next few days.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 06 Jul 2012 17:18:20 +0000","created_by":1,"updated_at":"Thu, 23 Aug 2012 17:35:29 +0000","updated_by":1,"published_at":"Fri, 06 Jul 2012 17:18:20 +0000","published_by":1},{"id":105,"title":"Portfolio","slug":"temp-slug-13","markdown":"\n\n## Grants Management Software\n\n[![](http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/07\/Grantastic-197x300.png \"Grantastic Screenshot 1\")](http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/07\/Grantastic.png)I developed a Rails-based backend and Bootstrap\/Backbone.js-based front end for a prototype of a software package that helps manage grant spending in the education industry. The software imports financial information from Excel spreadsheets in to a database and then renders graphical and tabular reports from that data.\n\nTechnologies used:\n\n- Rails 3\n- Google Charts\n- Backbone.js\n- Twitter Bootstrap\n\n\n## HealthyGeek.me\n\nHealthyGeek is a personal project to make a simple tool to estimate your nutritional intake. It uses data from the [USDA\u2019s SR24 Database](http:\/\/www.ars.usda.gov\/Services\/docs.htm?docid=8964)\u00a0served by a simple Rails backend and Backbone.js front-end. The goal of this site was to learn Backbone.js and also make a nutrition calculator that is faster to use than others on the Internet. Charts are created with Google Charts and the interface is based on Twitter Bootstrap.\n\n\n## Time Series Database\n\nThis project was started to store high-frequency foreign exchange data. It wraps the [HDF5 library](http:\/\/hdfgroup.org) with some C++ code that adds time series indexing. I developed a GUI in Qt\/C++ to visualize the data, as well as bindings for R and MATLAB. For more information, see the [project\u2019s page](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\").\n\nTechnologies used:\n\n- C\/C++ (primarily developed in Visual Studio, but builds on Unix with GNU Make)\n- HDF5\n- Qt\n- MATLAB\n- R\n\n\n","html":"<h2>Grants Management Software<\/h2>\n<p><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/07\/Grantastic.png\"><img class=\"size-medium wp-image-106 alignleft\" title=\"Grantastic Screenshot 1\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2012\/07\/Grantastic-197x300.png\" alt=\"\" width=\"197\" height=\"300\" \/><\/a>I developed a Rails-based backend and Bootstrap\/Backbone.js-based front end for a prototype of a software package that helps manage grant spending in the education industry. The software imports financial information from Excel spreadsheets in to a database and then renders graphical and tabular reports from that data.<\/p>\n<p>Technologies used:<\/p>\n<ul>\n<li>Rails 3<\/li>\n<li>Google Charts<\/li>\n<li>Backbone.js<\/li>\n<li>Twitter Bootstrap<\/li>\n<\/ul>\n<h2>HealthyGeek.me<\/h2>\n<p>HealthyGeek is a personal project to make a simple tool to estimate your nutritional intake. It uses data from the <a href=\"http:\/\/www.ars.usda.gov\/Services\/docs.htm?docid=8964\">USDA&#8217;s SR24 Database<\/a>\u00a0served by a simple Rails backend and Backbone.js front-end. The goal of this site was to learn Backbone.js and also make a nutrition calculator that is faster to use than others on the Internet. Charts are created with Google Charts and the interface is based on Twitter Bootstrap.<\/p>\n<h2>Time Series Database<\/h2>\n<p>This project was started to store high-frequency foreign exchange data. It wraps the <a href=\"http:\/\/hdfgroup.org\">HDF5 library<\/a> with some C++ code that adds time series indexing. I developed a GUI in Qt\/C++ to visualize the data, as well as bindings for R and MATLAB. For more information, see the <a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">project&#8217;s page<\/a>.<\/p>\n<p>Technologies used:<\/p>\n<ul>\n<li>C\/C++ (primarily developed in Visual Studio, but builds on Unix with GNU Make)<\/li>\n<li>HDF5<\/li>\n<li>Qt<\/li>\n<li>MATLAB<\/li>\n<li>R<\/li>\n<\/ul>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 09 Jul 2012 13:43:18 +0000","created_by":1,"updated_at":"Mon, 09 Jul 2012 13:43:18 +0000","updated_by":1,"published_at":"","published_by":1},{"id":117,"title":"Bio-rhythms and productivity ","slug":"temp-slug-14","markdown":"\nOne thing that I\u2019m noticing about myself is that I\u2019m far more productive when I do tasks at the appropriate time of the day. For example, I\u2019m a morning person and have the most energy to do high-level tasks soon after waking up. Things like writing, thinking about business strategy, and designing the architecture of software only get done well for me if I do them in the mornings. If I save these things for the afternoons, I\u2019m far less likely to finish them, the quality will be lower, and they will take far longer that I originally anticipated.\n\nAfternoons for me seem to be best spent doing the nitty-gritty aspects of writing software and running a business: actually banging out code, debugging, or doing\u00a0bookkeeping. If I do these things in the mornings, I\u2019ve wasting valuable time that could be spent on higher-level tasks. It took me a while to realize this, but now that I have, I\u2019m going to\u00a0consciously\u00a0try to reorganize my day to take advantage of it.\n\nI\u2019m sure other people have different bio-rhythms and are more productive in the afternoon or even late at night, so the key here isn\u2019t\u00a0necessarily to do your highest level work in the mornings.\u00a0\u00a0The key instead is to pay attention to when you are most productive at different tasks and plan your days around that.\n\n\n","html":"<p>One thing that I&#8217;m noticing about myself is that I&#8217;m far more productive when I do tasks at the appropriate time of the day. For example, I&#8217;m a morning person and have the most energy to do high-level tasks soon after waking up. Things like writing, thinking about business strategy, and designing the architecture of software only get done well for me if I do them in the mornings. If I save these things for the afternoons, I&#8217;m far less likely to finish them, the quality will be lower, and they will take far longer that I originally anticipated.<\/p>\n<p>Afternoons for me seem to be best spent doing the nitty-gritty aspects of writing software and running a business: actually banging out code, debugging, or doing\u00a0bookkeeping. If I do these things in the mornings, I&#8217;ve wasting valuable time that could be spent on higher-level tasks. It took me a while to realize this, but now that I have, I&#8217;m going to\u00a0consciously\u00a0try to reorganize my day to take advantage of it.<\/p>\n<p>I&#8217;m sure other people have different bio-rhythms and are more productive in the afternoon or even late at night, so the key here isn&#8217;t\u00a0necessarily to do your highest level work in the mornings.\u00a0\u00a0The key instead is to pay attention to when you are most productive at different tasks and plan your days around that.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 06 Aug 2012 10:18:30 +0000","created_by":1,"updated_at":"Mon, 06 Aug 2012 10:18:30 +0000","updated_by":1,"published_at":"","published_by":1},{"id":120,"title":"National Parks Mapping Project","slug":"temp-slug-15","markdown":"\nFor a few years I\u2019ve known about OpenStreetMap. If you haven\u2019t heard of the project, its basically a Wikipedia for maps. Just like Wikipedia, anyone can edit it. And also like Wikipedia, all of the changes to OpenStreetMap are tracked so that if someone goes in there and tried to vandalize it, it would not be to difficult to revert all of their changes. These two properties have make Wikipedia really successful, and OpenStreetMap is hoping to extend that model to mapping.\n\nIn the past year or so, OpenStreetMap has really started to take off. Internet heavyweights, like Foursquare and MapQuest, have adopted OpenStreetMap in place of Google Maps after Google decided to start charging for access. In Washington, DC, DevelopmentSeed has created some really interesting mapping products around OpenStreetMap for media, non-profit, and governmental organizations.\n\nAll of this buzz about OpenStreetMap and my own interest in GIS and mapping has inspired me to see what I can do with the OpenStreetMap database. As a bit of a side project, I decided to take the OpenStreetMap data and re-render it for hiking. In the database, there already are a lot of hiking trails imported from the U.S. Census\u00a0Bureau\u2019s TIGER dataset. The National Parks Service also has a fair amount of GIS products on their website that might be suitable to import into OpenStreetMap for hiking trails. Finally, I would not be opposed to hiking with a GPS and mapping some trails myself!\n\n\n","html":"<p>For a few years I&#8217;ve known about OpenStreetMap. If you haven&#8217;t heard of the project, its basically a Wikipedia for maps. Just like Wikipedia, anyone can edit it. And also like Wikipedia, all of the changes to OpenStreetMap are tracked so that if someone goes in there and tried to vandalize it, it would not be to difficult to revert all of their changes. These two properties have make Wikipedia really successful, and OpenStreetMap is hoping to extend that model to mapping.<\/p>\n<p>In the past year or so, OpenStreetMap has really started to take off. Internet heavyweights, like Foursquare and MapQuest, have adopted OpenStreetMap in place of Google Maps after Google decided to start charging for access. In Washington, DC, DevelopmentSeed has created some really interesting mapping products around OpenStreetMap for media, non-profit, and governmental organizations.<\/p>\n<p>All of this buzz about OpenStreetMap and my own interest in GIS and mapping has inspired me to see what I can do with the OpenStreetMap database. As a bit of a side project, I decided to take the OpenStreetMap data and re-render it for hiking. In the database, there already are a lot of hiking trails imported from the U.S. Census\u00a0Bureau&#8217;s TIGER dataset. The National Parks Service also has a fair amount of GIS products on their website that might be suitable to import into OpenStreetMap for hiking trails. Finally, I would not be opposed to hiking with a GPS and mapping some trails myself!<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 06 Aug 2012 10:35:25 +0000","created_by":1,"updated_at":"Mon, 06 Aug 2012 10:35:25 +0000","updated_by":1,"published_at":"","published_by":1},{"id":122,"title":"E-Myth: The entrepreneur, the manager, the technitian","slug":"temp-slug-16","markdown":"\nThis post will be about the E-Myth book as applied to my personal goals. Remember: switching from the entrepreneur, the manager, and the technition roles is BAD!! When the entrepreneur sets the vision, and you switch to technitian or \u00a0manager roles, don\u2019t question the vision!\n\n\n","html":"<p>This post will be about the E-Myth book as applied to my personal goals. Remember: switching from the entrepreneur, the manager, and the technition roles is BAD!! When the entrepreneur sets the vision, and you switch to technitian or \u00a0manager roles, don&#8217;t question the vision!<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 06 Aug 2012 13:34:43 +0000","created_by":1,"updated_at":"Mon, 06 Aug 2012 13:34:43 +0000","updated_by":1,"published_at":"","published_by":1},{"id":124,"title":"5 Government datasets for building web apps","slug":"5-government-datasets-for-building-web-apps","markdown":"\nRecently, I\u2019ve gotten interested in using some open government data to build useful web applications. There has been a bit of work in this space, with many Federal agencies sponsoring app contests for developers to create products that show off the agencies\u2019 datasets.[ Challenge.gov](http:\/\/challenge.gov \"U.S. Government Apps Challenge\") is the central listing of these app contests; take a look if you are interested in seeing what kind of challenges are open for entries. I\u2019ve had a bit of experience working with government data and here\u2019s some of the most interesting datasets that I\u2019ve found so far.\n\n### [U.S. Census TIGER](http:\/\/www.census.gov\/geo\/www\/tiger\/)\n\nThis is the mother of all mapping datasets in the U.S. It contains most roads, populated places, and administrative boundaries (for counties and states). There is also a ton of demographic data available subset by geographic location. These are huge datasets, and would need specialized GIS knowledge to do much with. The OpenStreetMap project has imported most of the roads and administrative boundary data, making the U.S. portion of their map pretty detailed. One issue with TIGER data is that many of the roads in the dataset no longer exist. This is especially true in areas that have lost population, like abandoned towns or areas converted to national parks.\n\n### [Federal Reserve Economic Data (FRED)](http:\/\/research.stlouisfed.org\/fred2\/)\n\nAlmost any conceivable economic indicator for the U.S., going back as far as government data exists. Want to know the USD\/GBP exchange rate in 1974? Got it. CPI for 1951? No problem. There are also a lot of useful graphing tools built into the site and a nice REST API for accessing the data from web services.\n\n### [NOAA Weather Radars](http:\/\/www.ncdc.noaa.gov\/nexradinv\/chooseday.jsp?id=klsx)\n\nHistorical RADARs back to the 1990s and near real-time radar images (10 minute or so delay). This website also has satellite images back to the 1970s. I\u2019m sure there is some kind of really cool big data project you could create with these. Maybe show the historical\u00a0likelihood\u00a0of rain in a particular location on a particular day of the year, or check historical satellite images to determine when is the best change for a sunny day.\n\n### [Securities and Exchange\u00a0Commission\u00a0EDGAR](http:\/\/www.sec.gov\/edgar.shtml)\n\nNear real-time display of all filings for\u00a0publicly-traded companies in the U.S. New share\u00a0issuance,\u00a0\u00a0corporate governance changes, and quarterly reports are all there.\n\n### [USGS Stream Flow](http:\/\/waterdata.usgs.gov\/nwis)\n\nI\u2019m mostly putting this on here because I check it frequently for kayaking. This site has near real-time data on the flow and temperature of many rivers in the U.S. The data comes from gauging stations operated by USGS.\n\n\u00a0\n\n\n","html":"<p>Recently, I&#8217;ve gotten interested in using some open government data to build useful web applications. There has been a bit of work in this space, with many Federal agencies sponsoring app contests for developers to create products that show off the agencies&#8217; datasets.<a title=\"U.S. Government Apps Challenge\" href=\"http:\/\/challenge.gov\"> Challenge.gov<\/a> is the central listing of these app contests; take a look if you are interested in seeing what kind of challenges are open for entries. I&#8217;ve had a bit of experience working with government data and here&#8217;s some of the most interesting datasets that I&#8217;ve found so far.<\/p>\n<h3><a href=\"http:\/\/www.census.gov\/geo\/www\/tiger\/\">U.S. Census TIGER<\/a><\/h3>\n<p>This is the mother of all mapping datasets in the U.S. It contains most roads, populated places, and administrative boundaries (for counties and states). There is also a ton of demographic data available subset by geographic location. These are huge datasets, and would need specialized GIS knowledge to do much with. The OpenStreetMap project has imported most of the roads and administrative boundary data, making the U.S. portion of their map pretty detailed. One issue with TIGER data is that many of the roads in the dataset no longer exist. This is especially true in areas that have lost population, like abandoned towns or areas converted to national parks.<\/p>\n<h3><a href=\"http:\/\/research.stlouisfed.org\/fred2\/\">Federal Reserve Economic Data (FRED)<\/a><\/h3>\n<p>Almost any conceivable economic indicator for the U.S., going back as far as government data exists. Want to know the USD\/GBP exchange rate in 1974? Got it. CPI for 1951? No problem. There are also a lot of useful graphing tools built into the site and a nice REST API for accessing the data from web services.<\/p>\n<h3><a href=\"http:\/\/www.ncdc.noaa.gov\/nexradinv\/chooseday.jsp?id=klsx\">NOAA Weather Radars<\/a><\/h3>\n<p>Historical RADARs back to the 1990s and near real-time radar images (10 minute or so delay). This website also has satellite images back to the 1970s. I&#8217;m sure there is some kind of really cool big data project you could create with these. Maybe show the historical\u00a0likelihood\u00a0of rain in a particular location on a particular day of the year, or check historical satellite images to determine when is the best change for a sunny day.<\/p>\n<h3><a href=\"http:\/\/www.sec.gov\/edgar.shtml\">Securities and Exchange\u00a0Commission\u00a0EDGAR<\/a><\/h3>\n<p>Near real-time display of all filings for\u00a0publicly-traded companies in the U.S. New share\u00a0issuance,\u00a0\u00a0corporate governance changes, and quarterly reports are all there.<\/p>\n<h3><a href=\"http:\/\/waterdata.usgs.gov\/nwis\">USGS Stream Flow<\/a><\/h3>\n<p>I&#8217;m mostly putting this on here because I check it frequently for kayaking. This site has near real-time data on the flow and temperature of many rivers in the U.S. The data comes from gauging stations operated by USGS.<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 08 Aug 2012 12:04:54 +0000","created_by":1,"updated_at":"Wed, 08 Aug 2012 12:05:59 +0000","updated_by":1,"published_at":"Wed, 08 Aug 2012 12:04:54 +0000","published_by":1},{"id":150,"title":"Using Capistrano to deploy Rails apps from Gitolite","slug":"using-capistrano-to-deploy-rails-apps-from-gitolite","markdown":"\nHere, I\u2019ll show you how to deploy a Rails app from a Gitolite repository via Capistrano. In this example, I\u2019m running a Phusion Passenger on NGINX on Ubuntu 10.04. The instructions should be very similar for Ubuntu 12.04.\n\nFirst, understand what we\u2019re doing here. I\u2019m assuming you are using Gitolite for version control (although similar instructions would probably work for Github). We\u2019re going to add a read-only deployment key to the Gitolite repository. When you run `cap deploy`, Capistrano will log into your production server via SSH (using public key authentication). Then the Capistrano script will instruct the production server to check out the latest version of your app from the Gitolite repository into a directory on the production server. Finally, the Capistrano script will change a symlink to the new version of your app and instruction Phusion Passenger to reload the app into memory on the next hit.\n\n\n## Setting up your production server\n\nCreate a new user for deployment-related tasks on your production server. Switch to that user.\n\nsudo adduser deployuser sudo su - deployuser\n\nNow, generate some SSH keys for that user. Run as the deployuser:\n\nssh-keygen -t rsa\n\nI don\u2019t typically enter a password for this keypair. The reason is that this keypair is only used for read-only access to your code repository in Gitolite. If your code is highly sensitive, you might want a password. If you enter one here, you will be prompted for it each time you deploy code.\n\nNow, wherever you have your Gitolite admin repository checked out, open it up and add the public key to your keydir folder. I like to keep my deployment keys in a subfolder called something like \u201cdeployment\u201d.\n\nSay, for example, your Gitolite admin repository is at `~\/repos\/gitolite-admin`. Switch to that path. Now enter the folder `keydir`. Make a new subfolder called `deployment`, and then a new file in that folder called something like `MyDeploymentKey.pub`. Open that file in your editor and paste the public key that you just created from your deployment server. Typically, that key is found at `~\/.ssh\/id_rsa.pub`.\n\nNow, open your `gitolite.conf` file (in the `conf` folder in your Gitolite repository). Find your project and add a directive to grant your deployment key read-only access. Here\u2019s an example project section:\n\nrepo my-project RW = JoeCoder R = MyDeploymentKey\n\nNote that even though the deployment key could be in a subfolder, you still just enter the filename minus the \u201c.pub\u201d.\n\nSave the Gitolite files, commit and push to your Gitolite server.\n\n\n## Setting up Capistrano\n\nNow, open up your Rails project you want to deploy. Add these gems:\n\n# Gems for deployment group :development do gem \"capistrano\" gem 'rvm-capistrano' end\n\nRun `bundle install` and then from the top directory of your project, run `capify .`. This adds Capistrano to your project. Open up `config\/deploy.rd` and add something like this:\n\nrequire \"bundler\/capistrano\" require \"rvm\/capistrano\" set :application, \"myapp\" set :domain, \"mydomain.com\" set :repository, \"git@mygitoliteserver.com:mygitrepo\" set :use_sudo, false set :deploy_to, \"\/srv\/mydomain.com\/public\/#{application}\" set :scm, \"git\" set :user, \"deployuser\" role :app, domain role :web, domain role :db, domain, :primary => true # Add RVM's lib directory to the load path. set :rvm_ruby_string, 'ruby-1.9.3-p358' set :rvm_type, :system namespace :deploy do task :start, :roles => :app do run \"touch #{current_release}\/tmp\/restart.txt\" end task :stop, :roles => :app do # Do nothing. end desc \"Restart Application\" task :restart, :roles => :app do run \"touch #{current_release}\/tmp\/restart.txt\" end end\n\nThis deploy script will checkout your code from the project myproject on mygitoliteserver.com and deploy it to `\/srv\/mydomain.com\/public` on your production server (make sure you create this directory). Whenever you deploy, Capistrano will touch `tmp\/restart.txt` so that Phusion Passenger restarts with the new code.\n\nOnce you are finished editing this script, commit your changes, push your latest code to your Gitolite server.\n\n\n## Deciding who gets to deploy\n\nFor each user you want to allow to deploy code, have them generate a SSH key. On your deployment server, open or create `~deployuser\/.ssh\/authorized_keys`. For each user you want to allow to deploy, add their public key (one key per line) to this file.\n\n\n## Deploying!\n\nNow, to test out deployment, run from your Rails root on your development machine (the machine that has the SSH key you added to `~deployuser\/.ssh\/authorized_keys`), run `cap deploy`.\n\n\n","html":"<p>Here, I&#8217;ll show you how to deploy a Rails app from a Gitolite repository via Capistrano. In this example, I&#8217;m running a Phusion Passenger on NGINX on Ubuntu 10.04. The instructions should be very similar for Ubuntu 12.04.<\/p>\n<p>First, understand what we&#8217;re doing here. I&#8217;m assuming you are using Gitolite for version control (although similar instructions would probably work for Github). We&#8217;re going to add a read-only deployment key to the Gitolite repository. When you run <code>cap deploy<\/code>, Capistrano will log into your production server via SSH (using public key authentication). Then the Capistrano script will instruct the production server to check out the latest version of your app from the Gitolite repository into a directory on the production server. Finally, the Capistrano script will change a symlink to the new version of your app and instruction Phusion Passenger to reload the app into memory on the next hit.<\/p>\n<h2>Setting up your production server<\/h2>\n<p>Create a new user for deployment-related tasks on your production server. Switch to that user.<\/p>\n<pre class=\"lang:sh\">sudo adduser deployuser\r\nsudo su - deployuser<\/pre>\n<p>Now, generate some SSH keys for that user. Run as the deployuser:<\/p>\n<pre class=\"lang:sh\">ssh-keygen -t rsa<\/pre>\n<p>I don&#8217;t typically enter a password for this keypair. The reason is that this keypair is only used for read-only access to your code repository in Gitolite. If your code is highly sensitive, you might want a password. If you enter one here, you will be prompted for it each time you deploy code.<\/p>\n<p>Now, wherever you have your Gitolite admin repository checked out, open it up and add the public key to your keydir folder. I like to keep my deployment keys in a subfolder called something like &#8220;deployment&#8221;.<\/p>\n<p>Say, for example, your Gitolite admin repository is at <code>~\/repos\/gitolite-admin<\/code>. Switch to that path. Now enter the folder <code>keydir<\/code>. Make a new subfolder called <code>deployment<\/code>, and then a new file in that folder called something like <code>MyDeploymentKey.pub<\/code>. Open that file in your editor and paste the public key that you just created from your deployment server. Typically, that key is found at <code>~\/.ssh\/id_rsa.pub<\/code>.<\/p>\n<p>Now, open your <code>gitolite.conf<\/code> file (in the <code>conf<\/code> folder in your Gitolite repository). Find your project and add a directive to grant your deployment key read-only access. Here&#8217;s an example project section:<\/p>\n<pre class=\"lang:default highlight:0 decode:true crayon-selected\">repo     my-project\r\n         RW = JoeCoder\r\n         R = MyDeploymentKey<\/pre>\n<p>Note that even though the deployment key could be in a subfolder, you still just enter the filename minus the &#8220;.pub&#8221;.<\/p>\n<p>Save the Gitolite files, commit and push to your Gitolite server.<\/p>\n<h2>Setting up Capistrano<\/h2>\n<p>Now, open up your Rails project you want to deploy. Add these gems:<\/p>\n<pre class=\"lang:ruby\"># Gems for deployment\r\ngroup :development do\r\n  gem \"capistrano\"\r\n  gem 'rvm-capistrano'\r\nend<\/pre>\n<p>Run <code>bundle install<\/code> and then from the top directory of your project, run <code>capify .<\/code>. This adds Capistrano to your project. Open up <code>config\/deploy.rd<\/code> and add something like this:<\/p>\n<pre class=\"lang:ruby\">require \"bundler\/capistrano\"\r\nrequire \"rvm\/capistrano\"\r\n\r\nset :application,   \"myapp\"\r\nset :domain,        \"mydomain.com\"\r\nset :repository,    \"git@mygitoliteserver.com:mygitrepo\"\r\nset :use_sudo,      false\r\nset :deploy_to,     \"\/srv\/mydomain.com\/public\/#{application}\"\r\nset :scm,           \"git\"\r\nset :user,          \"deployuser\"\r\n\r\nrole :app, domain\r\nrole :web, domain\r\nrole :db, domain, :primary =&gt; true\r\n\r\n# Add RVM's lib directory to the load path.\r\nset :rvm_ruby_string, 'ruby-1.9.3-p358'\r\nset :rvm_type, :system\r\n\r\nnamespace :deploy do\r\n  task :start, :roles =&gt; :app do\r\n    run \"touch #{current_release}\/tmp\/restart.txt\"\r\n  end\r\n\r\n  task :stop, :roles =&gt; :app do\r\n    # Do nothing.\r\n  end\r\n\r\n  desc \"Restart Application\"\r\n  task :restart, :roles =&gt; :app do\r\n    run \"touch #{current_release}\/tmp\/restart.txt\"\r\n  end\r\n\r\nend<\/pre>\n<p>This deploy script will checkout your code from the project myproject on mygitoliteserver.com and deploy it to <code>\/srv\/mydomain.com\/public<\/code> on your production server (make sure you create this directory). Whenever you deploy, Capistrano will touch <code>tmp\/restart.txt<\/code> so that Phusion Passenger restarts with the new code.<\/p>\n<p>Once you are finished editing this script, commit your changes, push your latest code to your Gitolite server.<\/p>\n<h2>Deciding who gets to deploy<\/h2>\n<p>For each user you want to allow to deploy code, have them generate a SSH key. On your deployment server, open or create <code>~deployuser\/.ssh\/authorized_keys<\/code>. For each user you want to allow to deploy, add their public key (one key per line) to this file.<\/p>\n<h2>Deploying!<\/h2>\n<p>Now, to test out deployment, run from your Rails root on your development machine (the machine that has the SSH key you added to <code>~deployuser\/.ssh\/authorized_keys<\/code>), run <code>cap deploy<\/code>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 23 Aug 2012 17:29:25 +0000","created_by":1,"updated_at":"Thu, 23 Aug 2012 17:37:10 +0000","updated_by":1,"published_at":"Thu, 23 Aug 2012 17:29:25 +0000","published_by":1},{"id":168,"title":"Backbone.js: Thankfully, a great MVC framework for the frontend","slug":"backbone-js-thankfully-a-great-mvc-framework-for-the-frontend","markdown":"\n\n## Frameworks, frameworks\u2026\n\nOn the backend, web development frameworks have been growing quickly in popularity. Rails, Django, CakePHP, and others are popular because they save developers a ton of time. Someone once said that a good developer is a lazy developer, and frameworks enable developers to kick back with a Corona on a beach (well, not quite, but close) by making a lot of the architectural decisions for the developer. Rails is a great example of this, with a clear MVC structure, preset file system\u00a0hierarchy, and even database schema conventions. If you were coding a site in pure Ruby, you\u2019d need to make all of these decision yourself.\n\nWhile backend frameworks are really popular, there has been a dearth of good choices in front end frameworks. As web apps are moving more processing to client-side Javascript, this was becoming a growing problem.\n\n\n## The front-end Javascript jungle\n\nFront-end javascript tends to be a huge mess of jQuery callbacks, DOM elements stuffed with extra object properties, and a polluted root `window` object. As client-side applications are get larger, this is completely unsustainable. It makes code hard to maintain, hard to understand, and un-testable with unit testing libraries. Backbone.js greatly helps with all of these issues.\n\n\n## Enter Backbone.js\n\nBackbone is a minimalist MVC framework for Javascript. It adds models and collections with persistance code that works well with JSON REST APIs, as well as views that automatically re-render on model updates and controllers that handle hash-bang URLs or HTML5 `pushState`.\n\nAll of this comes in 4KB of minified Javascript that ties in with jQuery, Underscore, or any other Javascript library.\n\n\n## Backbone dos and don\u2019ts\n\nI\u2019m currently working on a small side project to brush up on some Javascript coding, and decided to use Backbone as a front-end framework (I\u2019m using Rails on the backend). Here\u2019s some brief notes from a my first impressions:\n\n**Do**\n\n- Put most of your front-end application (or business) logic in the models. This is basically the same thing you would do with a MVC app on the server.\n- Use a templating library. Underscore.js has a pretty decent `_.template()` function. HTML in really clutters your Javascript code.\n- Try using the Rails asset pipeline or some other way to minify and compile your JS. This way, you can spread your Backbone code out into many files. I tended to use a folder\u00a0hierarchy\u00a0similar to Rails (folders for models, collections, controllers, and views).\n\n**Don\u2019t**\n\n- Put much logic in your views. It is\u00a0*very*\u00a0hard to debug view code because the function that renders the view is typically created\u00a0programmatically\u00a0by the templating library.\n- Don\u2019t prematurely optimize your view code. The easiest way to render a view is to just create an HTML fragment from a templating library then insert it into the DOM with jQuery. This is fine for most cases. You can also manipulate the DOM by changing the inner text of elements on a view re-render, which might be faster but often isn\u2019t worth the extra work.\n\n\n","html":"<h2>Frameworks, frameworks&#8230;<\/h2>\n<p>On the backend, web development frameworks have been growing quickly in popularity. Rails, Django, CakePHP, and others are popular because they save developers a ton of time. Someone once said that a good developer is a lazy developer, and frameworks enable developers to kick back with a Corona on a beach (well, not quite, but close) by making a lot of the architectural decisions for the developer. Rails is a great example of this, with a clear MVC structure, preset file system\u00a0hierarchy, and even database schema conventions. If you were coding a site in pure Ruby, you&#8217;d need to make all of these decision yourself.<\/p>\n<p>While backend frameworks are really popular, there has been a dearth of good choices in front end frameworks. As web apps are moving more processing to client-side Javascript, this was becoming a growing problem.<\/p>\n<h2>The front-end Javascript jungle<\/h2>\n<p>Front-end javascript tends to be a huge mess of jQuery callbacks, DOM elements stuffed with extra object properties, and a polluted root <code>window<\/code> object. As client-side applications are get larger, this is completely unsustainable. It makes code hard to maintain, hard to understand, and un-testable with unit testing libraries. Backbone.js greatly helps with all of these issues.<\/p>\n<h2>Enter Backbone.js<\/h2>\n<p>Backbone is a minimalist MVC framework for Javascript. It adds models and collections with persistance code that works well with JSON REST APIs, as well as views that automatically re-render on model updates and controllers that handle hash-bang URLs or HTML5 <code>pushState<\/code>.<\/p>\n<p>All of this comes in 4KB of minified Javascript that ties in with jQuery, Underscore, or any other Javascript library.<\/p>\n<h2>Backbone dos and don&#8217;ts<\/h2>\n<p>I&#8217;m currently working on a small side project to brush up on some Javascript coding, and decided to use Backbone as a front-end framework (I&#8217;m using Rails on the backend). Here&#8217;s some brief notes from a my first impressions:<\/p>\n<p><strong>Do<\/strong><\/p>\n<ul>\n<li>Put most of your front-end application (or business) logic in the models. This is basically the same thing you would do with a MVC app on the server.<\/li>\n<li>Use a templating library. Underscore.js has a pretty decent <code>_.template()<\/code> function. HTML in really clutters your Javascript code.<\/li>\n<li>Try using the Rails asset pipeline or some other way to minify and compile your JS. This way, you can spread your Backbone code out into many files. I tended to use a folder\u00a0hierarchy\u00a0similar to Rails (folders for models, collections, controllers, and views).<\/li>\n<\/ul>\n<p><strong>Don&#8217;t<\/strong><\/p>\n<ul>\n<li>Put much logic in your views. It is\u00a0<em>very<\/em>\u00a0hard to debug view code because the function that renders the view is typically created\u00a0programmatically\u00a0by the templating library.<\/li>\n<li>Don&#8217;t prematurely optimize your view code. The easiest way to render a view is to just create an HTML fragment from a templating library then insert it into the DOM with jQuery. This is fine for most cases. You can also manipulate the DOM by changing the inner text of elements on a view re-render, which might be faster but often isn&#8217;t worth the extra work.<\/li>\n<\/ul>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 28 Aug 2012 17:45:00 +0000","created_by":1,"updated_at":"Tue, 28 Aug 2012 17:45:00 +0000","updated_by":1,"published_at":"Tue, 28 Aug 2012 17:45:00 +0000","published_by":1},{"id":175,"title":"OpenStreetMap's rising popularity","slug":"openstreetmaps-rising-popularity","markdown":"\n[Craigslist is on board: OpenStreetMap soars to new heights](http:\/\/arstechnica.com\/business\/2012\/08\/craigslist-is-on-board-openstreetmap-continues-soaring-to-new-heights\/)\n\nCraigslist is now using OpenStreetMap for showing the location of apartment listings! This is great for two reasons. First, looking for apartments will become much easier on Craigslist. Second, it\u2019s validation that OpenStreetMap\u2019s dataset is hugely valuable and robust enough for commercial use. Hopefully, I\u2019ll be rolling out my OpenStreetMap project within the next week or so. The project is a re-rendering of the basic slippy map with topographic lines and hiking trails taking more\u00a0prominence. More to come!\n\n\n","html":"<p><a href=\"http:\/\/arstechnica.com\/business\/2012\/08\/craigslist-is-on-board-openstreetmap-continues-soaring-to-new-heights\/\">Craigslist is on board: OpenStreetMap soars to new heights<\/a><\/p>\n<p>Craigslist is now using OpenStreetMap for showing the location of apartment listings! This is great for two reasons. First, looking for apartments will become much easier on Craigslist. Second, it&#8217;s validation that OpenStreetMap&#8217;s dataset is hugely valuable and robust enough for commercial use. Hopefully, I&#8217;ll be rolling out my OpenStreetMap project within the next week or so. The project is a re-rendering of the basic slippy map with topographic lines and hiking trails taking more\u00a0prominence. More to come!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 28 Aug 2012 18:30:04 +0000","created_by":1,"updated_at":"Tue, 28 Aug 2012 18:30:31 +0000","updated_by":1,"published_at":"Tue, 28 Aug 2012 18:30:04 +0000","published_by":1},{"id":180,"title":"Using NGINX to proxy TileStache ","slug":"using-nginx-to-proxy-tilestache","markdown":"\nI\u2019m working on a re-rendering of OpenStreetMap for hiking, with hill shading and topographic lines and I decided to use TileStache to render the tiles. TileStache has the nice ability to render tiles from a bunch of different sources and then overlay them with different amounts of transparency and layer masks (just like Photoshop). TileStache is a Python WSGI server that you can run in mod_python or GUnicorn to serve tiles directly over HTTP. TileStache can cache map tiles to the file system and serve the static PNGs if they exist or render them from scratch using Mapnik if they don\u2019t. Its pretty fast, especially if the tiles are pre-rendered.\n\nHowever, GUnicorn is a pre-forking server. This means that it needs to fork a different process for each client connection. What happens if a slow client connects is that TileStache processes are rapidly used up to serve that client (typically clients make up to 8 separate HTTP connections for slippy maps, resulting in 8 processes each!). This is the case even if the tiles are being served from cache.\n\nWhat you need to do is add a reverse proxy in front of GUnicorn, using something like NGINX. The reverse proxy using an evented IO model, which enables it to manage sending data back to a slow client without using an operating system process. NGINX can also directly serve static assets from the filesystem, which means we can serve the cached tiles without even hitting GUnicorn\/TileStache.\n\nGetting this to work requires a bit of NGINX HttpRewriteModule voodoo, though. The issue is that TileStache saves cached tiles in a slightly different path than the URI path that comes in via HTTP. Say you have a OpenStreetMap-style URL like this: `myserver.com\/tiles\/layer\/$z\/$x\/$y.png`. In this URL, `$z` is zoom level (usually 1-19), and `$x` and `$y` are tile coordinates. For higher zoom levels, you can have 10,000+ by 10,0000+ tiles in the x and y directions. That\u2019s way too many PNG files to store in one folder on the filesystem. So, TileStache splits up the x and y paths into two levels. Say you have a URL like `\/tiles\/layer\/7\/12345\/67890.png`. TileStache will store that in the filesystem path `\/tiles\/layer\/7\/012\/345\/067\/890.png`. Notice how 12345 is broken into 012\/345? That means that there will be at most 1,000 files or folders in each directory\u2014a manageable amount. The issue is we need to get NGINX to rewrite URLs to server these static assets. Here\u2019s how I accomplished that:\n\n location ~ ^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]+)\\\/([\\d]+)\\.png { root \/osm\/cache; set $originaluri \/$1\/$2\/$3\/$4.png; # 1 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/00$3\/000\/00$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/00$3\/000\/0$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/000\/$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/00$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/0$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/$4\/$5.png break; # 2 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/0$3\/000\/00$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/0$3\/000\/0$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/000\/$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/00$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/0$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/$4\/$5.png break; # 3 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/$3\/000\/00$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/$3\/000\/0$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/000\/$4.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/00$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/0$4\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/$4\/$5.png break; # 4 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/00$3\/$4\/000\/00$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/00$3\/$4\/000\/0$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/000\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/00$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/0$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/$5\/$6.png break; # 5 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/0$3\/$4\/000\/00$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/0$3\/$4\/000\/0$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/000\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/00$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/0$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/$5\/$6.png break; # 6 char X rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/$3\/$4\/000\/00$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/$3\/$4\/000\/0$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/000\/$5.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/00$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/0$5\/$6.png break; rewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/$5\/$6.png break; # Try to serve the file from the disk. If that doesn't work, pass through the request via the proxy try_files $uri @tilestache; }\n\nThis mountain of rewrite lines will rewrite the request URL to the filesystem format, then look for tiles in the filesystem tree starting at `\/osm\/cache`. The last line tells NGINX to look for the rewritten URL, then if the file is not found, to send the request to the `@tilestache;` location block, which looks like this:\n\n location @tilestache { # Rewrite back to standard OSM form rewrite ^(.*)$ $originaluri break; add_header X-Static miss; proxy_pass http:\/\/127.0.0.1:8080; proxy_set_header Host $http_host; }\n\nThat location block proxies the request to the GUnicorn server listening on localhost:8080.\n\nThis seems to be working great. NGINX is far faster in serving static assets, and if all of the worker TileStache processes are busy rendering, the cached zoom levels of the map work fine!\n\n\n","html":"<p>I&#8217;m working on a re-rendering of OpenStreetMap for hiking, with hill shading and topographic lines and I decided to use TileStache to render the tiles. TileStache has the nice ability to render tiles from a bunch of different sources and then overlay them with different amounts of transparency and layer masks (just like Photoshop). TileStache is a Python WSGI server that you can run in mod_python or GUnicorn to serve tiles directly over HTTP. TileStache can cache map tiles to the file system and serve the static PNGs if they exist or render them from scratch using Mapnik if they don&#8217;t. Its pretty fast, especially if the tiles are pre-rendered.<\/p>\n<p>However, GUnicorn is a pre-forking server. This means that it needs to fork a different process for each client connection. What happens if a slow client connects is that TileStache processes are rapidly used up to serve that client (typically clients make up to 8 separate HTTP connections for slippy maps, resulting in 8 processes each!). This is the case even if the tiles are being served from cache.<\/p>\n<p>What you need to do is add a reverse proxy in front of GUnicorn, using something like NGINX. The reverse proxy using an evented IO model, which enables it to manage sending data back to a slow client without using an operating system process. NGINX can also directly serve static assets from the filesystem, which means we can serve the cached tiles without even hitting GUnicorn\/TileStache.<\/p>\n<p>Getting this to work requires a bit of NGINX HttpRewriteModule voodoo, though. The issue is that TileStache saves cached tiles in a slightly different path than the URI path that comes in via HTTP. Say you have a OpenStreetMap-style URL like this: <code>myserver.com\/tiles\/layer\/$z\/$x\/$y.png<\/code>. In this URL, <code>$z<\/code> is zoom level (usually 1-19), and <code>$x<\/code> and <code>$y<\/code> are tile coordinates. For higher zoom levels, you can have 10,000+ by 10,0000+ tiles in the x and y directions. That&#8217;s way too many PNG files to store in one folder on the filesystem. So, TileStache splits up the x and y paths into two levels. Say you have a URL like <code>\/tiles\/layer\/7\/12345\/67890.png<\/code>. TileStache will store that in the filesystem path <code>\/tiles\/layer\/7\/012\/345\/067\/890.png<\/code>. Notice how 12345 is broken into 012\/345? That means that there will be at most 1,000 files or folders in each directory\u2014a manageable amount. The issue is we need to get NGINX to rewrite URLs to server these static assets. Here&#8217;s how I accomplished that:<\/p>\n<pre class=\"lang:default highlight:0 decode:true\" title=\"Main NGINX location directive\">\tlocation ~ ^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]+)\\\/([\\d]+)\\.png {\r\n\t\troot \/osm\/cache;\r\n\r\n\t\tset $originaluri \/$1\/$2\/$3\/$4.png;\r\n\r\n\t\t# 1 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/00$3\/000\/00$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/00$3\/000\/0$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/000\/$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/00$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/0$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/00$3\/$4\/$5.png break;\r\n\r\n\t\t# 2 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/0$3\/000\/00$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/0$3\/000\/0$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/000\/$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/00$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/0$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/0$3\/$4\/$5.png break;\r\n\r\n\t\t# 3 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/000\/$3\/000\/00$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/000\/$3\/000\/0$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/000\/$4.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/00$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/0$4\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/000\/$3\/$4\/$5.png break;\r\n\r\n\t\t# 4 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/00$3\/$4\/000\/00$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/00$3\/$4\/000\/0$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/000\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/00$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/0$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{1})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/00$3\/$4\/$5\/$6.png break;\r\n\r\n\t\t# 5 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/0$3\/$4\/000\/00$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/0$3\/$4\/000\/0$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/000\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/00$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/0$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{2})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/0$3\/$4\/$5\/$6.png break;\r\n\r\n\t\t# 6 char X\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{1}).png$\" \/$1\/$2\/$3\/$4\/000\/00$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{2}).png$\" \/$1\/$2\/$3\/$4\/000\/0$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/000\/$5.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{1})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/00$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{2})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/0$5\/$6.png break;\r\n\t\trewrite \"^\\\/tiles\\\/([\\w\\-]+)\\\/([\\d]+)\\\/([\\d]{3})([\\d]{3})\\\/([\\d]{3})([\\d]{3}).png$\" \/$1\/$2\/$3\/$4\/$5\/$6.png break;\r\n\r\n\t\t# Try to serve the file from the disk. If that doesn't work, pass through the request via the proxy\r\n\t\ttry_files $uri @tilestache;\r\n\t}<\/pre>\n<p>This mountain of rewrite lines will rewrite the request URL to the filesystem format, then look for tiles in the filesystem tree starting at <code>\/osm\/cache<\/code>. The last line tells NGINX to look for the rewritten URL, then if the file is not found, to send the request to the <code>@tilestache;<\/code> location block, which looks like this:<\/p>\n<pre class=\"lang:default highlight:0 decode:true \" title=\"Proxy NGINX block\">\t location @tilestache {\r\n\r\n\t\t# Rewrite back to standard OSM form\r\n\t\trewrite ^(.*)$ $originaluri break; \r\n\t\tadd_header X-Static miss;\r\n\t\tproxy_pass http:\/\/127.0.0.1:8080;\r\n\t\tproxy_set_header Host $http_host;\r\n\t}<\/pre>\n<p>That location block proxies the request to the GUnicorn server listening on localhost:8080.<\/p>\n<p>This seems to be working great. NGINX is far faster in serving static assets, and if all of the worker TileStache processes are busy rendering, the cached zoom levels of the map work fine!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 30 Aug 2012 12:54:18 +0000","created_by":1,"updated_at":"Thu, 30 Aug 2012 12:54:18 +0000","updated_by":1,"published_at":"Thu, 30 Aug 2012 12:54:18 +0000","published_by":1},{"id":184,"title":"HealthyGeek.me","slug":"healthygeek-me","markdown":"\nThis site is a collection of health and exercise web apps. Currently, there is a [nutrition calculator](http:\/\/healthygeek.me\/menus\/new) but I might expand this site into other apps and blog posts in the future.\n\n\n## Technology behind the site\n\nThe frontend is written using Backbone.js as the main Javascript library. The stylesheet is Bootstrap, with a custom theme from [Bootswatch](http:\/\/bootswatch.com\/). The backend is written in Rails. The backend is very simple and likely could be written using Node.js and MongoDB, however the Rails asset pipeline was a great tool for compiling and packaging the Javascript and CSS.\n\n\n","html":"<p>This site is a collection of health and exercise web apps. Currently, there is a <a href=\"http:\/\/healthygeek.me\/menus\/new\">nutrition calculator<\/a> but I might expand this site into other apps and blog posts in the future.<\/p>\n<h2>Technology behind the site<\/h2>\n<p>The frontend is written using Backbone.js as the main Javascript library. The stylesheet is Bootstrap, with a custom theme from <a href=\"http:\/\/bootswatch.com\/\">Bootswatch<\/a>. The backend is written in Rails. The backend is very simple and likely could be written using Node.js and MongoDB, however the Rails asset pipeline was a great tool for compiling and packaging the Javascript and CSS.<\/p>\n","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 06 Sep 2012 20:08:01 +0000","created_by":1,"updated_at":"Thu, 06 Sep 2012 20:08:01 +0000","updated_by":1,"published_at":"Thu, 06 Sep 2012 20:08:01 +0000","published_by":1},{"id":197,"title":"Motorcycling in Laos","slug":"motorcycling-in-laos","markdown":"\nLast month I spent a week motorcycling around Laos with a friend, starting at the capital and heading north through probably one of the most scenic parts of Southeast Asia. This is still a fairly undeveloped part of the world, meaning empty roads, not many other tourists, and an unspoiled landscape. It also means the logistics of this trip were not easy, but the ride was definitely worth it. If you are interested in doing a trip like this, read on!\n\n\n## Preparing for the trip\n\nRoads in Laos range from two lanes and decent blacktop on the main highways to muddy dirt tracks in the villages, so you will want to be a fairly proficient rider with at least some dirt biking experience before you go. We went early in the rainy season, which contributed to the mud, but even in the dry season, a lot of the smaller roads will require some dirt riding skills.\n\nThe two most sensible starting points for a trip like this are the capital, Vientiane, and Luang Prabang, a UNESCO World Heritage Town about 340km to the north. Both cities have international airports, but they are very small, so expect to fly through a major Asian hub like Bangkok or Seoul if coming from the U.S. or Europe. You can also take a bus from Thailand, Cambodia or Vietnam if you are already in Southeast Asia, but expect it to be an overnight bus (12 hours+) on mountainous, winding roads.\n\n\n## Renting bikes\n\nWe rented a Honda CRF250 and CRF230 from [Remote Asia](http:\/\/remoteasia.com \"Remote Asia Outfitters\")\u00a0in Vientiane, who were excellent. The bikes were in great shape and Jim from Remote Asia was very helpful when we had a minor mishap (I lost an ignition key to my bike!). You really will want to rent bikes from someone who can also provide a cell phone and \u201croadside assistance\u201d, and by that I mean basically translating via cell phone and recommending mechanics. English is not widely spoken in Laos and emergency services are non-existent, so you will want an English speaking contact in-country from your rental agency.\n\nWe had no issues with Laotian police (in fact we did not see any outside of Vientiane), so it would seem that you don\u2019t strictly need an International Drivers License or a motorcycle license at all. Your rental bike should come with a Laotian number plate and turn signals, and riding with a headlight during the day is apparently illegal so should be avoided.\n\n\n## The route\n\nLaos has a few excellent motorcycle routes, and the one we took seems to be the most popular. You can order an excellent map of Laos with elevation profiles, road conditions, and city maps from GT Rider in Thailand [here](http:\/\/www.gt-rider.com\/maps-of-thailand-laos-maps\/laos-guide-map). This route took us 6 days of riding at a fairly leisurely pace. There are three long days of riding (over 150km) and one day each spent in Vang Vieng, Ponsavan, and Luang Prabang that you can use to explore the area surrounding these cities.\n\nIn addition to the GT Rider map, you should also get an Open Street Map app for your smart phone. OSM actually had pretty good coverage of Laos, including some of the caves and waterfalls outside of the cities. For Android, try [OSMAnd](https:\/\/play.google.com\/store\/apps\/details?id=net.osmand), which lets you download country maps for offline use.\n\nHere\u2019s a map of the route we took and details of each leg.\n\n<style type=\"text\/css\">\r\n                 .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; text-align:center; width:360px; }\r\n                 <\/style><style type=\"text\/css\"><\/style><script src=\"http:\/\/andyfiedler.com\/wp-content\/plugins\/advanced-iframe\/js\/ai.js\" type=\"text\/javascript\"><\/script><script type=\"text\/javascript\">    function aiShowIframe() { jQuery(\"#advanced_iframe\").css(\"visibility\", \"visible\");}    function aiShowIframeId(id_iframe) { jQuery(id_iframe).css(\"visibility\", \"visible\");}    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe); }    function aiResizeIframeHeightId(height,id) {\r\n                  aiResizeIframeHeightById(id,height); }\r\n                  <\/script><iframe allowtransparency=\"true\" border=\"0\" frameborder=\"0\" height=\"480\" id=\"advanced_iframe\" name=\"advanced_iframe\" scrolling=\"no\" src=\"https:\/\/maps.google.com\/maps\/ms?msa=0&msid=213059449424618329795.0004e1d4c6c3d82758c09&ie=UTF8&t=p&ll=18.921876,102.661743&spn=2.494061,3.515625&z=8&output=embed\" width=\"640\"><\/iframe><script type=\"text\/javascript\">var ifrm_advanced_iframe = document.getElementById(\"advanced_iframe\");<\/script><script type=\"text\/javascript\">\r\n                function resizeCallbackadvanced_iframe() {}<\/script>\u00a0\n\n### Vientiane to Vang Vieng, 154km\n\n<style type=\"text\/css\">\n\t\t\t#gallery-1 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-1 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-1 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-1 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style><div class=\"gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail\" id=\"gallery-1\"><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![This is a dirt road leading from a waterfall outside of Vang Vieng, Laos back into town. The photo was taken during the rainy season,  during a break in the afternoon showers.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_134937-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=212)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-1-212\"> This is a dirt road leading from a waterfall outside of Vang Vieng, Laos back into town. The photo was taken during the rainy season, during a break in the afternoon showers. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon portrait\">[![Waterfall outside Vang Vieng. This was taken in the rainy season, so there is a lot of water flowing.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_124142-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=217)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-1-217\"> Waterfall outside Vang Vieng. This was taken in the rainy season, so there is a lot of water flowing. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![Nam Song River at sunset.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_145318-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=218)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-1-218\"> Nam Song River at sunset. <\/dd><\/dl>  \n<\/div>If picking up your bikes in Vientiane, likely your first stop will be [Vang Vieng](http:\/\/en.wikipedia.org\/wiki\/Vang_Vieng). The ride to Vang Vieng is 154 km and fairly easy and flat until the last 50km or so where there are some small hills. Leaving Vientiane is where you will probably encounter the most traffic of the trip (not much!), but within 50km, it drops off a lot.\n\nVang Vieng is a tourist town along the Nam Song river with lots of caving, waterfalls, and rafting available nearby. It is definitely worth at least one whole day exploring outside of the town. I think of the three towns we visited, Vang Vieng had the most spectacular scenery. The town is surrounded by a dramatic\u00a0[karst landscape](http:\/\/en.wikipedia.org\/wiki\/Karst_topography)\u00a0that is even more surreal in the rainy season when the peaks are surrounded in fog. In Vang Vieng, we went to a cave, waterfall and \u201cblue lagoon\u201d (swimming hole) on our one full day there. These were all found on dirt paths leading out from the town, for which Open Street Map was a real help locating.\n\n### Vang Vieng to Ponsavan, 233km\n\n<style type=\"text\/css\">\n\t\t\t#gallery-2 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-2 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-2 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-2 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style><div class=\"gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail\" id=\"gallery-2\"><dl class=\"gallery-item\"><dt class=\"gallery-icon portrait\">[![View towards what I believe is the highest peak in Laos.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130713_074030-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=219)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-2-219\"> View towards what I believe is the highest peak in Laos. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![Ponsavan is a dusty provincial town that looks a bit like the American West.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130713_142645-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=220)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-2-220\"> Ponsavan is a dusty provincial town that looks a bit like the American West. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![Plain of Jars. These were created by a neolithic civilization for burials, but were damaged in the Vietnam War.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130714_074338-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=224)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-2-224\"> Plain of Jars. These were created by a neolithic civilization for burials, but were damaged in the Vietnam War. <\/dd><\/dl>  \n<\/div>The second long day of riding, Vang Vieng to Ponsavan, is 233 km of challenging riding with tons of very steep switchbacks and incredible views of some of the highest peaks in Laos. On this part of the ride, you are heading to [Ponsavan](http:\/\/wikitravel.org\/en\/Phonsavan), which is the capital of the providence containing the [Plain of Jars](http:\/\/wikitravel.org\/en\/Plain_of_Jars), a neolithic archeological site with large stone jars (some over 4 feet tall). The Plain of Jars was also bombed heavily during the Vietnam War as part of the CIA\u2019s covert war in Laos, so there is lots of war history in this area as well. Ponsavan has a few NGOs operating to clear unexploded ordinance that have interesting exhibits in the town on the war.\n\n### Ponsavan to Luang Prabang, 259km\n\n<style type=\"text\/css\">\n\t\t\t#gallery-3 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-3 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-3 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-3 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style><div class=\"gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail\" id=\"gallery-3\"><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![More excellent riding north of Pho Khoun.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130715_111204-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=221)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-3-221\"> More excellent riding north of Pho Khoun. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![Mekong River in Luang Prabang.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130718_143756-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=223)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-3-223\"> Mekong River in Luang Prabang. <\/dd><\/dl><dl class=\"gallery-item\"><dt class=\"gallery-icon landscape\">[![Great swimming hole outside Luang Prabang.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130716_083641-150x150.jpg)](http:\/\/andyfiedler.com\/?attachment_id=225)<\/dt><dd class=\"wp-caption-text gallery-caption\" id=\"gallery-3-225\"> Great swimming hole outside Luang Prabang. <\/dd><\/dl>  \n<\/div>The last long day of riding will take you 259 km from Ponsavan to Luang Prabang (backtracking to Pho Khoun, then north to Luang Prabang). Backtracking in this way isn\u2019t all that bad because the scenery and riding is so great heading from Pho Khoun to Ponsavan, you\u2019ll have no problem doing it twice.\n\nPho Khoun makes a good stopping place for lunch, as its about at the midpoint of this ride and there are a few places to refuel and grab lunch. From Pho Khoun, you\u2019ll have similar riding (and that\u2019s to say great!)\u00a0to Luang Prabang.\n\n[Luang Prabang](http:\/\/wikitravel.org\/en\/Luang_Prabang) is a UNESCO World Heritage Site and has by far the best developed tourist infrastructure of the three cities we visited. Luang Prabang is at the intersection of the Mekong River and the Nam Khan River and has a large temple in the center of the city with great views of both rivers. There are waterfalls about 20km outside of town as well as rafting, elephant tours, and kayaking if you choose to stay here for a few days.\n\n<div class=\"wp-caption aligncenter\" id=\"attachment_196\" style=\"width: 570px\">[![Route: Vientiane - Vang Vieng- Pho Khoun - Ponsavan - Phou Khoun - Luang Prabang](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/laos_ride_profile-1024x696.png \"Elevation Profile\")](http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/laos_ride_profile.png)Elevation Profile: Vientiane \u2013 Vang Vieng \u2013 Pho Khoun \u2013 Ponsavan \u2013 Phou Khoun \u2013 Luang Prabang\n\n<\/div>\n","html":"<p>Last month I spent a week motorcycling around Laos with a friend, starting at the capital and heading north through probably one of the most scenic parts of Southeast Asia. This is still a fairly undeveloped part of the world, meaning empty roads, not many other tourists, and an unspoiled landscape. It also means the logistics of this trip were not easy, but the ride was definitely worth it. If you are interested in doing a trip like this, read on!<\/p>\n<h2>Preparing for the trip<\/h2>\n<p>Roads in Laos range from two lanes and decent blacktop on the main highways to muddy dirt tracks in the villages, so you will want to be a fairly proficient rider with at least some dirt biking experience before you go. We went early in the rainy season, which contributed to the mud, but even in the dry season, a lot of the smaller roads will require some dirt riding skills.<\/p>\n<p>The two most sensible starting points for a trip like this are the capital, Vientiane, and Luang Prabang, a UNESCO World Heritage Town about 340km to the north. Both cities have international airports, but they are very small, so expect to fly through a major Asian hub like Bangkok or Seoul if coming from the U.S. or Europe. You can also take a bus from Thailand, Cambodia or Vietnam if you are already in Southeast Asia, but expect it to be an overnight bus (12 hours+) on mountainous, winding roads.<\/p>\n<h2>Renting bikes<\/h2>\n<p>We rented a Honda CRF250 and CRF230 from <a title=\"Remote Asia Outfitters\" href=\"http:\/\/remoteasia.com\" target=\"_blank\">Remote Asia<\/a>\u00a0in Vientiane, who were excellent. The bikes were in great shape and Jim from Remote Asia was very helpful when we had a minor mishap (I lost an ignition key to my bike!). You really will want to rent bikes from someone who can also provide a cell phone and &#8220;roadside assistance&#8221;, and by that I mean basically translating via cell phone and recommending mechanics. English is not widely spoken in Laos and emergency services are non-existent, so you will want an English speaking contact in-country from your rental agency.<\/p>\n<p>We had no issues with Laotian police (in fact we did not see any outside of Vientiane), so it would seem that you don&#8217;t strictly need an International Drivers License or a motorcycle license at all. Your rental bike should come with a Laotian number plate and turn signals, and riding with a headlight during the day is apparently illegal so should be avoided.<\/p>\n<h2>The route<\/h2>\n<p>Laos has a few excellent motorcycle routes, and the one we took seems to be the most popular. You can order an excellent map of Laos with elevation profiles, road conditions, and city maps from GT Rider in Thailand <a href=\"http:\/\/www.gt-rider.com\/maps-of-thailand-laos-maps\/laos-guide-map\" target=\"_blank\">here<\/a>. This route took us 6 days of riding at a fairly leisurely pace. There are three long days of riding (over 150km) and one day each spent in Vang Vieng, Ponsavan, and Luang Prabang that you can use to explore the area surrounding these cities.<\/p>\n<p>In addition to the GT Rider map, you should also get an Open Street Map app for your smart phone. OSM actually had pretty good coverage of Laos, including some of the caves and waterfalls outside of the cities. For Android, try <a href=\"https:\/\/play.google.com\/store\/apps\/details?id=net.osmand\">OSMAnd<\/a>, which lets you download country maps for offline use.<\/p>\n<p>Here&#8217;s a map of the route we took and details of each leg.<\/p>\n<style type=\"text\/css\">\r\n                 .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; text-align:center; width:360px; }\r\n                 <\/style><style type=\"text\/css\"><\/style><script type=\"text\/javascript\" src=\"http:\/\/andyfiedler.com\/wp-content\/plugins\/advanced-iframe\/js\/ai.js\" ><\/script><script type=\"text\/javascript\">    function aiShowIframe() { jQuery(\"#advanced_iframe_2\").css(\"visibility\", \"visible\");}    function aiShowIframeId(id_iframe) { jQuery(id_iframe).css(\"visibility\", \"visible\");}    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe_2); }    function aiResizeIframeHeightId(height,id) {\r\n                  aiResizeIframeHeightById(id,height); }\r\n                  <\/script><iframe id='advanced_iframe_2'  name='advanced_iframe_2'  src='https:\/\/maps.google.com\/maps\/ms?msa=0&amp;msid=213059449424618329795.0004e1d4c6c3d82758c09&amp;ie=UTF8&amp;t=p&amp;ll=18.921876,102.661743&amp;spn=2.494061,3.515625&amp;z=8&amp;output=embed'  width='640'  height='480'  scrolling='no'  frameborder='0'  border='0'  allowtransparency='true' ><\/iframe><script type=\"text\/javascript\">var ifrm_advanced_iframe_2 = document.getElementById(\"advanced_iframe_2\");<\/script><script type=\"text\/javascript\">\r\n                function resizeCallbackadvanced_iframe_2() {}<\/script>\n<p>&nbsp;<\/p>\n<h3>Vientiane to Vang Vieng, 154km<\/h3>\n\n\t\t<style type='text\/css'>\n\t\t\t#gallery-4 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-4 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-4 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-4 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style>\n\t\t<div id='gallery-4' class='gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail'><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=212'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_134937-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"This is a dirt road leading from a waterfall outside of Vang Vieng, Laos back into town. The photo was taken during the rainy season,  during a break in the afternoon showers.\" aria-describedby=\"gallery-4-212\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-4-212'>\n\t\t\t\tThis is a dirt road leading from a waterfall outside of Vang Vieng, Laos back into town. The photo was taken during the rainy season,  during a break in the afternoon showers.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon portrait'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=217'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_124142-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Waterfall outside Vang Vieng. This was taken in the rainy season, so there is a lot of water flowing.\" aria-describedby=\"gallery-4-217\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-4-217'>\n\t\t\t\tWaterfall outside Vang Vieng. This was taken in the rainy season, so there is a lot of water flowing.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=218'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_145318-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Nam Song River at sunset.\" aria-describedby=\"gallery-4-218\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-4-218'>\n\t\t\t\tNam Song River at sunset.\n\t\t\t\t<\/dd><\/dl><br style=\"clear: both\" \/>\n\t\t<\/div>\n\n<p>If picking up your bikes in Vientiane, likely your first stop will be <a href=\"http:\/\/en.wikipedia.org\/wiki\/Vang_Vieng\" target=\"_blank\">Vang Vieng<\/a>. The ride to Vang Vieng is 154 km and fairly easy and flat until the last 50km or so where there are some small hills. Leaving Vientiane is where you will probably encounter the most traffic of the trip (not much!), but within 50km, it drops off a lot.<\/p>\n<p>Vang Vieng is a tourist town along the Nam Song river with lots of caving, waterfalls, and rafting available nearby. It is definitely worth at least one whole day exploring outside of the town. I think of the three towns we visited, Vang Vieng had the most spectacular scenery. The town is surrounded by a dramatic\u00a0<a href=\"http:\/\/en.wikipedia.org\/wiki\/Karst_topography\">karst landscape<\/a>\u00a0that is even more surreal in the rainy season when the peaks are surrounded in fog. In Vang Vieng, we went to a cave, waterfall and &#8220;blue lagoon&#8221; (swimming hole) on our one full day there. These were all found on dirt paths leading out from the town, for which Open Street Map was a real help locating.<\/p>\n<h3>Vang Vieng to Ponsavan, 233km<\/h3>\n\n\t\t<style type='text\/css'>\n\t\t\t#gallery-5 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-5 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-5 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-5 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style>\n\t\t<div id='gallery-5' class='gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail'><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon portrait'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=219'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130713_074030-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"View towards what I believe is the highest peak in Laos.\" aria-describedby=\"gallery-5-219\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-5-219'>\n\t\t\t\tView towards what I believe is the highest peak in Laos.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=220'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130713_142645-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Ponsavan is a dusty provincial town that looks a bit like the American West.\" aria-describedby=\"gallery-5-220\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-5-220'>\n\t\t\t\tPonsavan is a dusty provincial town that looks a bit like the American West.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=224'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130714_074338-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Plain of Jars. These were created by a neolithic civilization for burials, but were damaged in the Vietnam War.\" aria-describedby=\"gallery-5-224\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-5-224'>\n\t\t\t\tPlain of Jars. These were created by a neolithic civilization for burials, but were damaged in the Vietnam War.\n\t\t\t\t<\/dd><\/dl><br style=\"clear: both\" \/>\n\t\t<\/div>\n\n<p>The second long day of riding, Vang Vieng to Ponsavan, is 233 km of challenging riding with tons of very steep switchbacks and incredible views of some of the highest peaks in Laos. On this part of the ride, you are heading to <a href=\"http:\/\/wikitravel.org\/en\/Phonsavan\">Ponsavan<\/a>, which is the capital of the providence containing the <a href=\"http:\/\/wikitravel.org\/en\/Plain_of_Jars\">Plain of Jars<\/a>, a neolithic archeological site with large stone jars (some over 4 feet tall). The Plain of Jars was also bombed heavily during the Vietnam War as part of the CIA&#8217;s covert war in Laos, so there is lots of war history in this area as well. Ponsavan has a few NGOs operating to clear unexploded ordinance that have interesting exhibits in the town on the war.<\/p>\n<h3>Ponsavan to Luang Prabang, 259km<\/h3>\n\n\t\t<style type='text\/css'>\n\t\t\t#gallery-6 {\n\t\t\t\tmargin: auto;\n\t\t\t}\n\t\t\t#gallery-6 .gallery-item {\n\t\t\t\tfloat: left;\n\t\t\t\tmargin-top: 10px;\n\t\t\t\ttext-align: center;\n\t\t\t\twidth: 33%;\n\t\t\t}\n\t\t\t#gallery-6 img {\n\t\t\t\tborder: 2px solid #cfcfcf;\n\t\t\t}\n\t\t\t#gallery-6 .gallery-caption {\n\t\t\t\tmargin-left: 0;\n\t\t\t}\n\t\t\t\/* see gallery_shortcode() in wp-includes\/media.php *\/\n\t\t<\/style>\n\t\t<div id='gallery-6' class='gallery galleryid-197 gallery-columns-3 gallery-size-thumbnail'><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=221'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130715_111204-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"More excellent riding north of Pho Khoun.\" aria-describedby=\"gallery-6-221\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-6-221'>\n\t\t\t\tMore excellent riding north of Pho Khoun.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=223'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130718_143756-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Mekong River in Luang Prabang.\" aria-describedby=\"gallery-6-223\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-6-223'>\n\t\t\t\tMekong River in Luang Prabang.\n\t\t\t\t<\/dd><\/dl><dl class='gallery-item'>\n\t\t\t<dt class='gallery-icon landscape'>\n\t\t\t\t<a href='http:\/\/andyfiedler.com\/?attachment_id=225'><img width=\"150\" height=\"150\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130716_083641-150x150.jpg\" class=\"attachment-thumbnail\" alt=\"Great swimming hole outside Luang Prabang.\" aria-describedby=\"gallery-6-225\" \/><\/a>\n\t\t\t<\/dt>\n\t\t\t\t<dd class='wp-caption-text gallery-caption' id='gallery-6-225'>\n\t\t\t\tGreat swimming hole outside Luang Prabang.\n\t\t\t\t<\/dd><\/dl><br style=\"clear: both\" \/>\n\t\t<\/div>\n\n<p>The last long day of riding will take you 259 km from Ponsavan to Luang Prabang (backtracking to Pho Khoun, then north to Luang Prabang). Backtracking in this way isn&#8217;t all that bad because the scenery and riding is so great heading from Pho Khoun to Ponsavan, you&#8217;ll have no problem doing it twice.<\/p>\n<p>Pho Khoun makes a good stopping place for lunch, as its about at the midpoint of this ride and there are a few places to refuel and grab lunch. From Pho Khoun, you&#8217;ll have similar riding (and that&#8217;s to say great!)\u00a0to Luang Prabang.<\/p>\n<p><a href=\"http:\/\/wikitravel.org\/en\/Luang_Prabang\">Luang Prabang<\/a> is a UNESCO World Heritage Site and has by far the best developed tourist infrastructure of the three cities we visited. Luang Prabang is at the intersection of the Mekong River and the Nam Khan River and has a large temple in the center of the city with great views of both rivers. There are waterfalls about 20km outside of town as well as rafting, elephant tours, and kayaking if you choose to stay here for a few days.<\/p>\n<div id=\"attachment_196\" style=\"width: 570px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/laos_ride_profile.png\"><img class=\"size-large wp-image-196 \" title=\"Elevation Profile\" alt=\"Route: Vientiane - Vang Vieng- Pho Khoun - Ponsavan - Phou Khoun - Luang Prabang\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/laos_ride_profile-1024x696.png\" width=\"560\" height=\"380\" \/><\/a><p class=\"wp-caption-text\">Elevation Profile: Vientiane &#8211; Vang Vieng &#8211; Pho Khoun &#8211; Ponsavan &#8211; Phou Khoun &#8211; Luang Prabang<\/p><\/div>\n","image":"http:\/\/andyfiedler.com\/wp-content\/uploads\/2013\/08\/IMG_20130712_134937.jpg","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 13 Aug 2013 12:16:59 +0000","created_by":1,"updated_at":"Tue, 13 Aug 2013 12:16:59 +0000","updated_by":1,"published_at":"Tue, 13 Aug 2013 12:16:59 +0000","published_by":1},{"id":237,"title":"Tech Predictions for 2014","slug":"tech-predictions-for-2014","markdown":"\nThis is a just a bunch of fun tech predictions for 2014. I can\u2019t claim any insider knowledge, but it will be interesting to look back on them at the end of the year to see what the outcomes are!\n\n\n## Bitcoin will become\u00a0\u201clegitimate\u201d, but not widespread\n\nIn 2014, regulators will likely issue guidance on how exactly companies and people can deal in Bitcoin without running afoul of anti-money laundering and money transmitter regulations, and how it is to be taxed. This will \u201clegitimize\u201d Bitcoin, but because of the difficulty of using the cryptocurrency and the insane [amount of volatility of Bitcoin](http:\/\/www.businessinsider.com\/bitcoin-volatility-2013-12 \"Bitcoin Volatility - Business Insider\") against fiat money, it will not become mainstream in 2014. Instead, Bitcoin will be relegated to particular types of transactions that the mainstream banks are either not accepting (due to the high probability of fraud or high compliance costs) or are charging exorbitant fees to facilitate. These will likely be cross-border transactions, especially in countries with capital controls (Argentina), [legally grey-area transactions](http:\/\/www.nytimes.com\/2014\/01\/12\/us\/banks-say-no-to-marijuana-money-legal-or-not.html \"Banks Say No to Marijuana Money, Legal or Not - NY Times\"), or transactions that the major payment processors don\u2019t accept.\n\n\n## Docker plus Deis or Flynn will result in an\u00a0\u201copen-source Heroku\u201d and lots of dev-ops innovation\n\nHeroku is probably one of the biggest dev-ops innovations in the last five years, making it vastly easier to deploy web applications, freeing development teams to actually build applications instead of focusing on DevOps. Heroku\u2019s [12 Factor App](http:\/\/12factor.net) architecture is now widely used and 2014 will see that continuing. However, Heroku has a few problems.\n\nFirst, There currently isn\u2019t a good way to \u201cbuild your own Heroku\u201d out of open source components. If Heroku is too constraining, you are forced to spin up your own servers on Amazon Web Services for part of your application, which eliminates lots of the advantages Heroku brings to the table. Last year there was a ton of excitement about Linux containers (LXC) and Docker, which is abstraction on top of LXC that makes them easier to manage. Both Heroku and Google Cloud use Linux containers internally, but they are closed-source implementations. Docker will likely begin to change that this year.\n\nHowever, Docker alone is not a Heroku replacement. Much of the innovation in Heroku lies in the build packs and routing mesh, which Docker does not provide. Two other open source projects aim to become that layer on top of Docker, and these are the ones I\u2019m most excited to watch. The first is [Deis](http:\/\/deis.io\/), which seems to be the furthest along in creating an open-source Heroku. Deis has both a routing mesh and an application layer created as well as scripts to automatically scale your \u201cpersonal Heroku\u201d on AWS, Digital Ocean and Rackspace. Flynn has many of the same goals, but doesn\u2019t appear to be as far along. Deis has commercial support from Opsware, while [Flynn is raising money Kickstarter-style](https:\/\/flynn.io\/) to build out their platform. In any case, while Heroku is great, it is very exciting to see open source competitors come to the scene.\n\n\n## AngularJS and EmberJS will win the front-end wars for web apps\n\nFor highly interactive web apps, both AngularJS and EmberJS will become the clear choices in 2014. Backbone, Knockout and other front-end JS frameworks will see declining usage simply because they don\u2019t provide as much of a framework as AngularJS or EmberJS. For new sites except for \u201cCMS-like\u201d apps, people will stop generating HTML on the server and push the page rendering to Javascript on the browser. Because of this, back-end frameworks will pivot towards being better REST API servers and focus less on their HTML template rendering abilities. The wildcard is \u201cCMS-style\u201d sites that need to be indexed by Google. While Google\u2019s crawler can execute Javascript, content-heavy sites will still need a mechanism to serve HTML from the server for reliable SEO. This means that full-stack Rails apps will still be important in 2014. I think the writing is on the wall for this kind of app, however.\n\n\n## Mobile will continue to be\u00a0\u201cwrite twice\u201d for the highest quality\n\nUnfortunately, while HTML5 is great, it still won\u2019t deliver the highest quality apps on mobile in 2014. As a cost-saving measure, or for apps that don\u2019t need lots of interaction, HTML5 will be a viable choice. However, to create the highest-quality mobile apps in 2014, we\u2019ll still need to write them twice: once for Android and once for iOS.\n\n\n## Wearable tech won\u2019t be mainstream; in fact, society will push back from being\u00a0\u201calways connected\u201d\n\nGoogle Glass and the like will remain curiosities and not mainstream. In fact, I think that people are beginning to push back from being always connected to the Internet. Smartphone usage in many social situations is become a faux pas\u00a0and the word [\u201cglassholes\u201d](http:\/\/www.wired.com\/gadgetlab\/2013\/12\/glasshole\/ \"I, Glasshole\") has already been coined for people that wear Google Glass in public. That being said, we will see the Internet smartly integrated into more consumer products, including continued innovation in automobile technology and home automation. The key for the \u201cInternet of things\u201d in 2014 will be *unobtrusive, discrete, and large value-add*, which probably isn\u2019t wearable technology in its current form.\n\n\n","html":"<p>This is a just a bunch of fun tech predictions for 2014. I can&#8217;t claim any insider knowledge, but it will be interesting to look back on them at the end of the year to see what the outcomes are!<\/p>\n<h2>Bitcoin will become\u00a0&#8220;legitimate&#8221;, but not widespread<\/h2>\n<p>In 2014, regulators will likely issue guidance on how exactly companies and people can deal in Bitcoin without running afoul of anti-money laundering and money transmitter regulations, and how it is to be taxed. This will \u201clegitimize\u201d Bitcoin, but because of the difficulty of using the cryptocurrency and the insane <a title=\"Bitcoin Volatility - Business Insider\" href=\"http:\/\/www.businessinsider.com\/bitcoin-volatility-2013-12\" target=\"_blank\">amount of volatility of Bitcoin<\/a> against fiat money, it will not become mainstream in 2014. Instead, Bitcoin will be relegated to particular types of transactions that the mainstream banks are either not accepting (due to the high probability of fraud or high compliance costs) or are charging exorbitant fees to facilitate. These will likely be cross-border transactions, especially in countries with capital controls (Argentina), <a title=\"Banks Say No to Marijuana Money, Legal or Not - NY Times\" href=\"http:\/\/www.nytimes.com\/2014\/01\/12\/us\/banks-say-no-to-marijuana-money-legal-or-not.html\" target=\"_blank\">legally grey-area transactions<\/a>, or transactions that the major payment processors don\u2019t accept.<\/p>\n<h2>Docker plus Deis or Flynn will result in an\u00a0&#8220;open-source Heroku&#8221; and lots of dev-ops innovation<\/h2>\n<p>Heroku is probably one of the biggest dev-ops innovations in the last five years, making it vastly easier to deploy web applications, freeing development teams to actually build applications instead of focusing on DevOps. Heroku\u2019s <a href=\"http:\/\/12factor.net\">12 Factor App<\/a> architecture is now widely used and 2014 will see that continuing. However, Heroku has a few problems.<\/p>\n<p>First, There currently isn\u2019t a good way to &#8220;build your own Heroku&#8221; out of open source components. If Heroku is too constraining, you are forced to spin up your own servers on Amazon Web Services for part of your application, which eliminates lots of the advantages Heroku brings to the table. Last year there was a ton of excitement about Linux containers (LXC) and Docker, which is abstraction on top of LXC that makes them easier to manage. Both Heroku and Google Cloud use Linux containers internally, but they are closed-source implementations. Docker will likely begin to change that this year.<\/p>\n<p>However, Docker alone is not a Heroku replacement. Much of the innovation in Heroku lies in the build packs and routing mesh, which Docker does not provide. Two other open source projects aim to become that layer on top of Docker, and these are the ones I\u2019m most excited to watch. The first is <a href=\"http:\/\/deis.io\/\" target=\"_blank\">Deis<\/a>, which seems to be the furthest along in creating an open-source Heroku. Deis has both a routing mesh and an application layer created as well as scripts to automatically scale your \u201cpersonal Heroku\u201d on AWS, Digital Ocean and Rackspace. Flynn has many of the same goals, but doesn\u2019t appear to be as far along. Deis has commercial support from Opsware, while <a href=\"https:\/\/flynn.io\/\" target=\"_blank\">Flynn is raising money Kickstarter-style<\/a> to build out their platform. In any case, while Heroku is great, it is very exciting to see open source competitors come to the scene.<\/p>\n<h2>AngularJS and EmberJS will win the front-end wars for web apps<\/h2>\n<p>For highly interactive web apps, both AngularJS and EmberJS will become the clear choices in 2014. Backbone, Knockout and other front-end JS frameworks will see declining usage simply because they don\u2019t provide as much of a framework as AngularJS or EmberJS. For new sites except for \u201cCMS-like\u201d apps, people will stop generating HTML on the server and push the page rendering to Javascript on the browser. Because of this, back-end frameworks will pivot towards being better REST API servers and focus less on their HTML template rendering abilities. The wildcard is \u201cCMS-style\u201d sites that need to be indexed by Google. While Google\u2019s crawler can execute Javascript, content-heavy sites will still need a mechanism to serve HTML from the server for reliable SEO. This means that full-stack Rails apps will still be important in 2014. I think the writing is on the wall for this kind of app, however.<\/p>\n<h2>Mobile will continue to be\u00a0&#8220;write twice&#8221; for the highest quality<\/h2>\n<p>Unfortunately, while HTML5 is great, it still won\u2019t deliver the highest quality apps on mobile in 2014. As a cost-saving measure, or for apps that don\u2019t need lots of interaction, HTML5 will be a viable choice. However, to create the highest-quality mobile apps in 2014, we\u2019ll still need to write them twice: once for Android and once for iOS.<\/p>\n<h2>Wearable tech won\u2019t be mainstream; in fact, society will push back from being\u00a0&#8220;always connected&#8221;<\/h2>\n<p>Google Glass and the like will remain curiosities and not mainstream. In fact, I think that people are beginning to push back from being always connected to the Internet. Smartphone usage in many social situations is become a faux pas\u00a0and the word <a title=\"I, Glasshole\" href=\"http:\/\/www.wired.com\/gadgetlab\/2013\/12\/glasshole\/\" target=\"_blank\">\u201cglassholes\u201d<\/a> has already been coined for people that wear Google Glass in public. That being said, we will see the Internet smartly integrated into more consumer products, including continued innovation in automobile technology and home automation. The key for the \u201cInternet of things\u201d in 2014 will be <i>unobtrusive, discrete, and large value-add<\/i>, which probably isn\u2019t wearable technology in its current form.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 28 Jan 2014 16:42:19 +0000","created_by":1,"updated_at":"Tue, 28 Jan 2014 16:42:19 +0000","updated_by":1,"published_at":"Tue, 28 Jan 2014 16:42:19 +0000","published_by":1},{"id":239,"title":"AngularJS and Rails - Making a skeleton app","slug":"temp-slug-25","markdown":"\nIf you asked me what today what the best architecture for an interactive single-page web app was, I\u2019d definitely tell you to check out AngularJS for the front-end and Rails for the backend.\n\nUsing these two frameworks together can be a bit of a challenge, though. Rails is a very integrated framework that is typically used for traditional web apps that deliver HTML to the browser (as opposed to single-page apps). It includes Sprockets (the asset pipeline) which handles many of the same tasks that Angular\u2019s build system also handles, such as Javascript minification and compiling templates.\n\n\u00a0\n\n\n","html":"<p>If you asked me what today what the best architecture for an interactive single-page web app was, I&#8217;d definitely tell you to check out AngularJS for the front-end and Rails for the backend.<\/p>\n<p>Using these two frameworks together can be a bit of a challenge, though. Rails is a very integrated framework that is typically used for traditional web apps that deliver HTML to the browser (as opposed to single-page apps). It includes Sprockets (the asset pipeline) which handles many of the same tasks that Angular&#8217;s build system also handles, such as Javascript minification and compiling templates.<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 07 Feb 2014 18:28:20 +0000","created_by":1,"updated_at":"Fri, 07 Feb 2014 18:28:20 +0000","updated_by":1,"published_at":"","published_by":1},{"id":241,"title":"AngularJS and Rails - ","slug":"temp-slug-26","markdown":"","html":"","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 13:15:53 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 13:15:53 +0000","updated_by":1,"published_at":"","published_by":1},{"id":242,"title":"AngularJS and Rails - Introduction (Part 1)","slug":"temp-slug-27","markdown":"\nI\u2019ve recently been working a lot with Rails and AngularJS and I am really excited about seeing these two technologies used together. Rails has been around for a while and is still a highly productive framework, but I think the \u201cfuture\u201d is with sophisticated Javascript frameworks backed by simple REST APIs on the server. This architecture has a ton of advantages, including good scalability with services like Heroku, clean separation of concerns for testing, the ability to use the same API on a mobile app, and the potential for a great, highly-interactive user interface in the browser.\n\nTurns out that we can keep Rails, our mainstay, around and couple it with AngularJS to achieve this. The only problem is that integrating these two frameworks into a coherent solution can be rather difficult at first. Rails, by its nature as an \u201copinionated\u201d framework, makes a lot of assumptions about how you app will be built that don\u2019t apply to an AngularJS plus Rails-based JSON REST API application.\n\nIn this tutorial, I\u2019ll show you one way to setup a Rails app coupled with an AngularJS app that makes development fairly simple, and I\u2019ll also explain how you can deploy it to Heroku.\n\n<div class=\"wp-caption aligncenter\" id=\"attachment_243\" style=\"width: 310px\">[![An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png)](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png)Nutrition Data lookup app created with AngularJS and Rails to demonstrate how to integrate these two technologies. [See it running here.](http:\/\/nutrition-data.herokuapp.com \"Nutrition Data App\")\n\n<\/div>The biggest problem to solve with using Rails and AngularJS together is creating a coherent build system for the front-end. With AngularJS, your front-end app is more than just a few Javascript files added into your HTML, which is basically what the Rails Asset Pipeline is designed to handle. Instead, your front-end code will turn into a complete application that really needs its own build system, tests, and deployment infrastructure.\u00a0Luckily, front-end development is starting to get some really good \u201cfront-end native\u201d solutions to these problem with Bower, Yeoman, and Grunt.\n\n- **Yeoman**: Front-end scaffolding. This creates front-end apps from templates, including an AngularJS template maintained by the Angular team.\n- **Bower**: Front-end dependency management, similar to NPM or RubyGems.\n- **Grunt**: Front-end build process automation. Grunt has many plugins to handle lots of front-end build tasks from JS and CSS minification to running tests.\n\n\nIn this tutorial, we\u2019ll\n\n\u00a0\n\n\n","html":"<p>I&#8217;ve recently been working a lot with Rails and AngularJS and I am really excited about seeing these two technologies used together. Rails has been around for a while and is still a highly productive framework, but I think the &#8220;future&#8221; is with sophisticated Javascript frameworks backed by simple REST APIs on the server. This architecture has a ton of advantages, including good scalability with services like Heroku, clean separation of concerns for testing, the ability to use the same API on a mobile app, and the potential for a great, highly-interactive user interface in the browser.<\/p>\n<p>Turns out that we can keep Rails, our mainstay, around and couple it with AngularJS to achieve this. The only problem is that integrating these two frameworks into a coherent solution can be rather difficult at first. Rails, by its nature as an &#8220;opinionated&#8221; framework, makes a lot of assumptions about how you app will be built that don&#8217;t apply to an AngularJS plus Rails-based JSON REST API application.<\/p>\n<p>In this tutorial, I&#8217;ll show you one way to setup a Rails app coupled with an AngularJS app that makes development fairly simple, and I&#8217;ll also explain how you can deploy it to Heroku.<\/p>\n<div id=\"attachment_243\" style=\"width: 310px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png\"><img class=\"size-medium wp-image-243\" alt=\"An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png\" width=\"300\" height=\"256\" \/><\/a><p class=\"wp-caption-text\">Nutrition Data lookup app created with AngularJS and Rails to demonstrate how to integrate these two technologies. <a title=\"Nutrition Data App\" href=\"http:\/\/nutrition-data.herokuapp.com\" target=\"_blank\">See it running here.<\/a><\/p><\/div>\n<p>The biggest problem to solve with using Rails and AngularJS together is creating a coherent build system for the front-end. With AngularJS, your front-end app is more than just a few Javascript files added into your HTML, which is basically what the Rails Asset Pipeline is designed to handle. Instead, your front-end code will turn into a complete application that really needs its own build system, tests, and deployment infrastructure.\u00a0Luckily, front-end development is starting to get some really good &#8220;front-end native&#8221; solutions to these problem with Bower, Yeoman, and Grunt.<\/p>\n<ul>\n<ul>\n<li><strong>Yeoman<\/strong>: Front-end scaffolding. This creates front-end apps from templates, including an AngularJS template maintained by the Angular team.<\/li>\n<li><strong>Bower<\/strong>: Front-end dependency management, similar to NPM or RubyGems.<\/li>\n<li><strong>Grunt<\/strong>: Front-end build process automation. Grunt has many plugins to handle lots of front-end build tasks from JS and CSS minification to running tests.<\/li>\n<\/ul>\n<\/ul>\n<p>In this tutorial, we&#8217;ll<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 13:58:44 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 13:58:44 +0000","updated_by":1,"published_at":"","published_by":1},{"id":244,"title":"AngularJS and Rails Tutorial","slug":"angularjs-and-rails-tutorial","markdown":"\nIn this tutorial, I\u2019ll show you one way to setup a Rails app coupled with an AngularJS app that makes development fairly simple, and I\u2019ll also explain how you can deploy it to Heroku.\n\n<div class=\"wp-caption aligncenter\" id=\"attachment_243\" style=\"width: 310px\">[![An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png)](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png)An app created with AngularJS and Rails to demonstrate how to integrate these two technologies. [See it running here.](http:\/\/nutrition-data.herokuapp.com \"Nutrition Data App\")\n\n<\/div>The biggest problem to solve with using Rails and AngularJS together is creating a coherent build system for the front-end. With AngularJS, your front-end app is more than just a few Javascript files added into your HTML, which is basically what the Rails Asset Pipeline is designed to handle. Instead, your front-end code will turn into a complete application that really needs its own build system, tests, and deployment infrastructure.\u00a0Luckily, front-end development is starting to get some really good \u201cfront-end native\u201d solutions to these problem with Bower, Yeoman, and Grunt.\n\n- **Yeoman**: Front-end scaffolding. This creates front-end apps from templates, including an AngularJS template maintained by the Angular team.\n- **Bower**: Front-end dependency management, similar to NPM or RubyGems.\n- **Grunt**: Front-end build process automation. Grunt has many plugins to handle lots of front-end build tasks from JS and CSS minification to running tests.\n\nIn this tutorial, we\u2019ll use Yeoman to create an AngularJS application. We\u2019ll embed that application in a Rails app created using Rails-API, a stripped-down, optimized version of Rails that is designed for serving only APIs. Finally, we\u2019ll see how to deploy it on Heroku. In the future, I hope to add some details on how to handle authentication with AngularJS.\n\n\n## What you\u2019ll need to do this tutorial\n\n- Some Rails knowledge and Rails installed on your computer. RVM is a good idea, too.\n- NodeJS, NPM, and Grunt installed globally\n- A basic understanding of Javascript and AngularJS\n\nReady to get started? [See the first part here!](http:\/\/andyfiedler.com\/projects\/angularjs-and-rails-tutorial\/making-the-scaffold-rails-api-and-yeoman\/ \"Making the Scaffold: Rails-API and Yeoman\")\n\n\n","html":"<p>In this tutorial, I&#8217;ll show you one way to setup a Rails app coupled with an AngularJS app that makes development fairly simple, and I&#8217;ll also explain how you can deploy it to Heroku.<\/p>\n<div id=\"attachment_243\" style=\"width: 310px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png\"><img class=\"size-medium wp-image-243\" alt=\"An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png\" width=\"300\" height=\"256\" \/><\/a><p class=\"wp-caption-text\">An app created with AngularJS and Rails to demonstrate how to integrate these two technologies. <a title=\"Nutrition Data App\" href=\"http:\/\/nutrition-data.herokuapp.com\" target=\"_blank\">See it running here.<\/a><\/p><\/div>\n<p>The biggest problem to solve with using Rails and AngularJS together is creating a coherent build system for the front-end. With AngularJS, your front-end app is more than just a few Javascript files added into your HTML, which is basically what the Rails Asset Pipeline is designed to handle. Instead, your front-end code will turn into a complete application that really needs its own build system, tests, and deployment infrastructure.\u00a0Luckily, front-end development is starting to get some really good &#8220;front-end native&#8221; solutions to these problem with Bower, Yeoman, and Grunt.<\/p>\n<ul>\n<li><strong>Yeoman<\/strong>: Front-end scaffolding. This creates front-end apps from templates, including an AngularJS template maintained by the Angular team.<\/li>\n<li><strong>Bower<\/strong>: Front-end dependency management, similar to NPM or RubyGems.<\/li>\n<li><strong>Grunt<\/strong>: Front-end build process automation. Grunt has many plugins to handle lots of front-end build tasks from JS and CSS minification to running tests.<\/li>\n<\/ul>\n<p>In this tutorial, we&#8217;ll use Yeoman to create an AngularJS application. We&#8217;ll embed that application in a Rails app created using Rails-API, a stripped-down, optimized version of Rails that is designed for serving only APIs. Finally, we&#8217;ll see how to deploy it on Heroku. In the future, I hope to add some details on how to handle authentication with AngularJS.<\/p>\n<h2>What you&#8217;ll need to do this tutorial<\/h2>\n<ul>\n<li>Some Rails knowledge and Rails installed on your computer. RVM is a good idea, too.<\/li>\n<li>NodeJS, NPM, and Grunt installed globally<\/li>\n<li>A basic understanding of Javascript and AngularJS<\/li>\n<\/ul>\n<p>Ready to get started? <a title=\"Making the Scaffold: Rails-API and Yeoman\" href=\"http:\/\/andyfiedler.com\/projects\/angularjs-and-rails-tutorial\/making-the-scaffold-rails-api-and-yeoman\/\">See the first part here!<\/a><\/p>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 14:06:17 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 16:23:30 +0000","updated_by":1,"published_at":"","published_by":1},{"id":245,"title":"Making the Scaffold: Rails API and Yeoman","slug":"making-the-scaffold-rails-api-and-yeoman","markdown":"\nThe first thing we\u2019ll need to do is create a new Rails app, except we won\u2019t use the normal distribution of Rails. We will use [Rails API](https:\/\/github.com\/rails-api\/rails-api \"Rails-API Gem\") instead, which is a stripped down version of Rails without all of the view rendering code that is normally included in Rails.\n\nInstall Rails API and create a new application:\n\ngem install rails-api rails-api new nutrition_data --skip-sprockets --skip-test-unit --database=postgresql\n\nWe are not going to use the Rails asset pipeline, so we are excluding Sprockets. I also prefer RSpec over Test::Unit, so I\u2019ve skipped that. We also want to use Postgres specifically because that is what will be used on Heroku.\n\nOnce you\u2019ve created your Rails API application, change into the `nutrition_data`\u00a0directory and create a new folder to hold your AngularJS application, install Yeoman with NPM (`-g`\u00a0 means to install globally), and generate a scaffold AngularJS application.\n\ncd nutrition_data mkdir frontend npm install yo -g yo angular\n\nYeoman will ask you a few questions as it runs to customize your application scaffold. You\u2019ll want to answer \u201cYes\u201d to including Sass, Bootstrap, and Bootstrap with Sass (these three questions were all that were required at the time of writing). Next, Yeoman will ask which packages you\u2019d like to install. Choose all of the available options (four at the time of writing).\n\nSince we included Sass, we\u2019ll need the Compass gem installed. Luckily we are building a Rails app too, so we can simply include it in the Rails app\u2019s Gemfile by adding the line `gem 'compass'`\u00a0and running `bundle install`.\n\nOnce you have installed Compass, make sure you are in the\u00a0`frontend` directory and run `grunt serve`. This will compile you AngularJS app and start a simple static file server on `localhost:9000`. It should also open a browser window if your operating system supports it. If not, open a browser and go to `http:\/\/localhost:9000`. You should see a Yeoman AngularJS sample app!\n\nYour AngularJS sample app does not make any API calls to your Rails API at this point. Before we can do that, we need to set up a simple proxy in Grunt to avoid cross-origin request problems because in development mode, your Rails app will be running on a separate origin (`localhost:3000` by default). In production, we\u2019ll avoid this problem by having the Rails server serve both the static application files and the API.\n\nTo get this to work, we\u2019ll use [grunt-connect-proxy](https:\/\/github.com\/drewzboto\/grunt-connect-proxy \"Grunt-Connect-Proxy\")\u00a0to proxy requests that start with `http:\/\/localhost:9000\/api`\u00a0to `http:\/\/localhost:3000\/api`. Install grunt-connect-proxy by running `npm install grunt-connect-proxy`\u00a0in the `frontend`\u00a0folder. Then follow the directions on GitHub for configuring your Gruntfile. You can also see the [final Gruntfile](https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/Gruntfile.js) for this project and copy the needed sections.\n\nWhen you are finished setting up the proxy, make sure that both `grunt serve` and `rails server` are running and try to access `http:\/\/localhost:9000\/test`. You should see a Rails-style error message. If you do, then you are in business!\n\n\n","html":"<p>The first thing we&#8217;ll need to do is create a new Rails app, except we won&#8217;t use the normal distribution of Rails. We will use <a title=\"Rails-API Gem\" href=\"https:\/\/github.com\/rails-api\/rails-api\">Rails API<\/a> instead, which is a stripped down version of Rails without all of the view rendering code that is normally included in Rails.<\/p>\n<p>Install Rails API and create a new application:<\/p>\n<pre class=\"toolbar:2 show-lang:2 marking:false ranges:false nums:false lang:default highlight:0 decode:true\">gem install rails-api\r\nrails-api new nutrition_data --skip-sprockets --skip-test-unit --database=postgresql<\/pre>\n<p>We are not going to use the Rails asset pipeline, so we are excluding Sprockets. I also prefer RSpec over Test::Unit, so I&#8217;ve skipped that. We also want to use Postgres specifically because that is what will be used on Heroku.<\/p>\n<p>Once you&#8217;ve created your Rails API application, change into the <code>nutrition_data<\/code>\u00a0directory and create a new folder to hold your AngularJS application, install Yeoman with NPM (<code>-g<\/code>\u00a0 means to install globally), and generate a scaffold AngularJS application.<\/p>\n<pre class=\"toolbar:2 nums:false lang:default highlight:0 decode:true\">cd nutrition_data\r\nmkdir frontend\r\nnpm install yo -g\r\nyo angular<\/pre>\n<p>Yeoman will ask you a few questions as it runs to customize your application scaffold. You&#8217;ll want to answer &#8220;Yes&#8221; to including Sass, Bootstrap, and Bootstrap with Sass (these three questions were all that were required at the time of writing). Next, Yeoman will ask which packages you&#8217;d like to install. Choose all of the available options (four at the time of writing).<\/p>\n<p>Since we included Sass, we&#8217;ll need the Compass gem installed. Luckily we are building a Rails app too, so we can simply include it in the Rails app&#8217;s Gemfile by adding the line <code>gem 'compass'<\/code>\u00a0and running <code>bundle install<\/code>.<\/p>\n<p>Once you have installed Compass, make sure you are in the\u00a0<code>frontend<\/code> directory and run <code>grunt serve<\/code>. This will compile you AngularJS app and start a simple static file server on <code>localhost:9000<\/code>. It should also open a browser window if your operating system supports it. If not, open a browser and go to <code>http:\/\/localhost:9000<\/code>. You should see a Yeoman AngularJS sample app!<\/p>\n<p>Your AngularJS sample app does not make any API calls to your Rails API at this point. Before we can do that, we need to set up a simple proxy in Grunt to avoid cross-origin request problems because in development mode, your Rails app will be running on a separate origin (<code>localhost:3000<\/code> by default). In production, we&#8217;ll avoid this problem by having the Rails server serve both the static application files and the API.<\/p>\n<p>To get this to work, we&#8217;ll use <a title=\"Grunt-Connect-Proxy\" href=\"https:\/\/github.com\/drewzboto\/grunt-connect-proxy\" target=\"_blank\">grunt-connect-proxy<\/a>\u00a0to proxy requests that start with <code>http:\/\/localhost:9000\/api<\/code>\u00a0to <code>http:\/\/localhost:3000\/api<\/code>. Install grunt-connect-proxy by running <code>npm install grunt-connect-proxy<\/code>\u00a0in the <code>frontend<\/code>\u00a0folder. Then follow the directions on GitHub for configuring your Gruntfile. You can also see the <a href=\"https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/Gruntfile.js\" target=\"_blank\">final Gruntfile<\/a> for this project and copy the needed sections.<\/p>\n<p>When you are finished setting up the proxy, make sure that both <code>grunt serve<\/code> and <code>rails server<\/code> are running and try to access <code>http:\/\/localhost:9000\/test<\/code>. You should see a Rails-style error message. If you do, then you are in business!<\/p>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 14:06:43 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 14:57:56 +0000","updated_by":1,"published_at":"","published_by":1},{"id":249,"title":"Building a Simple Application - Rails Portion","slug":"temp-slug-30","markdown":"\nFor this sample app, we\u2019ll create a simple tool that shows the nutrition breakdown of lots of common foods. For the data, we\u2019ll use the USDA\u2019s National Nutrient Database for Standard Reference, which is has nutrition data on over 8000 different foods. This database comes in a complete format that has way more information than we need, so we\u2019ll use the abbreviated version that you can download from [this page](http:\/\/www.ars.usda.gov\/Services\/docs.htm?docid=23634 \"USDA Nutrient Database for Standard Reference 26\"). Select the abbreviated ASCII file (ABBRIV.txt).\n\nWe\u2019ll need to import this file into a database table on the Rails backend. To do this, I\u2019ve first created a new `Food` model in Rails and then [added all of the column names and data types to a constant](https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/app\/models\/food.rb \"Food model on GitHub\"). We can use that constant in a database migration to make the table.\n\nclass CreateFoods < ActiveRecord::Migration def change create_table :foods do |t| # Add the USDA fields Food::USDA_ABBRIV_FIELDS.each do |k,v| t.send(v, k) end t.timestamps end reversible do |t| t.up do change_column :foods, :ndb_number, :string, null: false add_index :foods, [:ndb_number], unique: true end end end end\n\nNext, we\u2019ll need to write some code to import the USDA data file. Since the USDA data file is rather small, I\u2019ve just added it to the app as `db\/ABBREV.txt`. Then, you can write a seeds.rb script like this to import it:\n\nfoods = [] File.open(Rails.root.to_s + \"\/db\/ABBREV.txt\", \"r:iso-8859-1\").each_line do |line| cols = line.split('^') food = {} Food::USDA_ABBRIV_FIELDS.each_with_index do |(k,v), i| val = cols[i].gsub('~','') case v when :string food[k] = val when :decimal if val.strip.empty? val = nil else val = val.to_d end food[k] = val end end foods << food end Food.create!(foods) puts \"Seeded #{foods.length} food(s).\"\n\nImport the data by running the `db:setup` rake task. When that is completed you should see that over 8000 foods have been seeded into the database.\n\nNow, we need to create a very simple JSON REST API to serve up nutrition information. On the front-end we want to have a search box where we can auto-complete food names and we also want an API endpoint to access the full information about a specific food. That means we\u2019ll need two separate representations of a Food model: a shortened version for the auto-complete and a full version to get if a user selects a food.\n\nTo serialize ActiveRecord models to JSON, we\u2019ll use [ActiveModel Serializers](https:\/\/github.com\/rails-api\/active_model_serializers \"ActiveModel Serializers Gem\"), which is suggested by Rails API. Add the `'active_model_serializers'` gem to your Gemfile and generate two new serializers.\n\nrails generate serializer Food rails generate serializer FoodShort\n\nThe short serializer is very simple because we only need to show the food name and the database ID:\n\nclass FoodShortSerializer < ActiveModel::Serializer attributes :id, :short_desc end\n\nThe full food serializer is a little more complex. To help organize the nutrient information, I\u2019ve grouped the nutrients into macronutrients, vitamins, and fats. The USDA data also has four fields for common servings and their weights (for example a slice of pizza, which weights 98 grams on average). I\u2019ve included code to turn those weights into an array to help out the front-end.\n\nclass FoodSerializer < ActiveModel::Serializer attributes :id, :short_desc, :vitamins, :macronutrients, :weights VITAMIN_FIELDS = [:calcium, :iron, :magnesium, :phosphorus, :sodium, :zinc, :copper, :manganese, :selenium, :vitamin_c, :thiamin, :riboflavin, :niacin, :panto_acid, :vitamin_b6, :total_folate, :folic_acid, :food_folate, :folate, :choline, :vitamin_b12, :vitamin_a_iu, :vitamin_a_rae, :retinol, :alpha_carot, :beta_carot, :lycopene, :lutein, :vitamin_e, :vitamin_d_mcg, :vitamin_d_iu, :vitamin_k] MACRO_FIELDS = [:kcal, :protein, :total_fat, :carbs, :total_fiber, :sugar, :cholesterol] FAT_FIELDS = [:saturated_fat, :monounsaturated_fat, :polyunsaturated_fat] def vitamins grouped_attributes(VITAMIN_FIELDS) end def macronutrients grouped_attributes(MACRO_FIELDS) end def fats grouped_attributes(MACRO_FIELDS) end def weights r = [] unless object.weight_1_desc.empty? r << { id: 1, name: object.weight_1_desc, weight: object.weight_1_grams.to_f } end unless object.weight_2_desc.empty? r << { id: 2, name: object.weight_2_desc, weight: object.weight_2_grams.to_f } end r end protected def grouped_attributes(list) list.map do |v| r = {} r[v] = object.send(v).to_f unless (object.send(v).nil? || object.send(v) == 0.0) r end.reduce(:merge) end end\n\nFinally, create a controller for these two API endpoints called FoodsController. Also add the routes to `config\/routes.rb`.\n\nclass FoodsController < ApplicationController def show @food = Food.find(params[:id]) render json: @food, serializer: FoodSerializer end def index @foods = Food.limit(25) if params[:short_desc].present? @foods = @foods.where(\"upper(short_desc) LIKE ?\", [params[:short_desc].upcase.gsub('%','') + '%']) end render json: @foods, each_serializer: FoodShortSerializer end end\n\nNutritionData::Application.routes.draw do scope(path: '\/api', defaults: {format: :json}) do resources :foods, only: [:index, :show] end end\n\nThat should finish up the Rails app! Later in the tutorial, I\u2019ll show you what configuration settings need to be changed when deploying the application to Heroku.\n\nFor now, continue on to the AngularJS application.\n\n\n","html":"<p>For this sample app, we&#8217;ll create a simple tool that shows the nutrition breakdown of lots of common foods. For the data, we&#8217;ll use the USDA&#8217;s National Nutrient Database for Standard Reference, which is has nutrition data on over 8000 different foods. This database comes in a complete format that has way more information than we need, so we&#8217;ll use the abbreviated version that you can download from <a title=\"USDA Nutrient Database for Standard Reference 26\" href=\"http:\/\/www.ars.usda.gov\/Services\/docs.htm?docid=23634\" target=\"_blank\">this page<\/a>. Select the abbreviated ASCII file (ABBRIV.txt).<\/p>\n<p>We&#8217;ll need to import this file into a database table on the Rails backend. To do this, I&#8217;ve first created a new <code>Food<\/code> model in Rails and then <a title=\"Food model on GitHub\" href=\"https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/app\/models\/food.rb\" target=\"_blank\">added all of the column names and data types to a constant<\/a>. We can use that constant in a database migration to make the table.<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"Food table database migration\">class CreateFoods &lt; ActiveRecord::Migration\r\n  def change\r\n    create_table :foods do |t|\r\n\r\n      # Add the USDA fields\r\n      Food::USDA_ABBRIV_FIELDS.each do |k,v|\r\n        t.send(v, k)\r\n      end\r\n\r\n      t.timestamps\r\n    end\r\n\r\n    reversible do |t|\r\n      t.up do\r\n        change_column :foods, :ndb_number, :string, null: false\r\n        add_index :foods, [:ndb_number], unique: true\r\n      end\r\n    end\r\n\r\n  end\r\n\r\nend<\/pre>\n<p>Next, we&#8217;ll need to write some code to import the USDA data file. Since the USDA data file is rather small, I&#8217;ve just added it to the app as <code>db\/ABBREV.txt<\/code>. Then, you can write a seeds.rb script like this to import it:<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"Seed script for USDA data\">foods = []\r\nFile.open(Rails.root.to_s + \"\/db\/ABBREV.txt\", \"r:iso-8859-1\").each_line do |line|\r\n  cols = line.split('^')\r\n  food = {}\r\n  Food::USDA_ABBRIV_FIELDS.each_with_index do |(k,v), i|\r\n    val = cols[i].gsub('~','')\r\n    case v\r\n      when :string\r\n        food[k] = val\r\n      when :decimal\r\n        if val.strip.empty?\r\n          val = nil\r\n        else\r\n          val = val.to_d\r\n        end\r\n        food[k] = val\r\n    end\r\n  end\r\n  foods &lt;&lt; food\r\nend\r\n\r\nFood.create!(foods)\r\n\r\nputs \"Seeded #{foods.length} food(s).\"<\/pre>\n<p>Import the data by running the <code>db:setup<\/code> rake task. When that is completed you should see that over 8000 foods have been seeded into the database.<\/p>\n<p>Now, we need to create a very simple JSON REST API to serve up nutrition information. On the front-end we want to have a search box where we can auto-complete food names and we also want an API endpoint to access the full information about a specific food. That means we&#8217;ll need two separate representations of a Food model: a shortened version for the auto-complete and a full version to get if a user selects a food.<\/p>\n<p>To serialize ActiveRecord models to JSON, we&#8217;ll use <a title=\"ActiveModel Serializers Gem\" href=\"https:\/\/github.com\/rails-api\/active_model_serializers\" target=\"_blank\">ActiveModel Serializers<\/a>, which is suggested by Rails API. Add the <code>'active_model_serializers'<\/code> gem to your Gemfile and generate two new serializers.<\/p>\n<pre class=\"toolbar:2 nums:false lang:default highlight:0 decode:true\">rails generate serializer Food\r\nrails generate serializer FoodShort<\/pre>\n<p>The short serializer is very simple because we only need to show the food name and the database ID:<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"FoodSerializer\">class FoodShortSerializer &lt; ActiveModel::Serializer\r\n  attributes :id, :short_desc\r\nend<\/pre>\n<p>The full food serializer is a little more complex. To help organize the nutrient information, I&#8217;ve grouped the nutrients into macronutrients, vitamins, and fats. The USDA data also has four fields for common servings and their weights (for example a slice of pizza, which weights 98 grams on average). I&#8217;ve included code to turn those weights into an array to help out the front-end.<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"Food serializer\">class FoodSerializer &lt; ActiveModel::Serializer\r\n  attributes :id, :short_desc, :vitamins, :macronutrients, :weights\r\n\r\n  VITAMIN_FIELDS = [:calcium, :iron, :magnesium, :phosphorus, :sodium, :zinc, :copper, :manganese,\r\n    :selenium, :vitamin_c, :thiamin, :riboflavin, :niacin, :panto_acid, :vitamin_b6, :total_folate,\r\n    :folic_acid, :food_folate, :folate, :choline, :vitamin_b12, :vitamin_a_iu, :vitamin_a_rae,\r\n    :retinol, :alpha_carot, :beta_carot, :lycopene, :lutein, :vitamin_e, :vitamin_d_mcg,\r\n    :vitamin_d_iu, :vitamin_k]\r\n\r\n  MACRO_FIELDS = [:kcal, :protein, :total_fat, :carbs, :total_fiber, :sugar, :cholesterol]\r\n\r\n  FAT_FIELDS = [:saturated_fat, :monounsaturated_fat, :polyunsaturated_fat]\r\n\r\n  def vitamins\r\n    grouped_attributes(VITAMIN_FIELDS)\r\n  end\r\n\r\n  def macronutrients\r\n    grouped_attributes(MACRO_FIELDS)\r\n  end\r\n\r\n  def fats\r\n    grouped_attributes(MACRO_FIELDS)\r\n  end\r\n\r\n  def weights\r\n    r = []\r\n    unless object.weight_1_desc.empty?\r\n      r &lt;&lt; { id: 1, name: object.weight_1_desc, weight: object.weight_1_grams.to_f }\r\n    end\r\n    unless object.weight_2_desc.empty?\r\n      r &lt;&lt; { id: 2, name: object.weight_2_desc, weight: object.weight_2_grams.to_f }\r\n    end\r\n    r\r\n  end\r\n\r\n  protected\r\n\r\n  def grouped_attributes(list)\r\n    list.map do |v|\r\n      r = {}\r\n      r[v] = object.send(v).to_f unless (object.send(v).nil? || object.send(v) == 0.0)\r\n      r\r\n    end.reduce(:merge)\r\n  end\r\n\r\nend<\/pre>\n<p>Finally, create a controller for these two API endpoints called FoodsController. Also add the routes to <code>config\/routes.rb<\/code>.<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"FoodsController (app\/controllers\/foods_controller.rb)\">class FoodsController &lt; ApplicationController\r\n  def show\r\n    @food = Food.find(params[:id])\r\n    render json: @food, serializer: FoodSerializer\r\n  end\r\n\r\n  def index\r\n    @foods = Food.limit(25)\r\n\r\n    if params[:short_desc].present?\r\n      @foods =\r\n        @foods.where(\"upper(short_desc) LIKE ?\", [params[:short_desc].upcase.gsub('%','') + '%'])\r\n    end\r\n\r\n    render json: @foods, each_serializer: FoodShortSerializer\r\n  end\r\nend<\/pre>\n<pre class=\"lang:ruby decode:true\" title=\"config\/routes.rb\">NutritionData::Application.routes.draw do\r\n  scope(path: '\/api', defaults: {format: :json}) do\r\n    resources :foods, only: [:index, :show]\r\n  end\r\nend<\/pre>\n<p>That should finish up the Rails app! Later in the tutorial, I&#8217;ll show you what configuration settings need to be changed when deploying the application to Heroku.<\/p>\n<p>For now, continue on to the AngularJS application.<\/p>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 15:31:45 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 15:31:45 +0000","updated_by":1,"published_at":"","published_by":1},{"id":250,"title":"Building a Simple Application - AngularJS Portion","slug":"temp-slug-31","markdown":"\n<div class=\"wp-caption aligncenter\" id=\"attachment_243\" style=\"width: 310px\">[![An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png)](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png)Nutrition Data finished application.\n\n<\/div>We going to need to add a three libraries to the front-end: Underscore.JS, Google Charts, and Select2 (for the auto-complete text box). Bower makes this really easy to get these libraries. In the frontend folder, there is a file called bower.json that contains all of the frontend dependencies. This file was created with Yeoman when you build the scaffold Angular app. Open it up and compare it to the file I have. I\u2019ve added three libraries and incremented the version of AngularJS to the latest release (which you might not need to do, depending on when you are doing this tutorial):\n\n{ \"name\": \"angular\", \"version\": \"0.0.0\", \"dependencies\": { \"angular\": \"1.2.12\", \"json3\": \"~3.2.6\", \"es5-shim\": \"~2.1.0\", \"jquery\": \"~1.10.2\", \"underscore\": \"1.5.2\", \"sass-bootstrap\": \"~3.0.2\", \"angular-resource\": \"1.2.12\", \"angular-cookies\": \"1.2.12\", \"angular-sanitize\": \"1.2.12\", \"angular-route\": \"1.2.12\", \"angular-ui-select2\": \"~0.0.5\", \"angular-google-chart\": \">=0.0.8\" }, \"devDependencies\": { \"angular-mocks\": \"1.2.12\", \"angular-scenario\": \"1.2.12\" } }\n\nOnce you have the Bower changes complete, run `bower install` in the `frontend` folder. Bower will fetch all of you needed JS dependencies and put them in the `bower_components` folder. Next, I added some custom CSS to make Select2 work well with Bootstrap. It is located in [`frontend\/app\/styles\/overrides\/select2-bootstrap.scss`](https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/app\/styles\/overrides\/select2-bootstrap.scss). Once you have these changes ready, we can move on to coding the front-end functionality.\n\nThis application is a very simple AngularJS app. There is only one controller and one template. Requests to the server do not use ngResource because they are simple HTTP GETs that need tight integration with Select2.\n\nFirst, let\u2019s look at the main template at frontend\/app\/views\/main.html. This has directives for Select2 (which is added to an input tag) and the Google Charts (which is added to a div tag). The rest is fairly standard AngularJS.\n\n<div class=\"header\"> <h3 class=\"text-muted\">Nutrition Data<\/h3> <\/div> <div class=\"row selector\"> <div class=\"col-md-12\"> <div class=\"form-group\"> <label>Start entering a food<\/label> <input type=\"text\" ui-select2=\"foodSelectOptions\" ng-model=\"selectedFood\" class=\"form-control\"> <\/div> <div class=\"row\" ng-hide=\"!selectedFood\"> <div class=\"col-md-7\"> <label>Serving amount<\/label> <select ui-select2 ng-model=\"selectedWeightId\" class=\"form-control\"> <option ng-repeat=\"w in selectedFood.weights\" value=\"{{w.id}}\">{{w.name}}<\/option> <\/select> <\/div> <div class=\"col-md-5\"> <label>Number of servings<\/label> <input type=\"text\" min=\"0\" ng-model=\"selectedQty\" class=\"form-control\"> <\/div> <\/div> <\/div> <\/div> <div class=\"row nutrients\" ng-show=\"!!selectedFood\"> <div class=\"col-md-5\"> <div google-chart chart=\"kcalBreakdownChart\" style=\"width:100%;height:300px;\"><\/div> <\/div> <div class=\"col-md-7\"> <table class=\"table\"> <caption>Nutrition Facts ({{ getGramAmount() }} grams)<\/caption> <thead> <tr> <th>Nutrient<\/th> <th>Amount<\/th> <\/tr> <\/thead> <tbody ng-show=\"!!selectedFood.macronutrients\"> <tr ng-repeat=\"(key,amt) in selectedFood.macronutrients\"> <td>{{ getNutrientNiceName(key) }}<\/td> <td>{{ getNutrientNiceAmount(key,amt) }}<\/td> <\/tr> <\/tbody> <tbody ng-show=\"!selectedFood.macronutrients\"> <tr> <td colspan=\"2\">No food selected.<\/td> <\/tr> <\/tbody> <\/table> <\/div> <\/div> <div class=\"footer\"> <p> Sample AngularJS App. For the source code, see the project on <a href=\"https:\/\/github.com\/afiedler\/nutrition_data\">GitHub<\/a>. <\/p> <\/div>\n\nThe controller that backs this template can be found in [`frontend\/app\/scripts\/controllers\/main.js`](https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/app\/scripts\/controllers\/main.js). A couple of things to point out here are that we don\u2019t need to use Angular\u2019s dependency annotations even though in production the code will be minified. We can do that because the Yeoman build script uses the ngMinification library, which will handle adding dependency annotations automatically.\n\nWe also need to make sure that we are not specifying the protocol, domain name, or port on our $http calls to the API. This is because the origin will change when the app is deployed. In both development and production modes, these origins will be the same as the origin that is serving the AngularJS application, so it should not be a huge issue.\n\n\n## How Bower Complies the Application\n\nBower provides good Javascript minification and concatenation support, which is seamlessly turned on and off in development and production modes, respectively. If you look in frontend\/app\/index.html, you\u2019ll see a few sections that have Bower annotations in comments like this:\n\n<!-- build:js scripts\/vendor.js --> <!-- bower:js --> <script src=\"bower_components\/jquery\/jquery.js\"><\/script> <script src=\"bower_components\/angular\/angular.js\"><\/script> <script src=\"bower_components\/underscore\/underscore.js\"><\/script> <!-- ... other <script> tags ... --> <!-- endbower --> <!-- endbuild -->\n\nThis section is managed by Bower, so don\u2019t try to include tags in it! What Bower is doing is including all of the needed Javascript files from your JS libraries automatically. To aid debugging, Bower includes the full source code in development mode. When you build the deployment version of this app, Bower will instead minify and concatenate these files into one file called `scripts\/XXXXX.vendor.js`, where XXXXX is a cache-busting key.\n\nBower will do the same for your applications JS and for both vendor and application CSS, which you can see the annotations for in other parts of index.html. The one thing that Bower will not handle (currently) is images or other assets included in your dependencies. In development mode, the lightweight Grunt server can usually find these files. However, when you deploy your application, you\u2019ll need to ensure that they are copied to the correct deployment directory. We\u2019ll see an example of this in the section on deploying to Heroku.\n\nContinue on to learn how to deploy this application to Heroku.\n\n\n","html":"<div id=\"attachment_243\" style=\"width: 310px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM.png\"><img class=\"size-medium wp-image-243\" alt=\"An app created with AngularJS and Rails to demonstrate how to integrate these two technologies.\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/Screen-Shot-2014-02-09-at-1.50.57-PM-300x256.png\" width=\"300\" height=\"256\" \/><\/a><p class=\"wp-caption-text\">Nutrition Data finished application.<\/p><\/div>\n<p>We going to need to add a three libraries to the front-end: Underscore.JS, Google Charts, and Select2 (for the auto-complete text box). Bower makes this really easy to get these libraries. In the frontend folder, there is a file called bower.json that contains all of the frontend dependencies. This file was created with Yeoman when you build the scaffold Angular app. Open it up and compare it to the file I have. I&#8217;ve added three libraries and incremented the version of AngularJS to the latest release (which you might not need to do, depending on when you are doing this tutorial):<\/p>\n<pre class=\"lang:js mark:9,15-16 decode:true\" title=\"bower.json\">{\r\n  \"name\": \"angular\",\r\n  \"version\": \"0.0.0\",\r\n  \"dependencies\": {\r\n    \"angular\": \"1.2.12\",\r\n    \"json3\": \"~3.2.6\",\r\n    \"es5-shim\": \"~2.1.0\",\r\n    \"jquery\": \"~1.10.2\",\r\n    \"underscore\": \"1.5.2\",\r\n    \"sass-bootstrap\": \"~3.0.2\",\r\n    \"angular-resource\": \"1.2.12\",\r\n    \"angular-cookies\": \"1.2.12\",\r\n    \"angular-sanitize\": \"1.2.12\",\r\n    \"angular-route\": \"1.2.12\",\r\n    \"angular-ui-select2\": \"~0.0.5\",\r\n    \"angular-google-chart\": \"&gt;=0.0.8\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"angular-mocks\": \"1.2.12\",\r\n    \"angular-scenario\": \"1.2.12\"\r\n  }\r\n}<\/pre>\n<p>Once you have the Bower changes complete, run <code>bower install<\/code> in the <code>frontend<\/code> folder. Bower will fetch all of you needed JS dependencies and put them in the <code>bower_components<\/code> folder. Next, I added some custom CSS to make Select2 work well with Bootstrap. It is located in <a href=\"https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/app\/styles\/overrides\/select2-bootstrap.scss\"><code>frontend\/app\/styles\/overrides\/select2-bootstrap.scss<\/code><\/a>. Once you have these changes ready, we can move on to coding the front-end functionality.<\/p>\n<p>This application is a very simple AngularJS app. There is only one controller and one template. Requests to the server do not use ngResource because they are simple HTTP GETs that need tight integration with Select2.<\/p>\n<p>First, let&#8217;s look at the main template at frontend\/app\/views\/main.html. This has directives for Select2 (which is added to an input tag) and the Google Charts (which is added to a div tag). The rest is fairly standard AngularJS.<\/p>\n<pre class=\"lang:xhtml mark:9-12,34 decode:true\" title=\"frontend\/app\/views\/main.html\">&lt;div class=\"header\"&gt;\r\n  &lt;h3 class=\"text-muted\"&gt;Nutrition Data&lt;\/h3&gt;\r\n&lt;\/div&gt;\r\n\r\n&lt;div class=\"row selector\"&gt;\r\n  &lt;div class=\"col-md-12\"&gt;\r\n    &lt;div class=\"form-group\"&gt;\r\n      &lt;label&gt;Start entering a food&lt;\/label&gt;\r\n      &lt;input type=\"text\"\r\n             ui-select2=\"foodSelectOptions\"\r\n             ng-model=\"selectedFood\"\r\n             class=\"form-control\"&gt;\r\n    &lt;\/div&gt;\r\n    &lt;div class=\"row\" ng-hide=\"!selectedFood\"&gt;\r\n      &lt;div class=\"col-md-7\"&gt;\r\n        &lt;label&gt;Serving amount&lt;\/label&gt;\r\n        &lt;select\r\n          ui-select2\r\n          ng-model=\"selectedWeightId\"\r\n          class=\"form-control\"&gt;\r\n          &lt;option ng-repeat=\"w in selectedFood.weights\" value=\"{{w.id}}\"&gt;{{w.name}}&lt;\/option&gt;\r\n        &lt;\/select&gt;\r\n      &lt;\/div&gt;\r\n      &lt;div class=\"col-md-5\"&gt;\r\n        &lt;label&gt;Number of servings&lt;\/label&gt;\r\n        &lt;input type=\"text\" min=\"0\" ng-model=\"selectedQty\" class=\"form-control\"&gt;\r\n      &lt;\/div&gt;\r\n    &lt;\/div&gt;\r\n  &lt;\/div&gt;\r\n&lt;\/div&gt;\r\n\r\n&lt;div class=\"row nutrients\" ng-show=\"!!selectedFood\"&gt;\r\n  &lt;div class=\"col-md-5\"&gt;\r\n    &lt;div google-chart chart=\"kcalBreakdownChart\" style=\"width:100%;height:300px;\"&gt;&lt;\/div&gt;\r\n  &lt;\/div&gt;\r\n  &lt;div class=\"col-md-7\"&gt;\r\n    &lt;table class=\"table\"&gt;\r\n      &lt;caption&gt;Nutrition Facts ({{ getGramAmount() }} grams)&lt;\/caption&gt;\r\n      &lt;thead&gt;\r\n        &lt;tr&gt;\r\n          &lt;th&gt;Nutrient&lt;\/th&gt;\r\n          &lt;th&gt;Amount&lt;\/th&gt;\r\n        &lt;\/tr&gt;\r\n      &lt;\/thead&gt;\r\n      &lt;tbody ng-show=\"!!selectedFood.macronutrients\"&gt;\r\n        &lt;tr ng-repeat=\"(key,amt) in selectedFood.macronutrients\"&gt;\r\n          &lt;td&gt;{{ getNutrientNiceName(key) }}&lt;\/td&gt;\r\n          &lt;td&gt;{{ getNutrientNiceAmount(key,amt) }}&lt;\/td&gt;\r\n        &lt;\/tr&gt;\r\n      &lt;\/tbody&gt;\r\n      &lt;tbody ng-show=\"!selectedFood.macronutrients\"&gt;\r\n        &lt;tr&gt;\r\n          &lt;td colspan=\"2\"&gt;No food selected.&lt;\/td&gt;\r\n        &lt;\/tr&gt;\r\n      &lt;\/tbody&gt;\r\n    &lt;\/table&gt;\r\n  &lt;\/div&gt;\r\n&lt;\/div&gt;\r\n\r\n&lt;div class=\"footer\"&gt;\r\n  &lt;p&gt;\r\n    Sample AngularJS App. For the source code, see the project on\r\n    &lt;a href=\"https:\/\/github.com\/afiedler\/nutrition_data\"&gt;GitHub&lt;\/a&gt;.\r\n  &lt;\/p&gt;\r\n&lt;\/div&gt;<\/pre>\n<p>The controller that backs this template can be found in <a href=\"https:\/\/github.com\/afiedler\/nutrition_data\/blob\/master\/frontend\/app\/scripts\/controllers\/main.js\"><code>frontend\/app\/scripts\/controllers\/main.js<\/code><\/a>. A couple of things to point out here are that we don&#8217;t need to use Angular&#8217;s dependency annotations even though in production the code will be minified. We can do that because the Yeoman build script uses the ngMinification library, which will handle adding dependency annotations automatically.<\/p>\n<p>We also need to make sure that we are not specifying the protocol, domain name, or port on our $http calls to the API. This is because the origin will change when the app is deployed. In both development and production modes, these origins will be the same as the origin that is serving the AngularJS application, so it should not be a huge issue.<\/p>\n<h2>How Bower Complies the Application<\/h2>\n<p>Bower provides good Javascript minification and concatenation support, which is seamlessly turned on and off in development and production modes, respectively. If you look in frontend\/app\/index.html, you&#8217;ll see a few sections that have Bower annotations in comments like this:<\/p>\n<pre class=\"lang:xhtml decode:true\" title=\"app\/index.html (Bower fragments)\">&lt;!-- build:js scripts\/vendor.js --&gt;\r\n&lt;!-- bower:js --&gt;\r\n&lt;script src=\"bower_components\/jquery\/jquery.js\"&gt;&lt;\/script&gt;\r\n&lt;script src=\"bower_components\/angular\/angular.js\"&gt;&lt;\/script&gt;\r\n&lt;script src=\"bower_components\/underscore\/underscore.js\"&gt;&lt;\/script&gt;\r\n&lt;!-- ... other &lt;script&gt; tags ... --&gt;\r\n&lt;!-- endbower --&gt;\r\n&lt;!-- endbuild --&gt;<\/pre>\n<p>This section is managed by Bower, so don&#8217;t try to include tags in it! What Bower is doing is including all of the needed Javascript files from your JS libraries automatically. To aid debugging, Bower includes the full source code in development mode. When you build the deployment version of this app, Bower will instead minify and concatenate these files into one file called <code>scripts\/XXXXX.vendor.js<\/code>, where XXXXX is a cache-busting key.<\/p>\n<p>Bower will do the same for your applications JS and for both vendor and application CSS, which you can see the annotations for in other parts of index.html. The one thing that Bower will not handle (currently) is images or other assets included in your dependencies. In development mode, the lightweight Grunt server can usually find these files. However, when you deploy your application, you&#8217;ll need to ensure that they are copied to the correct deployment directory. We&#8217;ll see an example of this in the section on deploying to Heroku.<\/p>\n<p>Continue on to learn how to deploy this application to Heroku.<\/p>\n","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 16:21:48 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 16:21:48 +0000","updated_by":1,"published_at":"","published_by":1},{"id":251,"title":"AngularJS and Rails Tutorial","slug":"temp-slug-32","markdown":"\n![AngularJS_Rails](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/AngularJS_Rails1.png)I\u2019ve recently been working a lot with Rails and AngularJS and I am really excited about seeing these two technologies used together. Rails has been around for a while and is still a highly productive framework, but I think the \u201cfuture\u201d is with sophisticated Javascript frameworks backed by simple REST APIs on the server. This architecture has a ton of advantages, including good scalability with services like Heroku, clean separation of concerns for testing, the ability to use the same API on mobile the browser, and the potential for a great, highly-interactive user interface in the browser.\n\nTurns out that we can keep Rails, our mainstay, around and couple it with AngularJS to achieve this. The only problem is that integrating these two frameworks into a coherent solution can be rather difficult at first. Rails, by its nature as an \u201copinionated\u201d framework, makes a lot of assumptions about how you app will be built that don\u2019t apply to an AngularJS plus Rails-based JSON REST API application.\n\nAfter a lot of work on the Literacy Design Collaborative\u2019s CoreTools application, which uses this architecture, I\u2019ve created a quick tutorial on how to use a similar architecture on your application. Check it out here.\n\n\n","html":"<p><img class=\"alignleft  wp-image-254\" style=\"border: 0px;\" alt=\"AngularJS_Rails\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/AngularJS_Rails1.png\" width=\"323\" height=\"149\" \/>I&#8217;ve recently been working a lot with Rails and AngularJS and I am really excited about seeing these two technologies used together. Rails has been around for a while and is still a highly productive framework, but I think the &#8220;future&#8221; is with sophisticated Javascript frameworks backed by simple REST APIs on the server. This architecture has a ton of advantages, including good scalability with services like Heroku, clean separation of concerns for testing, the ability to use the same API on mobile the browser, and the potential for a great, highly-interactive user interface in the browser.<\/p>\n<p>Turns out that we can keep Rails, our mainstay, around and couple it with AngularJS to achieve this. The only problem is that integrating these two frameworks into a coherent solution can be rather difficult at first. Rails, by its nature as an &#8220;opinionated&#8221; framework, makes a lot of assumptions about how you app will be built that don&#8217;t apply to an AngularJS plus Rails-based JSON REST API application.<\/p>\n<p>After a lot of work on the Literacy Design Collaborative&#8217;s CoreTools application, which uses this architecture, I&#8217;ve created a quick tutorial on how to use a similar architecture on your application. Check it out here.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 09 Feb 2014 16:49:19 +0000","created_by":1,"updated_at":"Sun, 09 Feb 2014 16:49:19 +0000","updated_by":1,"published_at":"","published_by":1},{"id":255,"title":"Volatility of Bitcoin Index","slug":"volatility-of-bitcoin-index","markdown":"\nA while back when I was a research assistant at the Federal Reserve, I worked on a project to make exchange rate volatility indexes for the major world currencies. We basically had some high frequency data for USD\/EUR, USD\/CHF, and USD\/JPY and wanted to see how the financial crisis affected volatility. With all of the hype and turmoil around Bitcoin, I though it would be interesting to make a similar index for the Bitcoin\/USD exchange rate.\n\nBefore Bitcoin is ever able to become a viable \u201ccurrency\u201d, volatility needs to come down a lot. Low volatility isn\u2019t sufficient for it to take off, but is probably is necessary. If you take the traditional definition of a currency as a \u201cstore of value\u201d, a \u201cmedium of exchange\u201d, and a \u201cunit of account\u201d, persistently high volatility is absolutely a death knell. This is especially true in Bitcoin\u2019s case where there is no government backing and there are attractive substitutes in traditional currencies.\n\nOne of the cool things about Bitcoin, however, is that lots of the data is fairly open. Most of the rest of the financial market data in the U.S. is behind the copyright lock and seal of the major exchanges and electronic trading networks. Both the NYSE and NASDAQ make lots of money off of selling market data, and they recently won [a court case to raise their rates even further](http:\/\/www.reuters.com\/article\/2013\/04\/30\/us-sec-exchanges-netcoalition-idUSBRE93T0Q220130430 \"SEC court case on market data fees\"). That makes doing this kind of analysis on other securities an expensive endeavor for armchair quarterbacks like myself!\n\nBitcoin, however, has a rather open ethos and most of the exchanges publish realtime or near-realtime price data for free. CoinDesk aggregates this into their [Bitcoin Price Index](http:\/\/www.coindesk.com\/price\/ \"Bitcoin Price Index\"), which they graciously agreed to send over for this analysis.\n\nThe Volatility of Bitcoin Index (VOB Index\u2014I\u2019m open to suggestions on the name) is a simple rolling standard deviation of minutely log-returns. That is not nearly as complicated as it sounds. To create the index, I started with a time series of minutely Bitcoin price data and calculated the [log returns](http:\/\/en.wikipedia.org\/wiki\/Rate_of_return#Comparing_ordinary_return_with_logarithmic_return \"Wikipedia - Log Returns\") (basically percent increase or decrease for each minute). \u00a0Then for each period, I took all of the returns within a trailing window and computed the standard deviation. I did this for three window lengths: 30, 60 and 90 days, and then annualized it. The Bitcoin markets are 24\/7\/365, so there are no holiday or weekend adjustments, which makes things a bit easier.\n\nHere\u2019s what the index looks like for the period August 2011 to January 31, 2014.\n\n![Volatility of Bitcoin Index](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/vob_plot.png)Here\u2019s the BPI (Bitcoin Price Index) over that period.\n\n![BPI Index](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/bpi_plot.png)  \n Bitcoin volatility looks like it is dropping over time, especially from the early days in 2011 and 2012, except for a large bump in volatility in April of 2013. That was probably sparked by problems at Bitcoin\u2019s largest exchange, Mt. Gox during that time period. This decrease is despite an increase in speculative interest in Bitcoin and moves by China to curtail the currency\u2019s use.\n\nWhether Bitcoin takes off is anyone\u2019s guess and predicting if it does is probably a fool\u2019s errand. There are lots of smart people working on it, but big questions about its viability still remain. In either case, it should be exciting to watch!\n\n\n","html":"<p>A while back when I was a research assistant at the Federal Reserve, I worked on a project to make exchange rate volatility indexes for the major world currencies. We basically had some high frequency data for USD\/EUR, USD\/CHF, and USD\/JPY and wanted to see how the financial crisis affected volatility. With all of the hype and turmoil around Bitcoin, I though it would be interesting to make a similar index for the Bitcoin\/USD exchange rate.<\/p>\n<p>Before Bitcoin is ever able to become a viable \u201ccurrency\u201d, volatility needs to come down a lot. Low volatility isn\u2019t sufficient for it to take off, but is probably is necessary. If you take the traditional definition of a currency as a \u201cstore of value\u201d, a \u201cmedium of exchange\u201d, and a \u201cunit of account\u201d, persistently high volatility is absolutely a death knell. This is especially true in Bitcoin\u2019s case where there is no government backing and there are attractive substitutes in traditional currencies.<\/p>\n<p>One of the cool things about Bitcoin, however, is that lots of the data is fairly open. Most of the rest of the financial market data in the U.S. is behind the copyright lock and seal of the major exchanges and electronic trading networks. Both the NYSE and NASDAQ make lots of money off of selling market data, and they recently won <a title=\"SEC court case on market data fees\" href=\"http:\/\/www.reuters.com\/article\/2013\/04\/30\/us-sec-exchanges-netcoalition-idUSBRE93T0Q220130430\">a court case to raise their rates even further<\/a>. That makes doing this kind of analysis on other securities an expensive endeavor for armchair quarterbacks like myself!<\/p>\n<p>Bitcoin, however, has a rather open ethos and most of the exchanges publish realtime or near-realtime price data for free. CoinDesk aggregates this into their <a title=\"Bitcoin Price Index\" href=\"http:\/\/www.coindesk.com\/price\/\">Bitcoin Price Index<\/a>, which they graciously agreed to send over for this analysis.<\/p>\n<p>The Volatility of Bitcoin Index (VOB Index\u2014I\u2019m open to suggestions on the name) is a simple rolling standard deviation of minutely log-returns. That is not nearly as complicated as it sounds. To create the index, I started with a time series of minutely Bitcoin price data and calculated the <a title=\"Wikipedia - Log Returns\" href=\"http:\/\/en.wikipedia.org\/wiki\/Rate_of_return#Comparing_ordinary_return_with_logarithmic_return\">log returns<\/a> (basically percent increase or decrease for each minute). \u00a0Then for each period, I took all of the returns within a trailing window and computed the standard deviation. I did this for three window lengths: 30, 60 and 90 days, and then annualized it. The Bitcoin markets are 24\/7\/365, so there are no holiday or weekend adjustments, which makes things a bit easier.<\/p>\n<p>Here\u2019s what the index looks like for the period August 2011 to January 31, 2014.<\/p>\n<p><img class=\"aligncenter size-full wp-image-256\" alt=\"Volatility of Bitcoin Index\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/vob_plot.png\" width=\"800\" height=\"462\" \/>Here\u2019s the BPI (Bitcoin Price Index) over that period.<\/p>\n<p><img class=\"aligncenter size-full wp-image-257\" alt=\"BPI Index\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/02\/bpi_plot.png\" width=\"800\" height=\"462\" \/><br \/>\nBitcoin volatility looks like it is dropping over time, especially from the early days in 2011 and 2012, except for a large bump in volatility in April of 2013. That was probably sparked by problems at Bitcoin\u2019s largest exchange, Mt. Gox during that time period. This decrease is despite an increase in speculative interest in Bitcoin and moves by China to curtail the currency\u2019s use.<\/p>\n<p>Whether Bitcoin takes off is anyone\u2019s guess and predicting if it does is probably a fool\u2019s errand. There are lots of smart people working on it, but big questions about its viability still remain. In either case, it should be exciting to watch!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 24 Feb 2014 15:31:32 +0000","created_by":1,"updated_at":"Mon, 24 Feb 2014 15:31:32 +0000","updated_by":1,"published_at":"Mon, 24 Feb 2014 15:31:32 +0000","published_by":1},{"id":260,"title":"Querying Inside Postgres JSON Arrays","slug":"querying-inside-postgres-json-arrays","markdown":"\nPostgres JSON support is pretty amazing. I\u2019ve been using it extensively for storing semi-structured data for a project and it has been great for that use case. In Postgres 9.3, the maintainers added the ability to perform some simple queries on JSON structures and a few functions to convert from JSON to Postgres arrays and result sets.\n\nOne feature that I couldn\u2019t figure out how to implement using the built-in Postgres functions was the ability to query within a JSON array. This is fairly critical for lots of the reporting queries that I\u2019ve been building over the part few days. Suppose you have some JSON like this, stored in two rows in a table called \u201corders\u201d, in the column \u201cjson_field\u201d:\n\n\/\/ Row 1, \"json_field\" column ----- { \"products\": [ { \"id\": 1, \"name\": \"Fish Tank\" }, { \"id\": 2, \"name\": \"Bird Feeder\" } ] } \/\/ Row 2, \"json_field\" column ----- { \"products\": [ { \"id\": 2, \"name\": \"Bird Feeder\" }, { \"id\": 3, \"name\": \"Cat Pole\" } ] }\n\nIf you want to run a query like \u201cfind all distinct IDs in the json_field\u2019s products array\u201d, you can\u2019t do that with the built in JSON functions that Postgres currently supplies (as far as I\u2019m aware!). This is a fairly common use case, especially for reporting.\n\nTo get this work, I wrote this simple PgPL\/SQL function to map a JSON array.\n\nCREATE OR REPLACE FUNCTION json_array_map(json_arr json, path TEXT[]) RETURNS json[] LANGUAGE plpgsql IMMUTABLE AS $$ DECLARE rec json; len int; ret json[]; BEGIN -- If json_arr is not an array, return an empty array as the result BEGIN len := json_array_length(json_arr); EXCEPTION WHEN OTHERS THEN RETURN ret; END; -- Apply mapping in a loop FOR rec IN SELECT json_array_elements#>path FROM json_array_elements(json_arr) LOOP ret := array_append(ret,rec); END LOOP; RETURN ret; END $$;\n\nWhat this function does is given a JSON array as \u201cjson_arr\u201d and a JSON path as \u201cpath\u201d, it will loop through all elements of the JSON array, locate the element at the path, and store it in a Postgres native array of JSON elements. You can then use other Postgres array functions to aggregate it.\n\nFor the query above where we want to find distinct product IDs in the orders table, we could write something like this:\n\nSELECT DISTINCT unnest(json_array_map(orders.json_field#>'{products}', '{id}'::text[]))::text AS \"id\" FROM orders;\n\nThat would give you the result:\n\n id ----- 2 3 1\n\nPretty cool!\n\n\n","html":"<p>Postgres JSON support is pretty amazing. I&#8217;ve been using it extensively for storing semi-structured data for a project and it has been great for that use case. In Postgres 9.3, the maintainers added the ability to perform some simple queries on JSON structures and a few functions to convert from JSON to Postgres arrays and result sets.<\/p>\n<p>One feature that I couldn&#8217;t figure out how to implement using the built-in Postgres functions was the ability to query within a JSON array. This is fairly critical for lots of the reporting queries that I&#8217;ve been building over the part few days. Suppose you have some JSON like this, stored in two rows in a table called &#8220;orders&#8221;, in the column &#8220;json_field&#8221;:<\/p>\n<pre class=\"lang:js decode:true\" title=\"JSON Array Example\">\/\/ Row 1, \"json_field\" column -----\r\n{\r\n   \"products\": [\r\n      { \"id\": 1, \"name\": \"Fish Tank\" },\r\n      { \"id\": 2, \"name\": \"Bird Feeder\" }\r\n   ]\r\n}\r\n\r\n\/\/ Row 2, \"json_field\" column -----\r\n{\r\n   \"products\": [\r\n      { \"id\": 2, \"name\": \"Bird Feeder\" },\r\n      { \"id\": 3, \"name\": \"Cat Pole\" }\r\n   ]\r\n}<\/pre>\n<p>If you want to run a query like &#8220;find all distinct IDs in the json_field&#8217;s products array&#8221;, you can&#8217;t do that with the built in JSON functions that Postgres currently supplies (as far as I&#8217;m aware!). This is a fairly common use case, especially for reporting.<\/p>\n<p>To get this work, I wrote this simple PgPL\/SQL function to map a JSON array.<\/p>\n<pre class=\"lang:pgsql decode:true\" title=\"Maps elements in a JSON array to an \" array=\"\" of=\"\" type=\"\" json=\"\" for=\"\" aggregation=\"\">CREATE OR REPLACE FUNCTION json_array_map(json_arr json, path TEXT[]) RETURNS json[]\r\nLANGUAGE plpgsql IMMUTABLE AS $$\r\nDECLARE\r\n\trec json;\r\n\tlen int;\r\n\tret json[];\r\nBEGIN\r\n\t-- If json_arr is not an array, return an empty array as the result\r\n\tBEGIN\r\n\t\tlen := json_array_length(json_arr);\r\n\tEXCEPTION\r\n\t\tWHEN OTHERS THEN\r\n\t\t\tRETURN ret;\r\n\tEND;\r\n\r\n\t-- Apply mapping in a loop\r\n\tFOR rec IN SELECT json_array_elements#&gt;path FROM json_array_elements(json_arr)\r\n\tLOOP\r\n\t\tret := array_append(ret,rec);\r\n\tEND LOOP;\r\n\tRETURN ret;\r\nEND $$;<\/pre>\n<p>What this function does is given a JSON array as &#8220;json_arr&#8221; and a JSON path as &#8220;path&#8221;, it will loop through all elements of the JSON array, locate the element at the path, and store it in a Postgres native array of JSON elements. You can then use other Postgres array functions to aggregate it.<\/p>\n<p>For the query above where we want to find distinct product IDs in the orders table, we could write something like this:<\/p>\n<pre class=\"lang:pgsql decode:true\" title=\"Select distinct product IDs from the \" orders=\"\" table=\"\">SELECT DISTINCT unnest(json_array_map(orders.json_field#&gt;'{products}', '{id}'::text[]))::text AS \"id\" FROM orders;<\/pre>\n<p>That would give you the result:<\/p>\n<pre class=\"lang:default highlight:0 decode:true\"> id \r\n-----\r\n 2\r\n 3\r\n 1<\/pre>\n<p>Pretty cool!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 14 Mar 2014 12:17:58 +0000","created_by":1,"updated_at":"Wed, 21 May 2014 17:03:45 +0000","updated_by":1,"published_at":"Fri, 14 Mar 2014 12:17:58 +0000","published_by":1},{"id":265,"title":"Writing Zero-Downtime Migrations for Rails and Postgres","slug":"writing-zero-downtime-migrations-for-rails-and-postgres","markdown":"\nLet\u2019s suppose you are building an app. It is under heavy development and the dev team is cranking out new features left and right. Developers need to continually change the database schema, but you don\u2019t want to take down the app for database migrations if at all possible. How the heck do you do this with Rails?\n\nWe had this problem recently, and have come up with a procedure that solves it for most small database migrations. The goals of this procedure are to:\n\n- Avoid downtime by running database migrations while the app is live\n- Avoid too many separate deployments to production\n- Keep the application code as clean as possible\n- Balance the cost of additional coding with the benefit of having a zero-downtime migration. If the cost or complexity of coding a migration in this way is too great, then a maintenance window is scheduled and the migration is written in a non-zero downtime fashion.\n\nThe first thing to understand when writing a zero downtime migration is what types of Postgres data definition language (DDL) queries can be run without locking tables. As of Postgres 9.3, the following DDL queries can be run without locking a table:\n\nCREATE INDEX CONCURRENTLY\n\nPostgres can create indexes concurrently (without table locks) in most cases. `CREATE INDEX CONCURRENTLY` can take significantly longer than `CREATE INDEX`, but it will allow both reads and writes while the index is being generated.\n\nALTER TABLE ... ADD COLUMN -- certain cases only!\n\nYou can add a column to a table without a table lock if the column being added is nullable and has no default value or other constraints.\n\nIf you want to add a column with a constraint or a column with a default value, one option may be to add the column first without a default value and no constraint, then in a separate transaction set the default value (using `UPDATE`) \u00a0or use `CREATE INDEX CONCURRENTLY` to add a index that will be used for the constraint. Finally, a third transaction can add the constraint or default to the table. If the third transaction is adding a constraint that uses an existing index, no table scan is required.\n\nALTER TABLE ... DROP COLUMN\n\nDropping a column only results in a metadata change, so it is non-blocking. When the table is `VACUUMED`, the data is actually removed.\n\nCREATE TABLE, CREATE FUNCTION\n\nCreating a table or a function is obviously safe because no one will have a lock on these objects before they are created.\n\n\n## Process for Coding the Migration\n\nThe guidelines I have been using for writing a zero-downtime migration are to:\n\n- **Step 1:\u00a0**Write the database migration in Rails.\n- **Step 2:\u00a0**Modify the application code in such a way that it will work both before and after the migration has been applied (more details on this below). This will probably entail writing code that branches depending on that database state.\n- **Step 3:\u00a0**Run your test suite with the modified code in step 2* but before you apply the database migration!*\n- **Step 4:\u00a0**Run your test suite with the modified code in step 2 *after applying the database migration*. Tests should pass in both cases.\n- **Step 5:\u00a0**Create a pull request on Github (or the equivalent in whatever tool you are using). Tag this in such a way that whoever is reviewing your code knows that there is a database migration that needs careful review.\n- **Step 6:\u00a0**Create a separate pull request on Github that cleans up the branching code you wrote in step 2. The code you write in this step can assume that the DB is migrated.\n\nWhen the migration is deployed, you\u2019ll deploy first the code reviewed in step 5. This code will be running against the non-migrated database, but that is a-ok because you have tested that case in step 3. Next, you will run the migration \u201clive\u201d. Once the migration is applied, you will still be running the code reviewed in step 5, but against the migrated database. Again, this is fine because you have tested that in step 4.**  \n**\n\nFinally, once the production database has been migrated, you should merge your pull request from step 6. This eliminates the dead code supporting the unmigrated version of the database. You should write the code for step 6 at the same time you write the rest of this code. Then just leave the pull request open until you are ready to merge. The advantage of this is that you will be \u201ccleaning up\u201d the extraneous code while it is still fresh in your mind.\n\n\n## Branching Application Code to Support Multiple DB States\n\nThe key to making this strategy work is that you\u2019ll need to write you application code in step 2 in a way that supports two database states: the pre-migrated state and the post-migrated state. The way to do this is to check the database state in the models and branch accordingly.\n\nSuppose you are dropping a column called \u201cdeleted\u201d. Prior to dropping the column, you have a default scope that excludes deleted rows. After dropping the column, you want the default scope to include all rows.\n\nYou would code a migration to do that like this:\n\nclass DropDeletedFromPosts < ActiveRecord::Migration def up drop_column :posts, :deleted end def down add_column :posts, :boolean, :deleted, default: false, null: false end end\n\nThen, in your Post model, you\u2019d add branching like this:\n\nclass Post < ActiveRecord::Base # TODO: Remove this code after the DropDeletedFromPosts migration has # been applied. if Post.attribute_names.include? 'deleted' default_scope -> { where(deleted: false) } end # Other model code here ... end\n\n### \u00a0But doesn\u2019t this get complicated for larger migrations?\n\nYes, absolutely it does. What we do when branching like this and it gets too complicated, we either sequence the DB changes over multiple deployments (and multiple sprints in the Agile sense) or \u201cgive up\u201d and schedule a maintenance window (downtime) to do the change.\n\nWriting zero-downtime migrations is not easy, and you\u2019ll need to do a cost-benefit analysis between scheduling downtime and writing lots of hairy branching code to support a zero-downtime deploy. That decision will depend on how downtime impacts your customers and your development schedule.\n\nHopefully, if you decide to go the zero-downtime route, this procedure will make your life easier!\n\n\n","html":"<p>Let&#8217;s suppose you are building an app. It is under heavy development and the dev team is cranking out new features left and right. Developers need to continually change the database schema, but you don&#8217;t want to take down the app for database migrations if at all possible. How the heck do you do this with Rails?<\/p>\n<p>We had this problem recently, and have come up with a procedure that solves it for most small database migrations. The goals of this procedure are to:<\/p>\n<ul>\n<li>Avoid downtime by running database migrations while the app is live<\/li>\n<li>Avoid too many separate deployments to production<\/li>\n<li>Keep the application code as clean as possible<\/li>\n<li>Balance the cost of additional coding with the benefit of having a zero-downtime migration. If the cost or complexity of coding a migration in this way is too great, then a maintenance window is scheduled and the migration is written in a non-zero downtime fashion.<\/li>\n<\/ul>\n<p>The first thing to understand when writing a zero downtime migration is what types of Postgres data definition language (DDL) queries can be run without locking tables. As of Postgres 9.3, the following DDL queries can be run without locking a table:<\/p>\n<pre class=\"toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true\">CREATE INDEX CONCURRENTLY<\/pre>\n<p style=\"padding-left: 30px;\">Postgres can create indexes concurrently (without table locks) in most cases. <code>CREATE INDEX CONCURRENTLY<\/code> can take significantly longer than <code>CREATE INDEX<\/code>, but it will allow both reads and writes while the index is being generated.<\/p>\n<pre class=\"toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true\">ALTER TABLE ... ADD COLUMN -- certain cases only!<\/pre>\n<p style=\"padding-left: 30px;\">You can add a column to a table without a table lock if the column being added is nullable and has no default value or other constraints.<\/p>\n<p style=\"padding-left: 30px;\">If you want to add a column with a constraint or a column with a default value, one option may be to add the column first without a default value and no constraint, then in a separate transaction set the default value (using <code>UPDATE<\/code>) \u00a0or use <code>CREATE INDEX CONCURRENTLY<\/code> to add a index that will be used for the constraint. Finally, a third transaction can add the constraint or default to the table. If the third transaction is adding a constraint that uses an existing index, no table scan is required.<\/p>\n<pre class=\"toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true\">ALTER TABLE ... DROP COLUMN<\/pre>\n<p style=\"padding-left: 30px;\">Dropping a column only results in a metadata change, so it is non-blocking. When the table is <code>VACUUMED<\/code>, the data is actually removed.<\/p>\n<pre class=\"toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true\">CREATE TABLE, CREATE FUNCTION<\/pre>\n<p style=\"padding-left: 30px;\">Creating a table or a function is obviously safe because no one will have a lock on these objects before they are created.<\/p>\n<h2>Process for Coding the Migration<\/h2>\n<p>The guidelines I have been using for writing a zero-downtime migration are to:<\/p>\n<ul>\n<li><strong>Step 1:\u00a0<\/strong>Write the database migration in Rails.<\/li>\n<li><strong>Step 2:\u00a0<\/strong>Modify the application code in such a way that it will work both before and after the migration has been applied (more details on this below). This will probably entail writing code that branches depending on that database state.<\/li>\n<li><strong>Step 3:\u00a0<\/strong>Run your test suite with the modified code in step 2<i> but before you apply the database migration!<\/i><\/li>\n<li><strong>Step 4:\u00a0<\/strong>Run your test suite with the modified code in step 2 <em>after applying the database migration<\/em>. Tests should pass in both cases.<\/li>\n<li><strong>Step 5:\u00a0<\/strong>Create a pull request on Github (or the equivalent in whatever tool you are using). Tag this in such a way that whoever is reviewing your code knows that there is a database migration that needs careful review.<\/li>\n<li><strong>Step 6:\u00a0<\/strong>Create a separate pull request on Github that cleans up the branching code you wrote in step 2. The code you write in this step can assume that the DB is migrated.<\/li>\n<\/ul>\n<p>When the migration is deployed, you&#8217;ll deploy first the code reviewed in step 5. This code will be running against the non-migrated database, but that is a-ok because you have tested that case in step 3. Next, you will run the migration &#8220;live&#8221;. Once the migration is applied, you will still be running the code reviewed in step 5, but against the migrated database. Again, this is fine because you have tested that in step 4.<strong><br \/>\n<\/strong><\/p>\n<p>Finally, once the production database has been migrated, you should merge your pull request from step 6. This eliminates the dead code supporting the unmigrated version of the database. You should write the code for step 6 at the same time you write the rest of this code. Then just leave the pull request open until you are ready to merge. The advantage of this is that you will be &#8220;cleaning up&#8221; the extraneous code while it is still fresh in your mind.<\/p>\n<h2>Branching Application Code to Support Multiple DB States<\/h2>\n<p>The key to making this strategy work is that you&#8217;ll need to write you application code in step 2 in a way that supports two database states: the pre-migrated state and the post-migrated state. The way to do this is to check the database state in the models and branch accordingly.<\/p>\n<p>Suppose you are dropping a column called &#8220;deleted&#8221;. Prior to dropping the column, you have a default scope that excludes deleted rows. After dropping the column, you want the default scope to include all rows.<\/p>\n<p>You would code a migration to do that like this:<\/p>\n<pre class=\"lang:ruby decode:true\" title=\"Rails migration to drop \" deleted=\"\" column=\"\">class DropDeletedFromPosts &lt; ActiveRecord::Migration\r\n   def up\r\n     drop_column :posts, :deleted\r\n   end\r\n\r\n   def down\r\n      add_column :posts, :boolean, :deleted, default: false, null: false\r\n   end\r\nend<\/pre>\n<p>Then, in your Post model, you&#8217;d add branching like this:<\/p>\n<pre class=\"lang:default decode:true\" title=\"Post model with branching\">class Post &lt; ActiveRecord::Base\r\n   \r\n   # TODO: Remove this code after the DropDeletedFromPosts migration has\r\n   # been applied.\r\n   if Post.attribute_names.include? 'deleted'\r\n      default_scope -&gt; { where(deleted: false) }\r\n   end \r\n\r\n   # Other model code here ...\r\n\r\nend<\/pre>\n<h3>\u00a0But doesn&#8217;t this get complicated for larger migrations?<\/h3>\n<p>Yes, absolutely it does. What we do when branching like this and it gets too complicated, we either sequence the DB changes over multiple deployments (and multiple sprints in the Agile sense) or &#8220;give up&#8221; and schedule a maintenance window (downtime) to do the change.<\/p>\n<p>Writing zero-downtime migrations is not easy, and you&#8217;ll need to do a cost-benefit analysis between scheduling downtime and writing lots of hairy branching code to support a zero-downtime deploy. That decision will depend on how downtime impacts your customers and your development schedule.<\/p>\n<p>Hopefully, if you decide to go the zero-downtime route, this procedure will make your life easier!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 18 Apr 2014 09:38:26 +0000","created_by":1,"updated_at":"Fri, 18 Apr 2014 09:38:26 +0000","updated_by":1,"published_at":"Fri, 18 Apr 2014 09:38:26 +0000","published_by":1},{"id":229,"title":"The Business Model Sketch","slug":"the-business-model-sketch","markdown":"\nI was helping a friend work through some business ideas, and realized that like writing an outline helps you structure a essay, doing a \u201cbusiness model sketch\u201d can help you break apart a business idea and evaluate its viability. Just like an essay needs a thesis, a body and a conclusion or it can fall flat, a business model needs specific components or it is not viable. I identified six components that roughly draw from [The Personal MBA](http:\/\/www.amazon.com\/The-Personal-MBA-Master-Business\/dp\/1591845572 \"The Personal MBA by Joshua Kaufman at Amazon\") and the \u201clean startup\u201d philosophy. If you are thinking of starting a new business, consider taking one page of paper, drawing six boxes, and filling out these six areas: customer, value proposition, marketing, sales, value delivery, and finance.\n\nThe problem with many business ideas is that, while they sometimes hit on a few of these areas, they are weak in others. That can be okay, but as a startup, your goal should be to develop a plan about how to address your idea\u2019s weak points, and quickly test whether that plan is viable. If it is, great! If not, reconsider or proceed with caution!\n\n\n## The template\n\n### The customer\n\nThe specific person to whom value is being provided **and is making a purchasing decision**. Many times a business\u2019s value proposition will include many people. For example, if you are selling family vacations, your business is offering value to the father (maybe you bundle in a few rounds of golf), mother (there\u2019s a reason cruises have spas), and kids (I don\u2019t think Disney pays actors to dress as Micky Mouse for dad). This is important to understand, especially when you get to the value delivery portion of the business model sketch. However, first identify the decision maker!\n\n### Value proposition\n\nThe value proposition is a description of the value you are providing your customer. Remember that your customer is the person making the purchasing decision, so when crafting your value proposition, you need to understand the wants and needs of that particular person above any secondary party.\n\nIdeally, focus your value proposition on addressing core human drives. The Personal MBA identifies [five core human drives](http:\/\/book.personalmba.com\/core-human-drives\/): the drives to acquire, bond, learn, defend, and feel. Many consumer products clearly target their value propositions to these core human drives. This can be difficult if you are not selling consumer products or services, but if you look closely, many business-to-business or business-to-government sales are targeted the same way. Living in Washington, DC I notice this every time I go through the Pentagon metro station. Large defense contractors clearly target their buyer\u2019s core human drive to defend with advertisements that depict the strength and sophistication of their offerings. This example of [advertisements for\u00a0Northrop Grumman\u2019s unmanned drones](http:\/\/www.huffingtonpost.com\/2013\/09\/06\/drone-ads-dc-metro_n_3880026.html) is a great example.\n\n### Marketing\n\nMarketing is fundamentally your strategy for getting your customer\u2019s attention to deliver your value proposition. When you create a business model sketch, you need to attempt to identify how you will reach enough customers at an economical cost. If you have an excellent value proposition but your customers do not know about it, your business model will fail.\n\nIn the marketing portion of your sketch, you should also attempt to identify in rough terms the market size and dynamics: how many potential customers do you have and is that number increasing and\/or gaining purchasing power?\n\n### Sales\n\nIf marketing is your strategy for reaching your target customer, sales is your strategy for negotiating a contract to deliver your value proposition in exchange for money. For an online business, this could be as simple as \u201csign up for PayPal and put a Buy Now button on my website\u201d. If you are an enterprise software, this could be extensive contract negotiations.\n\nSales and marketing are closely intertwined, but separating them out to different boxes in the business model sketch should help you to separate the\u00a0*act of reaching your customer and delivering your value proposition* (marketing) and\u00a0*the mechanics of \u201cclosing the deal\u201d* (sales).\n\n### Value delivery\n\nThis is what many people think of as the meat of your business model, but the previous sections are also key to determining its viability. Value delivery is the processes that delivery the promised value to your customers. In the family vacation example, this is the operations of your hotel from hiring staff to procuring food and drinks for the restaurant. If you are an online business, this is the cost of developing your website as well as operational cost of running your servers and supporting your customers.\n\n### Finance\n\nThe finance section should answer \u201cwhat financial resources do I need to support this business model, and what is the return on investment?\u201d. This is hard to answer in specifics in a business model sketch, and should be tackled in more detail in a complete business plan. However, once you complete the other sections, you should be able to answer these questions:\n\n- Is this a financial capital-intensive business to set up? To run? If yes, what is the risk to capital committed to the business? What return can I offer owners of capital given that risk? How can I source that capital, loans or equity investment?\n- Is this a human capital-intensive business to set up? If yes, what is my recruitment strategy? Can I attract the right talent? Would I compensate them with equity or a salary?\n\n\n","html":"<p>I was helping a friend work through some business ideas, and realized that like writing an outline helps you structure a essay, doing a &#8220;business model sketch&#8221; can help you break apart a business idea and evaluate its viability. Just like an essay needs a thesis, a body and a conclusion or it can fall flat, a business model needs specific components or it is not viable. I identified six components that roughly draw from <a title=\"The Personal MBA by Joshua Kaufman at Amazon\" href=\"http:\/\/www.amazon.com\/The-Personal-MBA-Master-Business\/dp\/1591845572\" target=\"_blank\">The Personal MBA<\/a> and the &#8220;lean startup&#8221; philosophy. If you are thinking of starting a new business, consider taking one page of paper, drawing six boxes, and filling out these six areas: customer, value proposition, marketing, sales, value delivery, and finance.<\/p>\n<p>The problem with many business ideas is that, while they sometimes hit on a few of these areas, they are weak in others. That can be okay, but as a startup, your goal should be to develop a plan about how to address your idea&#8217;s weak points, and quickly test whether that plan is viable. If it is, great! If not, reconsider or proceed with caution!<\/p>\n<h2>The template<\/h2>\n<h3>The customer<\/h3>\n<p>The specific person to whom value is being provided <strong>and is making a purchasing decision<\/strong>. Many times a business&#8217;s value proposition will include many people. For example, if you are selling family vacations, your business is offering value to the father (maybe you bundle in a few rounds of golf), mother (there&#8217;s a reason cruises have spas), and kids (I don&#8217;t think Disney pays actors to dress as Micky Mouse for dad). This is important to understand, especially when you get to the value delivery portion of the business model sketch. However, first identify the decision maker!<\/p>\n<h3>Value proposition<\/h3>\n<p>The value proposition is a description of the value you are providing your customer. Remember that your customer is the person making the purchasing decision, so when crafting your value proposition, you need to understand the wants and needs of that particular person above any secondary party.<\/p>\n<p>Ideally, focus your value proposition on addressing core human drives. The Personal MBA identifies <a href=\"http:\/\/book.personalmba.com\/core-human-drives\/\" target=\"_blank\">five core human drives<\/a>: the drives to acquire, bond, learn, defend, and feel. Many consumer products clearly target their value propositions to these core human drives. This can be difficult if you are not selling consumer products or services, but if you look closely, many business-to-business or business-to-government sales are targeted the same way. Living in Washington, DC I notice this every time I go through the Pentagon metro station. Large defense contractors clearly target their buyer&#8217;s core human drive to defend with advertisements that depict the strength and sophistication of their offerings. This example of <a href=\"http:\/\/www.huffingtonpost.com\/2013\/09\/06\/drone-ads-dc-metro_n_3880026.html\" target=\"_blank\">advertisements for\u00a0Northrop Grumman&#8217;s unmanned drones<\/a> is a great example.<\/p>\n<h3>Marketing<\/h3>\n<p>Marketing is fundamentally your strategy for getting your customer&#8217;s attention to deliver your value proposition. When you create a business model sketch, you need to attempt to identify how you will reach enough customers at an economical cost. If you have an excellent value proposition but your customers do not know about it, your business model will fail.<\/p>\n<p>In the marketing portion of your sketch, you should also attempt to identify in rough terms the market size and dynamics: how many potential customers do you have and is that number increasing and\/or gaining purchasing power?<\/p>\n<h3>Sales<\/h3>\n<p>If marketing is your strategy for reaching your target customer, sales is your strategy for negotiating a contract to deliver your value proposition in exchange for money. For an online business, this could be as simple as &#8220;sign up for PayPal and put a Buy Now button on my website&#8221;. If you are an enterprise software, this could be extensive contract negotiations.<\/p>\n<p>Sales and marketing are closely intertwined, but separating them out to different boxes in the business model sketch should help you to separate the\u00a0<em>act of reaching your customer and delivering your value proposition<\/em> (marketing) and\u00a0<em>the mechanics of &#8220;closing the deal&#8221;<\/em> (sales).<\/p>\n<h3>Value delivery<\/h3>\n<p>This is what many people think of as the meat of your business model, but the previous sections are also key to determining its viability. Value delivery is the processes that delivery the promised value to your customers. In the family vacation example, this is the operations of your hotel from hiring staff to procuring food and drinks for the restaurant. If you are an online business, this is the cost of developing your website as well as operational cost of running your servers and supporting your customers.<\/p>\n<h3>Finance<\/h3>\n<p>The finance section should answer &#8220;what financial resources do I need to support this business model, and what is the return on investment?&#8221;. This is hard to answer in specifics in a business model sketch, and should be tackled in more detail in a complete business plan. However, once you complete the other sections, you should be able to answer these questions:<\/p>\n<ul>\n<li>Is this a financial capital-intensive business to set up? To run? If yes, what is the risk to capital committed to the business? What return can I offer owners of capital given that risk? How can I source that capital, loans or equity investment?<\/li>\n<li>Is this a human capital-intensive business to set up? If yes, what is my recruitment strategy? Can I attract the right talent? Would I compensate them with equity or a salary?<\/li>\n<\/ul>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 22 Apr 2014 09:25:19 +0000","created_by":1,"updated_at":"Wed, 21 May 2014 17:01:00 +0000","updated_by":1,"published_at":"Tue, 22 Apr 2014 09:25:19 +0000","published_by":1},{"id":267,"title":"The Ultimate Guide to Dealing with High Frequency Data","slug":"the-ultimate-guide-to-dealing-with-high-frequency-data","markdown":"\nI have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!\n\nI\u2019m currently working on a project to replace the [Time Series Database](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\")\u00a0(TSDB) that I wrote a few years ago. By re-writing it, I\u2019m learning\u00a0a lot about what works and what doesn\u2019t when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I\u2019ve developed while working on this project.\n\n\n## Partition your data!\n\nThe biggest issue with this much time series data is that you simply cannot index the timestamp column with any normal indexing scheme. In DB-speak, the timestamp column will be a [\u201chigh cardinality\u201d](http:\/\/en.wikipedia.org\/wiki\/Cardinality_(SQL_statements) \"Cardinality (Wikipedia)\") column, which means it does not lend itself well to\u00a0indexing. This is a problem because most queries on this kind of high frequency data are to fetch a subset by timestamp, and you do NOT want to make a table scan of a billion plus records to find a few minutes of data.\n\nTSDB attempts to fix this problem by keeping rows in order and creating a [sparse index](http:\/\/www.cs.sfu.ca\/CourseCentral\/354\/zaiane\/material\/notes\/Chapter11\/node5.html \"Dense versus Sparse Indexes\") (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I\u2019m not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.\n\nAnother approach is to\u00a0*partition* your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union\u00a0queries across tables.\n\nPartitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I\u2019m working on will use\u00a0[PyTables](http:\/\/pytables.github.io \"PyTables\") to partition time series data by date.\n\n\n## Store timestamps in UTC\n\nMost of the time, your source data will have timestamps in UTC. If it doesn\u2019t, I suggest you convert to UTC before storing. Most libraries use either UTC or local time internally, and because you can never be sure what time zone your users will be in, using UTC is the least common denominator.\n\nUTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24\/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.\n\n\n## Store timestamps as integers, even if your programming language uses floats\n\nMATLAB, Excel, and R all store timestamps internally as floats by default. This gives their timestamp types a large range and high precision, but I don\u2019t think it is appropriate for archiving time series data. Why? Floats are imprecise. You do not know with any accuracy the number of significant digits when using a float, and you cannot make comparisons without worrying about round off errors. Admittedly, even with microsecond\u00a0data, these systems that use a 64-bit double and 1970-01-01 as the epoch will not loose precision until <span style=\"color: #000000;\">2242-03-16, but why worry about it? I recommend a 64-bit integer as the timestamp column. With one tick equaling one millisecond, you have a time range of\u00a0<span style=\"color: #252525;\">\u00b1<\/span><span style=\"color: #252525;\">292 million years. This is Java\u2019s internal representation. With one tick equaling 100 nanoseconds (0.1 microsecond), you have a time range of \u00b129,227 years, which is what Win32 does. Should be plenty!<\/span><\/span>\n\n\n## Have a solid ETL procedure\n\nETL means \u201cextract, transform, load\u201d and is the term for taking data out of one format or system and loading it into another. The step to make sure is solid when you are dealing with high frequency data is the \u201cload\u201d step. Try to make a process where you can revert a botched load automatically. If you don\u2019t do this, guaranteed someone or something will screw up an import, and you will be left wading through millions of rows of data to fix it or re-importing everything from your source system.\n\nThe most basic way to make the load step revertible is to just make a copy of\u00a0your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!\n\n\n","html":"<p>I have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!<\/p>\n<p>I&#8217;m currently working on a project to replace the <a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">Time Series Database<\/a>\u00a0(TSDB) that I wrote a few years ago. By re-writing it, I&#8217;m learning\u00a0a lot about what works and what doesn&#8217;t when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I&#8217;ve developed while working on this project.<\/p>\n<h2>Partition your data!<\/h2>\n<p>The biggest issue with this much time series data is that you simply cannot index the timestamp column with any normal indexing scheme. In DB-speak, the timestamp column will be a <a title=\"Cardinality (Wikipedia)\" href=\"http:\/\/en.wikipedia.org\/wiki\/Cardinality_(SQL_statements)\" target=\"_blank\">&#8220;high cardinality&#8221;<\/a> column, which means it does not lend itself well to\u00a0indexing. This is a problem because most queries on this kind of high frequency data are to fetch a subset by timestamp, and you do NOT want to make a table scan of a billion plus records to find a few minutes of data.<\/p>\n<p>TSDB attempts to fix this problem by keeping rows in order and creating a <a title=\"Dense versus Sparse Indexes\" href=\"http:\/\/www.cs.sfu.ca\/CourseCentral\/354\/zaiane\/material\/notes\/Chapter11\/node5.html\" target=\"_blank\">sparse index<\/a> (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I&#8217;m not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.<\/p>\n<p>Another approach is to\u00a0<em>partition<\/em> your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union\u00a0queries across tables.<\/p>\n<p>Partitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I&#8217;m working on will use\u00a0<a title=\"PyTables\" href=\"http:\/\/pytables.github.io\" target=\"_blank\">PyTables<\/a> to partition time series data by date.<\/p>\n<h2>Store timestamps in UTC<\/h2>\n<p>Most of the time, your source data will have timestamps in UTC. If it doesn&#8217;t, I suggest you convert to UTC before storing. Most libraries use either UTC or local time internally, and because you can never be sure what time zone your users will be in, using UTC is the least common denominator.<\/p>\n<p>UTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24\/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.<\/p>\n<h2>Store timestamps as integers, even if your programming language uses floats<\/h2>\n<p>MATLAB, Excel, and R all store timestamps internally as floats by default. This gives their timestamp types a large range and high precision, but I don&#8217;t think it is appropriate for archiving time series data. Why? Floats are imprecise. You do not know with any accuracy the number of significant digits when using a float, and you cannot make comparisons without worrying about round off errors. Admittedly, even with microsecond\u00a0data, these systems that use a 64-bit double and 1970-01-01 as the epoch will not loose precision until <span style=\"color: #000000;\">2242-03-16, but why worry about it? I recommend a 64-bit integer as the timestamp column. With one tick equaling one millisecond, you have a time range of\u00a0<span style=\"color: #252525;\">\u00b1<\/span><span style=\"color: #252525;\">292 million years. This is Java&#8217;s internal representation. With one tick equaling 100 nanoseconds (0.1 microsecond), you have a time range of \u00b129,227 years, which is what Win32 does. Should be plenty!<\/span><\/span><\/p>\n<h2>Have a solid ETL procedure<\/h2>\n<p>ETL means &#8220;extract, transform, load&#8221; and is the term for taking data out of one format or system and loading it into another. The step to make sure is solid when you are dealing with high frequency data is the &#8220;load&#8221; step. Try to make a process where you can revert a botched load automatically. If you don&#8217;t do this, guaranteed someone or something will screw up an import, and you will be left wading through millions of rows of data to fix it or re-importing everything from your source system.<\/p>\n<p>The most basic way to make the load step revertible is to just make a copy of\u00a0your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 07 May 2014 09:10:24 +0000","created_by":1,"updated_at":"Wed, 21 May 2014 17:00:28 +0000","updated_by":1,"published_at":"Wed, 07 May 2014 09:10:24 +0000","published_by":1},{"id":269,"title":"Two Podcasts I've been Enjoying Recently","slug":"two-podcasts-ive-been-enjoying-recently","markdown":"\nSince I\u2019ve moved offices to [WeWork Chinatown](http:\/\/www.wework.com\/location\/chinatown\/), I have about a 20 minute commute on Metro. The nice thing about commutes is that you have some downtime, which recently I\u2019ve been using to listen to podcasts. \u00a0If you\u2019re looking for podcast recommendations, two good ones are Monocle\u2019s *The Entrepreneurs* and the\u00a0*Tim Ferriss Show*.\n\n[*The Entrepreneurs*](http:\/\/monocle.com\/radio\/shows\/the-entrepreneurs\/ \"Monocle - The Entrepreneurs\")\u00a0kind of makes you feel like you are listening to BBC\u2019s World Service or NPR, but the content is focused on business and entrepreneurship. It has a refreshing international and non-technology bent, which is great for getting out of the U.S. technology world and realizing that every new business doesn\u2019t need to be a website started in San Francisco!\n\nThe\u00a0[*Tim Ferriss Show*](http:\/\/www.stitcher.com\/podcast\/tim-ferriss-show\/the-tim-ferriss-show \"Tim Ferriss Show\") is by the author of the\u00a0*Four Hour Body\u00a0*and\u00a0*Four Hour**Workweek*, both NY Times bestsellers. The format is much longer than\u00a0*The Entrepreneurs,* and is more of a conversation than a highly-produced magazine-style show. There are only a few episodes of this one, but so far the guests have been very interesting and Tim does a good job interviewing them in depth.\n\n\n","html":"<p>Since I&#8217;ve moved offices to <a href=\"http:\/\/www.wework.com\/location\/chinatown\/\" target=\"_blank\">WeWork Chinatown<\/a>, I have about a 20 minute commute on Metro. The nice thing about commutes is that you have some downtime, which recently I&#8217;ve been using to listen to podcasts. \u00a0If you&#8217;re looking for podcast recommendations, two good ones are Monocle&#8217;s <em>The Entrepreneurs<\/em> and the\u00a0<em>Tim Ferriss Show<\/em>.<\/p>\n<p><a title=\"Monocle - The Entrepreneurs\" href=\"http:\/\/monocle.com\/radio\/shows\/the-entrepreneurs\/\" target=\"_blank\"><em>The Entrepreneurs<\/em><\/a>\u00a0kind of makes you feel like you are listening to BBC&#8217;s World Service or NPR, but the content is focused on business and entrepreneurship. It has a refreshing international and non-technology bent, which is great for getting out of the U.S. technology world and realizing that every new business doesn&#8217;t need to be a website started in San Francisco!<\/p>\n<p>The\u00a0<a title=\"Tim Ferriss Show\" href=\"http:\/\/www.stitcher.com\/podcast\/tim-ferriss-show\/the-tim-ferriss-show\" target=\"_blank\"><em>Tim Ferriss Show<\/em><\/a> is by the author of the\u00a0<em>Four Hour Body\u00a0<\/em>and\u00a0<em>Four Hour<\/em> <em>Workweek<\/em>, both NY Times bestsellers. The format is much longer than\u00a0<em>The Entrepreneurs,<\/em> and is more of a conversation than a highly-produced magazine-style show. There are only a few episodes of this one, but so far the guests have been very interesting and Tim does a good job interviewing them in depth.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 21 May 2014 16:57:02 +0000","created_by":1,"updated_at":"Wed, 21 May 2014 16:57:02 +0000","updated_by":1,"published_at":"Wed, 21 May 2014 16:57:02 +0000","published_by":1},{"id":272,"title":"The barebones profiling method that is surprisingly effective","slug":"the-barebones-profiling-method-that-is-surprisingly-effective","markdown":"\nI\u2019m working on profiling my time series database ([TsTables](http:\/\/github.com\/afiedler\/tstables \"TsTables\")) because append performance is not what I want it to be. I know that the issue is a few loops that are written in Python instead of using NumPy\u2019s optimized vector operations. I\u2019m not exactly sure which loop is the slowest.\n\nI started trying to get cProfile to work, but ended up with way too much data to be useful. So I reverted to my old school, barebones profiling method: Ctrl-C.\n\nHow do you use this method you might ask? Start your program and randomly hit Ctrl-C. Wherever your program stops most frequently is the probably the slowest part. Speed that up and repeat!\n\n\n","html":"<p>I&#8217;m working on profiling my time series database (<a title=\"TsTables\" href=\"http:\/\/github.com\/afiedler\/tstables\" target=\"_blank\">TsTables<\/a>) because append performance is not what I want it to be. I know that the issue is a few loops that are written in Python instead of using NumPy&#8217;s optimized vector operations. I&#8217;m not exactly sure which loop is the slowest.<\/p>\n<p>I started trying to get cProfile to work, but ended up with way too much data to be useful. So I reverted to my old school, barebones profiling method: Ctrl-C.<\/p>\n<p>How do you use this method you might ask? Start your program and randomly hit Ctrl-C. Wherever your program stops most frequently is the probably the slowest part. Speed that up and repeat!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 27 May 2014 08:01:06 +0000","created_by":1,"updated_at":"Wed, 28 May 2014 08:35:27 +0000","updated_by":1,"published_at":"Tue, 27 May 2014 08:01:06 +0000","published_by":1},{"id":273,"title":"Installing WxPython and RunSnakeRun on Mac OSX 10.9","slug":"installing-wxpython-and-runsnakerun-on-mac-osx-10-9","markdown":"\nI just [posted about my Ctrl-C strategy for profiling](http:\/\/andyfiedler.com\/blog\/the-barebones-profiling-method-that-is-surprisingly-effective-272\/ \"The barebones profiling method that is surprisingly effective\") and now I\u2019m going to completely flip-flop and explain how I installed RunSnakeRun, a way to visualize the output of Python\u2019s cProfile. The Ctrl-C way of profiling worked really well for optimizing append performance of my [time series storage library](http:\/\/github.com\/afiedler\/tstables \"TsTables Github\"), but doesn\u2019t work so great for profiling things that are already very fast (on the order of milliseconds) and need to be faster.\n\nFor that, RunSnakeRun worked really well. RunSnakeRun gives you a nice rectangle chart showing in which functions your program spends most of its time.\n\n<div class=\"wp-caption aligncenter\" id=\"attachment_274\" style=\"width: 635px\">![RunSnakeRun's rectangle plot of function cumulative run time](http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/05\/Screen-Shot-2014-05-28-at-9.27.01-AM-800x499.png)RunSnakeRun\u2019s rectangle plot of function cumulative run time\n\n<\/div>To install RunSnakeRun on Mac OSX, you\u2019ll need Homebrew and PIP. You can install it like this:\n\n$ brew install wxpython --python --devel $ pip install SquareMap RunSnakeRun\n\nNext, you\u2019ll need to export a pstats database with your profiler information. Use cProfile to do this. For TsTables, you can run the benchmark with profiling information like this:\n\nimport cProfile import tstables cProfile.run('tstables.Benchmark.main()','tstables.profile')\n\nThis will create a tstables.profile file in the current directory, which you can open with RunSnakeRun. Start RunSnakeRun by running <span class=\"lang:default highlight:0 decode:true  crayon-inline \">runsnake<\/span>\u00a0\u00a0(assuming that PIP\u2019s bin folder is in your path).\n\n\n","html":"<p>I just <a title=\"The barebones profiling method that is surprisingly effective\" href=\"http:\/\/andyfiedler.com\/blog\/the-barebones-profiling-method-that-is-surprisingly-effective-272\/\">posted about my Ctrl-C strategy for profiling<\/a> and now I&#8217;m going to completely flip-flop and explain how I installed RunSnakeRun, a way to visualize the output of Python&#8217;s cProfile. The Ctrl-C way of profiling worked really well for optimizing append performance of my <a title=\"TsTables Github\" href=\"http:\/\/github.com\/afiedler\/tstables\" target=\"_blank\">time series storage library<\/a>, but doesn&#8217;t work so great for profiling things that are already very fast (on the order of milliseconds) and need to be faster.<\/p>\n<p>For that, RunSnakeRun worked really well. RunSnakeRun gives you a nice rectangle chart showing in which functions your program spends most of its time.<\/p>\n<div id=\"attachment_274\" style=\"width: 635px\" class=\"wp-caption aligncenter\"><img class=\"wp-image-274 size-large\" src=\"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/05\/Screen-Shot-2014-05-28-at-9.27.01-AM-800x499.png\" alt=\"RunSnakeRun's rectangle plot of function cumulative run time\" width=\"625\" height=\"389\" \/><p class=\"wp-caption-text\">RunSnakeRun&#8217;s rectangle plot of function cumulative run time<\/p><\/div>\n<p>To install RunSnakeRun on Mac OSX, you&#8217;ll need Homebrew and PIP. You can install it like this:<\/p>\n<pre class=\"lang:default highlight:0 decode:true crayon-selected\" title=\"Bash commands to install RunSnakeRun\">$ brew install wxpython --python --devel\r\n$ pip install SquareMap RunSnakeRun<\/pre>\n<p>Next, you&#8217;ll need to export a pstats database with your profiler information. Use cProfile to do this. For TsTables, you can run the benchmark with profiling information like this:<\/p>\n<pre class=\"lang:python decode:true \" title=\"How to run the TsTables benchmark with profiling information\">import cProfile\r\nimport tstables\r\n\r\ncProfile.run('tstables.Benchmark.main()','tstables.profile')<\/pre>\n<p>This will create a tstables.profile file in the current directory, which you can open with RunSnakeRun. Start RunSnakeRun by running <span class=\"lang:default highlight:0 decode:true  crayon-inline \">runsnake<\/span>\u00a0\u00a0(assuming that PIP&#8217;s bin folder is in your path).<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 28 May 2014 09:37:42 +0000","created_by":1,"updated_at":"Wed, 28 May 2014 09:37:42 +0000","updated_by":1,"published_at":"Wed, 28 May 2014 09:37:42 +0000","published_by":1},{"id":277,"title":"TsTables - Store High Frequency Data with PyTables","slug":"tstables-store-high-frequency-data-with-pytables","markdown":"\nTsTables is a simple extension to the [PyTables library](http:\/\/pytables.github.io\/ \"PyTables\") that helps with storing large volumes of high frequency time series data in the [HDF5](http:\/\/www.hdfgroup.org\/ \"HDF5 Group\") format. It is being used to store hundreds of gigabytes of FX trading data for analysis and research.\u00a0TsTables is designed for an append-once, read-lots workflow. It automatically partitions\u00a0new time series data into daily tables when appending\u00a0to a series, greatly speeding up lookup times over a \u201cone big table\u201d approach. TsTables also contains a function to query across date boundaries and join the result into one Pandas DataFrame for easier analysis.\n\nThis project was designed as a replacement to a C\/C++ library I wrote while I was a Research Assistant at the Federal Reserve Board called [TSDB](http:\/\/andyfiedler.com\/projects\/time-series-database\/ \"Time Series Database\"). Since I finished that project, time series tooling in Python has improved a lot\u00a0and dealing with recompiling a C\/C++ library was becoming onerous. This project aims to solve the same problem as TSDB\u2014storing lots of time series data in a simple flat file for easy querying\u2014in a much more maintainable and accessible way.\n\n\n## Example usage\n\nThis example reads in minutely bitcoin price data and then fetches a range of that data into a Pandas DataFrame.\n\n# Class to use as the table description class BpiValues(tables.IsDescription): timestamp = tables.Int64Col(pos=0) bpi = tables.Float64Col(pos=1) # Use pandas to read in the CSV data bpi = pandas.read_csv('bpi_2014_01.csv',index_col=0,names=['date','bpi'],parse_dates=True) f = tables.open_file('bpi.h5','a') # Create a new time series ts = f.create_ts('\/','BPI',BpiValues) # Append the BPI data ts.append(bpi) # Read in some data read_start_dt = datetime(2014,1,4,12,00) read_end_dt = datetime(2014,1,4,14,30) rows = ts.read_range(read_start_dt,read_end_dt) # `rows` will be a pandas DataFrame with a DatetimeIndex.\n\n\n## \u00a0Benchmarks\n\nThe main goal of TsTables is to make it very fast to read a subset of data, given a time range. Using a simple time series with one year of secondly data with two columns (timestamp and a 32-bit integer price), TsTables has this performance on my 2013 MacBook Pro with a SSD:\n\n- Time to append one month of data (2.67 million rows): **0.771 seconds**\n- Time to fetch a random one hour subset into memory: **0.305 seconds**\n- File size (32 million rows, uncompressed): **391.6 MB**\n\n\n## Installing\n\nTsTables requires Python 3, Pandas and PyTables (which requires HDF5). It is available from PyPI.\n\n$ pip3 install tstables\n\nYou can also view and download the code on [Github](https:\/\/github.com\/afiedler\/tstables \"TsTables Github\").\n\n\n","html":"<p>TsTables is a simple extension to the <a title=\"PyTables\" href=\"http:\/\/pytables.github.io\/\" target=\"_blank\">PyTables library<\/a> that helps with storing large volumes of high frequency time series data in the <a title=\"HDF5 Group\" href=\"http:\/\/www.hdfgroup.org\/\" target=\"_blank\">HDF5<\/a> format. It is being used to store hundreds of gigabytes of FX trading data for analysis and research.\u00a0TsTables is designed for an append-once, read-lots workflow. It automatically partitions\u00a0new time series data into daily tables when appending\u00a0to a series, greatly speeding up lookup times over a &#8220;one big table&#8221; approach. TsTables also contains a function to query across date boundaries and join the result into one Pandas DataFrame for easier analysis.<\/p>\n<p>This project was designed as a replacement to a C\/C++ library I wrote while I was a Research Assistant at the Federal Reserve Board called <a title=\"Time Series Database\" href=\"http:\/\/andyfiedler.com\/projects\/time-series-database\/\">TSDB<\/a>. Since I finished that project, time series tooling in Python has improved a lot\u00a0and dealing with recompiling a C\/C++ library was becoming onerous. This project aims to solve the same problem as TSDB\u2014storing lots of time series data in a simple flat file for easy querying\u2014in a much more maintainable and accessible way.<\/p>\n<h2>Example usage<\/h2>\n<p>This example reads in minutely bitcoin price data and then fetches a range of that data into a Pandas DataFrame.<\/p>\n<pre class=\"lang:python decode:true\"># Class to use as the table description\r\nclass BpiValues(tables.IsDescription):\r\n    timestamp = tables.Int64Col(pos=0)\r\n    bpi = tables.Float64Col(pos=1)\r\n\r\n# Use pandas to read in the CSV data\r\nbpi = pandas.read_csv('bpi_2014_01.csv',index_col=0,names=['date','bpi'],parse_dates=True)\r\n\r\nf = tables.open_file('bpi.h5','a')\r\n\r\n# Create a new time series\r\nts = f.create_ts('\/','BPI',BpiValues)\r\n\r\n# Append the BPI data\r\nts.append(bpi)\r\n\r\n# Read in some data\r\nread_start_dt = datetime(2014,1,4,12,00)\r\nread_end_dt = datetime(2014,1,4,14,30)\r\n\r\nrows = ts.read_range(read_start_dt,read_end_dt)\r\n\r\n# `rows` will be a pandas DataFrame with a DatetimeIndex.<\/pre>\n<h2>\u00a0Benchmarks<\/h2>\n<p>The main goal of TsTables is to make it very fast to read a subset of data, given a time range. Using a simple time series with one year of secondly data with two columns (timestamp and a 32-bit integer price), TsTables has this performance on my 2013 MacBook Pro with a SSD:<\/p>\n<ul>\n<li>Time to append one month of data (2.67 million rows): <strong>0.771 seconds<\/strong><\/li>\n<li>Time to fetch a random one hour subset into memory: <strong>0.305 seconds<\/strong><\/li>\n<li>File size (32 million rows, uncompressed): <strong>391.6 MB<\/strong><\/li>\n<\/ul>\n<h2>Installing<\/h2>\n<p>TsTables requires Python 3, Pandas and PyTables (which requires HDF5). It is available from PyPI.<\/p>\n<pre class=\"lang:sh decode:true\">$ pip3 install tstables<\/pre>\n<p>You can also view and download the code on <a title=\"TsTables Github\" href=\"https:\/\/github.com\/afiedler\/tstables\" target=\"_blank\">Github<\/a>.<\/p>\n","image":"http:\/\/andyfiedler.com\/wp-content\/uploads\/2014\/06\/3113965583_a96cbbaea8_o-e1404138873541.jpg","featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 30 Jun 2014 10:36:13 +0000","created_by":1,"updated_at":"Mon, 30 Jun 2014 10:36:23 +0000","updated_by":1,"published_at":"Mon, 30 Jun 2014 10:36:13 +0000","published_by":1},{"id":279,"title":"TsTables, a long overdue replacement for TSDB!","slug":"tstables-a-long-overdue-replacement-for-tsdb","markdown":"\nI just posted my replacement for my old time series database, [check it out on the project page](http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/ \"TsTables \u2013 Store High Frequency Data with PyTables\").\n\n\n","html":"<p>I just posted my replacement for my old time series database, <a title=\"TsTables \u2013 Store High Frequency Data with PyTables\" href=\"http:\/\/andyfiedler.com\/projects\/tstables-store-high-frequency-data-with-pytables\/\">check it out on the project page<\/a>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 30 Jun 2014 12:58:47 +0000","created_by":1,"updated_at":"Mon, 30 Jun 2014 13:59:36 +0000","updated_by":1,"published_at":"Mon, 30 Jun 2014 12:58:47 +0000","published_by":1},{"id":282,"title":"How secure is the OAuth2 \"Resource Owner Password Credential\" flow for single-page apps?","slug":"how-secure-is-the-oauth2-resource-owner-password-credential-flow-for-single-page-apps","markdown":"\nI\u2019ve been working on a single-page, browser-based app and I was investigating using the OAuth2 \u201cResource Owner Password Credential\u201d (ROPC) flow to log users in without needing a normal OAuth popup or redirect. The single-page app is written by the same developers as the backend API, so it is more trusted than a third-party application (which should *never* touch a user\u2019s password). However, since it is a client-side application in Javascript, it was unclear to me how to take steps to make this as secure as possible, so I did some research. In this post, I\u2019ll describe what I found.\n\n\n## What the OAuth spec says\n\nThe OAuth spec is a dense monster, but is worth digging into since so many sites are using OAuth today. The [relevant section of the spec](http:\/\/tools.ietf.org\/html\/rfc6749#section-4.3) says that the ROPC flow can be used when the resource owner (the user) \u201chas a trust relationship with the client, such as the device operating system or a highly privileged application\u201d, which would apply to an application developed by the same developers as the API server. The spec also says that it should only be used when other flows are \u201cnot viable\u201d. This isn\u2019t strictly the case for single-page Javascript applications, which can use the [Implicit Grant](http:\/\/tools.ietf.org\/html\/rfc6749#section-1.3.2) flow or the [Authorization Code](http:\/\/tools.ietf.org\/html\/rfc6749#section-1.3.1) flow. However, for clients \u201cowned\u201d by the same owner as the authorization server, the OAuth popup or redirect can be a poor user experience and may confuse users since they wouldn\u2019t expect to \u201cauthorize\u201d an app that they perceive as one and the same as the service itself. So, assuming you trust the client and are willing to consider \u201cbad user experience\u201d as \u201cnot viable\u201d, you could use the ROPC flow for a front-end client.\n\nThe other issue is that Javascript clients cannot disguise their client credentials because the user may just \u201cview source\u201d to retrieve the credentials. This makes [client impersonation](https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-v2-31#section-10.2) possible. It also means the the client is a \u201cpublic\u201d client for the purposes of the OAuth spec, and client authentication is not possible. The OAuth spec states that when client authentication is not possible, the authorization server SHOULD employ other means to validate the client\u2019s identity.\n\n\n## How can we \u201cvalidate the client\u2019s identity\u201d as best as possible with Javascript clients?\n\nFirst, we need to accept that because that this is a public client under control of the user, we\u2019ll have to accept that it is impossible to completely prevent client impersonation. You always could impersonate a client with cURL or a web scraper, which is something that is out of the control of the API owner. To prevent this, we\u2019d need some kind of trusted computing architecture where we are 100% certain that the client credentials are protected from prying eyes.\n\nSince we can\u2019t completely prevent client impersonation, we need to define what types of impersonation we are trying to prevent. For Javascript clients, I want to prevent two types of impersonation:\n\n1. Impersonation by another Javascript client running in a standards-compliant browser on a domain other the official client\u2019s domain\n2. Compromised client Javascript or HTML\n\nBoth types of impersonation are already well-known and have solutions in other Internet standards that we can use for this case.\n\n### Preventing compromised client source code\n\nFor this one, we can simply use SSL for the client\u2019s domain. If the source code has been compromised through a man-in-the-middle attack, the user will see an SSL error in the browser. The OAuth spec already requires that communication to the authorization server\u2019s token and authorization endpoints occur over SSL. It is permitted in the OAuth spec to have a client delivered over HTTP, however.\n\nIn order to use the ROPC grant type for Javascript clients, we need to be more strict than the spec and absolutely ensure that the client is delivered over SSL. If the Javascript client is not delivered over SSL, a middleman could tamper with the client\u2019s Javascript to intercept either the resource owner\u2019s credentials or the access token. This makes it impossible for the resource owner to trust the client, which breaks the first chain of trust between the resource owner and the authorization server.\n\n### Preventing impersonation by other Javascript clients\n\nThe other kind of impersonation we\u2019d like to prevent is another Javascript client (on some other domain) using the official client\u2019s credentials to retrieve access tokens. To do this, we can use the browser\u2019s cross-origin security model.\n\n#### If your client is on the same origin as your authentication server\n\nIf you are running a client on the same origin as the authentication server, requests to the authentication server will be permitted through \u201cnormal\u201d AJAX and I believe that all you will need to do is *not* permit cross-domain requests (i.e. don\u2019t enable CORS) on your authentication server and the ROPC flow will be unavailable to impersonating clients. Here\u2019s why:\n\n- It is possible to submit a form from another domain to kick off the ROPC flow (a POST to your token endpoint), however, it is not possible for Javascript running on that other domain to access the response. This means that the impersonating Javascript may cause your API server to return an access token via a form submission, but it wouldn\u2019t be possible for it to read that token. Since we are not using cookie-based authentication, the client needs to parse the token response for it to become authorized.\n- It is not possible for a third-party (an intermediate proxy) to intercept the token in this way because the browser will be communicating with your server over SSL (you are using SSL for your authentication server, right!?).\n- You need to ensure that potentially-impersonated POSTs to your token endpoint are not in any way destructive. Typically, CSRF attacks (of which this technically is one) lead to a compromise by either setting a cookie that is later used to access a protected resource or cause a POST that takes an abusive action (withdrawing money). You\u2019ll need to ensure that a POST to your token endpoint doesn\u2019t do either of these things.\n\n#### If your client is on a different origin from your authentication server\n\nIf you are running your client on \u201cyourdomain.com\u201d and your API server on \u201capi.yourdomain.com\u201d, you will need to implement CORS anyway. In this case, you should leverage CORS to validate the client. Here\u2019s how you can do it:\n\n- For every ROPC-enabled client, record in your API server\u2019s database the acceptable Javascript origins for that client.\n- When an incoming ROPC grant type comes in, require your client to provide a client ID. Look up that client ID in your database and confirm that the CORS \u201cOrigin\u201d header matches the expected origin. Browsers do not permit Javascript clients to forge the \u201cOrigin\u201d header, making this robust against Javascript client spoofing.\n\n### Additional considerations\n\nSince IE9 and below don\u2019t implement CORS correctly, many sites implement work-arounds such as iframe proxies or Flash-based work-arounds. I haven\u2019t looked into the implications of using these, but they definitely need careful consideration to make sure they are not exploitable.\n\nYou absolutely should implement some kind of rate-limiting on your token endpoint to prevent brute-force attacks.\n\nFinally, you should never issue public clients a refresh token (or any long-lived access token). The reason for this is that, depending on your backend architecture, these could be difficult to revoke should you need to revoke access to a specific client. For example, if you are using a [JSON Web Token](http:\/\/jwt.io) instead of a database record, you would need to blacklist all of them it to revoke them.\n\n\n## Comments welcome!\n\nOAuth2 is still relatively new (as is CORS), so if I\u2019ve missed any ways for this to be exploited, let me know in the comments! Thanks.\n\n\n","html":"<p>I&#8217;ve been working on a single-page, browser-based app and I was investigating using the OAuth2 &#8220;Resource Owner Password Credential&#8221; (ROPC) flow to log users in without needing a normal OAuth popup or redirect. The single-page app is written by the same developers as the backend API, so it is more trusted than a third-party application (which should <em>never<\/em> touch a user&#8217;s password). However, since it is a client-side application in Javascript, it was unclear to me how to take steps to make this as secure as possible, so I did some research. In this post, I&#8217;ll describe what I found.<\/p>\n<h2>What the OAuth spec says<\/h2>\n<p>The OAuth spec is a dense monster, but is worth digging into since so many sites are using OAuth today. The <a href=\"http:\/\/tools.ietf.org\/html\/rfc6749#section-4.3\" target=\"_blank\">relevant section of the spec<\/a> says that the ROPC flow can be used when the resource owner (the user) &#8220;has a trust relationship with the client, such as the device operating system or a highly privileged application&#8221;, which would apply to an application developed by the same developers as the API server. The spec also says that it should only be used when other flows are &#8220;not viable&#8221;. This isn&#8217;t strictly the case for single-page Javascript applications, which can use the <a href=\"http:\/\/tools.ietf.org\/html\/rfc6749#section-1.3.2\" target=\"_blank\">Implicit Grant<\/a> flow or the <a href=\"http:\/\/tools.ietf.org\/html\/rfc6749#section-1.3.1\" target=\"_blank\">Authorization Code<\/a> flow. However, for clients &#8220;owned&#8221; by the same owner as the authorization server, the OAuth popup or redirect can be a poor user experience and may confuse users since they wouldn&#8217;t expect to &#8220;authorize&#8221; an app that they perceive as one and the same as the service itself. So, assuming you trust the client and are willing to consider &#8220;bad user experience&#8221; as &#8220;not viable&#8221;, you could use the ROPC flow for a front-end client.<\/p>\n<p>The other issue is that Javascript clients cannot disguise their client credentials because the user may just &#8220;view source&#8221; to retrieve the credentials. This makes <a href=\"https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-v2-31#section-10.2\" target=\"_blank\">client impersonation<\/a> possible. It also means the the client is a &#8220;public&#8221; client for the purposes of the OAuth spec, and client authentication is not possible. The OAuth spec states that when client authentication is not possible, the authorization server SHOULD employ other means to validate the client&#8217;s identity.<\/p>\n<h2>How can we &#8220;validate the client&#8217;s identity&#8221; as best as possible with Javascript clients?<\/h2>\n<p>First, we need to accept that because that this is a public client under control of the user, we&#8217;ll have to accept that it is impossible to completely prevent client impersonation. You always could impersonate a client with cURL or a web scraper, which is something that is out of the control of the API owner. To prevent this, we&#8217;d need some kind of trusted computing architecture where we are 100% certain that the client credentials are protected from prying eyes.<\/p>\n<p>Since we can&#8217;t completely prevent client impersonation, we need to define what types of impersonation we are trying to prevent. For Javascript clients, I want to prevent two types of impersonation:<\/p>\n<ol>\n<li>Impersonation by another Javascript client running in a standards-compliant browser on a domain other the official client&#8217;s domain<\/li>\n<li>Compromised client Javascript or HTML<\/li>\n<\/ol>\n<p>Both types of impersonation are already well-known and have solutions in other Internet standards that we can use for this case.<\/p>\n<h3>Preventing compromised client source code<\/h3>\n<p>For this one, we can simply use SSL for the client&#8217;s domain. If the source code has been compromised through a man-in-the-middle attack, the user will see an SSL error in the browser. The OAuth spec already requires that communication to the authorization server&#8217;s token and authorization endpoints occur over SSL. It is permitted in the OAuth spec to have a client delivered over HTTP, however.<\/p>\n<p>In order to use the ROPC grant type for Javascript clients, we need to be more strict than the spec and absolutely ensure that the client is delivered over SSL. If the Javascript client is not delivered over SSL, a middleman could tamper with the client&#8217;s Javascript to intercept either the resource owner&#8217;s credentials or the access token. This makes it impossible for the resource owner to trust the client, which breaks the first chain of trust between the resource owner and the authorization server.<\/p>\n<h3>Preventing impersonation by other Javascript clients<\/h3>\n<p>The other kind of impersonation we&#8217;d like to prevent is another Javascript client (on some other domain) using the official client&#8217;s credentials to retrieve access tokens. To do this, we can use the browser&#8217;s cross-origin security model.<\/p>\n<h4>If your client is on the same origin as your authentication server<\/h4>\n<p>If you are running a client on the same origin as the authentication server, requests to the authentication server will be permitted through &#8220;normal&#8221; AJAX and I believe that all you will need to do is <em>not<\/em> permit cross-domain requests (i.e. don&#8217;t enable CORS) on your authentication server and the ROPC flow will be unavailable to impersonating clients. Here&#8217;s why:<\/p>\n<ul>\n<li>It is possible to submit a form from another domain to kick off the ROPC flow (a POST to your token endpoint), however, it is not possible for Javascript running on that other domain to access the response. This means that the impersonating Javascript may cause your API server to return an access token via a form submission, but it wouldn&#8217;t be possible for it to read that token. Since we are not using cookie-based authentication, the client needs to parse the token response for it to become authorized.<\/li>\n<li>It is not possible for a third-party (an intermediate proxy) to intercept the token in this way because the browser will be communicating with your server over SSL (you are using SSL for your authentication server, right!?).<\/li>\n<li>You need to ensure that potentially-impersonated POSTs to your token endpoint are not in any way destructive. Typically, CSRF attacks (of which this technically is one) lead to a compromise by either setting a cookie that is later used to access a protected resource or cause a POST that takes an abusive action (withdrawing money). You&#8217;ll need to ensure that a POST to your token endpoint doesn&#8217;t do either of these things.<\/li>\n<\/ul>\n<h4>If your client is on a different origin from your authentication server<\/h4>\n<p>If you are running your client on &#8220;yourdomain.com&#8221; and your API server on &#8220;api.yourdomain.com&#8221;, you will need to implement CORS anyway. In this case, you should leverage CORS to validate the client. Here&#8217;s how you can do it:<\/p>\n<ul>\n<li>For every ROPC-enabled client, record in your API server&#8217;s database the acceptable Javascript origins for that client.<\/li>\n<li>When an incoming ROPC grant type comes in, require your client to provide a client ID. Look up that client ID in your database and confirm that the CORS &#8220;Origin&#8221; header matches the expected origin. Browsers do not permit Javascript clients to forge the &#8220;Origin&#8221; header, making this robust against Javascript client spoofing.<\/li>\n<\/ul>\n<h3>Additional considerations<\/h3>\n<p>Since IE9 and below don&#8217;t implement CORS correctly, many sites implement work-arounds such as iframe proxies or Flash-based work-arounds. I haven&#8217;t looked into the implications of using these, but they definitely need careful consideration to make sure they are not exploitable.<\/p>\n<p>You absolutely should implement some kind of rate-limiting on your token endpoint to prevent brute-force attacks.<\/p>\n<p>Finally, you should never issue public clients a refresh token (or any long-lived access token). The reason for this is that, depending on your backend architecture, these could be difficult to revoke should you need to revoke access to a specific client. For example, if you are using a <a href=\"http:\/\/jwt.io\">JSON Web Token<\/a> instead of a database record, you would need to blacklist all of them it to revoke them.<\/p>\n<h2>Comments welcome!<\/h2>\n<p>OAuth2 is still relatively new (as is CORS), so if I&#8217;ve missed any ways for this to be exploited, let me know in the comments! Thanks.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 08 Sep 2014 15:07:47 +0000","created_by":1,"updated_at":"Wed, 10 Sep 2014 14:52:33 +0000","updated_by":1,"published_at":"Mon, 08 Sep 2014 15:07:47 +0000","published_by":1},{"id":283,"title":"Using the Draft OAuth Assertion Grant with Google+","slug":"using-the-draft-oauth-assertion-grant-with-google","markdown":"\nThe IETF has been working on a [new OAuth standard for \u201cassertions\u201d](https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-assertions-17) which enables OAuth to work with other types of authentication systems. This can be used to allow users to authenticate with your API through Google+ or other third-party identity providers.\n\nFor example, let\u2019s say you are developing a single-page Javascript app or a mobile app that uses both Google\u2019s APIs as well as your own APIs. You\u2019d like to have users authenticate with Google to obtain access to Google\u2019s APIs, but then you\u2019d also like your app to authenticate with your server to gain access to some additional resources. You\u2019d like to not reinvent the wheel and use OAuth for your own API. You also implicitly trust Google to verify the user\u2019s identity, so you don\u2019t want the user to need to go through another OAuth flow just to use your API.\n\nAssertion grants allow you to do this in a standards-compliant way. This is a draft standard that was just submitted in July of 2014, but for this simple use-case, it is already fairly usable.\n\n\n## How Google+ handles sign in for \u201ccombination\u201d apps (with both a client and a server)\n\nGoogle has [some great documentation on how to authenticate both a client and a server](https:\/\/developers.google.com\/+\/web\/signin\/server-side-flow#step_6_send_the_authorization_code_to_the_server), which is worth reading if you plan on implementing this. The gist of it is that first the client authenticates with Google through a OAuth popup or redirect. This gives the client both an access token and an access code. The code is then passed to the server to authenticate the backend.\n\nThis \u201cpassing the code to the backend step\u201d is what OAuth assertion grants enable in a standards-compliant way.\n\n\n## OAuth Assertion Grants\n\nThe IETF Assertion Grant spec defines a way to define new grant types that are assertions of identity from third parties. An assertion grant looks like this (from the [example in the spec](https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-assertions-17#section-4.1)):\n\n{ \"grant_type\": \"urn:ietf:params:oauth:grant-type:saml2-bearer\", \"assertion\": \"PHNhbWxwOl...[omitted for brevity]...ZT4\", \"scope\": \"\" }\n\nAssertions are very similar to [Resource Owner Password Credential grants](http:\/\/tools.ietf.org\/html\/rfc6749#section-4.3) in that they are passed as HTTP POSTs directly to the `\/token` endpoint. The \u201cgrant_type\u201d for an assertion must be a absolute URI that defines the assertion type, the \u201cassertion\u201d is a Base64-encoded string (using URL-safe encoding) that contains the actual asserrtion, and the \u201cscope\u201d is the same as other OAuth grant types.\n\n\n## An OAuth Assertion Grant for Google+\n\nSince Google has not defined an assertion grant format for Google+ identity, I\u2019ve decided to make one up! You can feel free to steal this format for your own apps.\n\n{ \"grant_type\": \"urn:googlepluscode\", \"assertion\": \"(see below)\", \"scope\": \"(specific to your app)\" }\n\nFor my Google+ assertion grant, I\u2019ve just chose \u201curn:googlepluscode\u201d as the URL. This is arbitrary, but Google would need to standardize this so we currently don\u2019t have a better option. For the assertion itself, I use a Base64-encoded, url-safe version of this JSON:\n\n{ \"code\": \"(access code provided by the front-end when it authenticates with Google)\", \"google_plus_user_id\": \"(Google+ user ID)\" }\n\n\n## Verifying the Google+ assertion grant\n\nWhen the backend receives the Google+ assertion grant, it should do these steps to verify it:\n\n1. Convert the access code into an access token\n2. Call the `\/oauth\/tokeninfo` endpoint with the access token from the previous step\n3. In the response from the `tokeninfo` endpoint, confirm these things: 1. The `user_id` matches the `google_plus_user_id` in the assertion\n2. The `issued_to` from the `tokeninfo` response matches the `client_id` of your application (both the front-end and back-end share the same `client_id`.\n\nStay tuned for a future post on how to implement this with Rails\u00a0and [Doorkeeper](https:\/\/github.com\/doorkeeper-gem\/doorkeeper)!\n\n\n","html":"<p>The IETF has been working on a <a href=\"https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-assertions-17\">new OAuth standard for &#8220;assertions&#8221;<\/a> which enables OAuth to work with other types of authentication systems. This can be used to allow users to authenticate with your API through Google+ or other third-party identity providers.<\/p>\n<p>For example, let&#8217;s say you are developing a single-page Javascript app or a mobile app that uses both Google&#8217;s APIs as well as your own APIs. You&#8217;d like to have users authenticate with Google to obtain access to Google&#8217;s APIs, but then you&#8217;d also like your app to authenticate with your server to gain access to some additional resources. You&#8217;d like to not reinvent the wheel and use OAuth for your own API. You also implicitly trust Google to verify the user&#8217;s identity, so you don&#8217;t want the user to need to go through another OAuth flow just to use your API.<\/p>\n<p>Assertion grants allow you to do this in a standards-compliant way. This is a draft standard that was just submitted in July of 2014, but for this simple use-case, it is already fairly usable.<\/p>\n<h2>How Google+ handles sign in for &#8220;combination&#8221; apps (with both a client and a server)<\/h2>\n<p>Google has <a href=\"https:\/\/developers.google.com\/+\/web\/signin\/server-side-flow#step_6_send_the_authorization_code_to_the_server\">some great documentation on how to authenticate both a client and a server<\/a>, which is worth reading if you plan on implementing this. The gist of it is that first the client authenticates with Google through a OAuth popup or redirect. This gives the client both an access token and an access code. The code is then passed to the server to authenticate the backend.<\/p>\n<p>This &#8220;passing the code to the backend step&#8221; is what OAuth assertion grants enable in a standards-compliant way.<\/p>\n<h2>OAuth Assertion Grants<\/h2>\n<p>The IETF Assertion Grant spec defines a way to define new grant types that are assertions of identity from third parties. An assertion grant looks like this (from the <a href=\"https:\/\/tools.ietf.org\/html\/draft-ietf-oauth-assertions-17#section-4.1\">example in the spec<\/a>):<\/p>\n<pre class=\"lang:js decode:true\">{\r\n   \"grant_type\": \"urn:ietf:params:oauth:grant-type:saml2-bearer\",\r\n   \"assertion\": \"PHNhbWxwOl...[omitted for brevity]...ZT4\",\r\n   \"scope\": \"\"\r\n}<\/pre>\n<p>Assertions are very similar to <a href=\"http:\/\/tools.ietf.org\/html\/rfc6749#section-4.3\">Resource Owner Password Credential grants<\/a> in that they are passed as HTTP POSTs directly to the <code>\/token<\/code> endpoint. The &#8220;grant_type&#8221; for an assertion must be a absolute URI that defines the assertion type, the &#8220;assertion&#8221; is a Base64-encoded string (using URL-safe encoding) that contains the actual asserrtion, and the &#8220;scope&#8221; is the same as other OAuth grant types.<\/p>\n<h2>An OAuth Assertion Grant for Google+<\/h2>\n<p>Since Google has not defined an assertion grant format for Google+ identity, I&#8217;ve decided to make one up! You can feel free to steal this format for your own apps.<\/p>\n<pre class=\"lang:js decode:true\">{\r\n   \"grant_type\": \"urn:googlepluscode\",\r\n   \"assertion\": \"(see below)\",\r\n   \"scope\": \"(specific to your app)\"\r\n}<\/pre>\n<p>For my Google+ assertion grant, I&#8217;ve just chose &#8220;urn:googlepluscode&#8221; as the URL. This is arbitrary, but Google would need to standardize this so we currently don&#8217;t have a better option. For the assertion itself, I use a Base64-encoded, url-safe version of this JSON:<\/p>\n<pre lanng:js=\"\" decode:true=\"\">{\r\n   \"code\": \"(access code provided by the front-end when it authenticates with Google)\",\r\n   \"google_plus_user_id\": \"(Google+ user ID)\"\r\n}<\/pre>\n<h2>Verifying the Google+ assertion grant<\/h2>\n<p>When the backend receives the Google+ assertion grant, it should do these steps to verify it:<\/p>\n<ol>\n<li>Convert the access code into an access token<\/li>\n<li>Call the <code>\/oauth\/tokeninfo<\/code> endpoint with the access token from the previous step<\/li>\n<li>In the response from the <code>tokeninfo<\/code> endpoint, confirm these things:\n<ol>\n<li>The <code>user_id<\/code> matches the <code>google_plus_user_id<\/code> in the assertion<\/li>\n<li>The <code>issued_to<\/code> from the <code>tokeninfo<\/code> response matches the <code>client_id<\/code> of your application (both the front-end and back-end share the same <code>client_id<\/code>.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Stay tuned for a future post on how to implement this with Rails\u00a0and <a href=\"https:\/\/github.com\/doorkeeper-gem\/doorkeeper\">Doorkeeper<\/a>!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 10 Sep 2014 14:38:35 +0000","created_by":1,"updated_at":"Wed, 10 Sep 2014 14:38:35 +0000","updated_by":1,"published_at":"Wed, 10 Sep 2014 14:38:35 +0000","published_by":1},{"id":286,"title":"Simple Regex to Match Options Symbology Initiative Tickers","slug":"simple-regex-to-match-options-symbology-initiative-tickers","markdown":"\nThis is a simple regular expression to match Options Symbology Initiative (OSI) tickers. It has been tested in C#:\n\nvar ticker = \"AAPL 131101C00470000\"; var OSITicker = new Regex(@\"(.{6})(\\d{2})(0\\d|1[0-2])(0[1-9]|[12]\\d|3[01])(C|P)(\\d{8})\"); var match = OSITicker.Match(ticker); var underlying = match.Groups[1].ToString().Trim(); var expirationDate = new DateTime( Int32.Parse(match.Groups[2].ToString()) + 2000, Int32.Parse(match.Groups[3].ToString()), Int32.Parse(match.Groups[4].ToString())); var isPut = (match.Groups[5].ToString() == \"P\"); var strikePrice = Double.Parse(match.Groups[6].ToString()) \/ 1000;\n\n\u00a0\n\n\n","html":"<p>This is a simple regular expression to match Options Symbology Initiative (OSI) tickers. It has been tested in C#:<\/p>\n<pre class=\"lang:c# decode:true\">var ticker = \"AAPL  131101C00470000\";\r\nvar OSITicker = new Regex(@\"(.{6})(\\d{2})(0\\d|1[0-2])(0[1-9]|[12]\\d|3[01])(C|P)(\\d{8})\");\r\n\r\nvar match = OSITicker.Match(ticker);\r\nvar underlying = match.Groups[1].ToString().Trim();\r\nvar expirationDate = new DateTime(\r\n     Int32.Parse(match.Groups[2].ToString()) + 2000,\r\n     Int32.Parse(match.Groups[3].ToString()),\r\n     Int32.Parse(match.Groups[4].ToString()));\r\nvar isPut = (match.Groups[5].ToString() == \"P\");\r\nvar strikePrice = Double.Parse(match.Groups[6].ToString()) \/ 1000;<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 18 Nov 2014 11:40:04 +0000","created_by":1,"updated_at":"Wed, 19 Nov 2014 16:36:40 +0000","updated_by":1,"published_at":"Tue, 18 Nov 2014 11:40:04 +0000","published_by":1}],"tags":[{"id":26,"name":"big data","slug":"big-data","description":""},{"id":15,"name":"economics","slug":"economics","description":""},{"id":29,"name":"google+","slug":"google","description":""},{"id":13,"name":"government","slug":"government","description":""},{"id":16,"name":"kayaking","slug":"kayaking","description":""},{"id":21,"name":"laos","slug":"laos","description":""},{"id":22,"name":"motorcycling","slug":"motorcycling","description":""},{"id":28,"name":"oauth","slug":"oauth","description":""},{"id":12,"name":"open data","slug":"open-data","description":""},{"id":14,"name":"open street map","slug":"open-street-map","description":""},{"id":23,"name":"postgres","slug":"postgres","description":""},{"id":24,"name":"rails","slug":"rails","description":""},{"id":25,"name":"time series","slug":"time-series","description":""}],"posts_tags":[{"tag_id":9,"post_id":117},{"tag_id":15,"post_id":124},{"tag_id":13,"post_id":124},{"tag_id":16,"post_id":124},{"tag_id":12,"post_id":124},{"tag_id":14,"post_id":124},{"tag_id":21,"post_id":197},{"tag_id":22,"post_id":197},{"tag_id":23,"post_id":265},{"tag_id":24,"post_id":265},{"tag_id":26,"post_id":267},{"tag_id":25,"post_id":267},{"tag_id":28,"post_id":282},{"tag_id":29,"post_id":283},{"tag_id":28,"post_id":283}],"users":[{"id":1,"slug":"afiedler","bio":false,"website":"","created_at":"Wed, 07 Sep 2011 18:22:49 +0000","created_by":1,"email":"andy@andyfiedler.com","name":"afiedler"},{"id":2,"slug":"wpengine","bio":false,"website":"http:\/\/wpengine.com","created_at":"Sat, 25 Jan 2014 08:32:20 +0000","created_by":1,"email":"bitbucket@wpengine.com","name":"wpengine"}]},"meta":{"exported_on":"Wed, 04 Feb 2015 15:28:25 +0000","version":"000"}}
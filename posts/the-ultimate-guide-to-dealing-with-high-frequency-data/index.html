<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="description" content="Andy Fiedler&#x27;s Personal Site"/><title>The Ultimate Guide to Dealing with High Frequency Data</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/588d179d14f543de1f7e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/588d179d14f543de1f7e.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-8495f486508069a84016.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.abffcf18e526b7c0dbcd.js" as="script"/><link rel="preload" href="/_next/static/chunks/f6078781a05fe1bcb0902d23dbbb2662c8d200b3.aa021d425006d69b9da8.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-2f6c726a4c5d3dfd1e8f.js" as="script"/><link rel="preload" href="/_next/static/chunks/0a3463f7b698f32fd7af40216f1292ceb318b3f7.39ce756e4870b72d4174.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/posts/%5Bid%5D-29dccdc32ec4574aab67.js" as="script"/></head><body><div id="__next"><div id="main"><div class="max-w-screen-md mx-auto px-4"><div class="flex items-center justify-between py-6 lg:py-10"><a href="/" class="flex items-center"><p class="font-body font-bold text-2xl text-primary dark:text-white">Andy Fiedler</p></a><div class="flex items-center lg:hidden"><svg width="24" height="15" xmlns="http://www.w3.org/2000/svg" class="fill-current text-primary dark:text-white"><g fill-rule="evenodd"><rect width="24" height="3" rx="1.5"></rect><rect x="8" y="6" width="16" height="3" rx="1.5"></rect><rect x="4" y="12" width="20" height="3" rx="1.5"></rect></g></svg></div><div class="hidden lg:block"><ul class="flex items-center"><li class="mr-6 relative group mb-1"><div class="absolute left-0 bottom-0 w-full transition-all h-0 group-hover:h-2 group-hover:bg-yellow opacity-75 z-20"></div><a href="/posts" class="font-body font-medium text-lg text-primary dark:text-white group-hover:text-green dark:group-hover:text-secondary px-2 z-30 block relative transition-colors">Posts</a></li><li><i class="bx text-3xl text-primary dark:text-white cursor-pointer"></i></li></ul></div><div class="bg-black bg-opacity-80 fixed inset-0 z-20 flex opacity-0 pointer-events-none transition-opacity lg:hidden "><div class="ml-auto bg-blue-600 w-2/3 md:w-1/3 p-4"><svg viewBox="0 0 20 20" width="24" height="24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-white absolute top-0 right-0 mt-4 mr-4"><path d="M15.898,4.045c-0.271-0.272-0.713-0.272-0.986,0l-4.71,4.711L5.493,4.045c-0.272-0.272-0.714-0.272-0.986,0s-0.272,0.714,0,0.986l4.709,4.711l-4.71,4.711c-0.272,0.271-0.272,0.713,0,0.986c0.136,0.136,0.314,0.203,0.492,0.203c0.179,0,0.357-0.067,0.493-0.203l4.711-4.711l4.71,4.711c0.137,0.136,0.314,0.203,0.494,0.203c0.178,0,0.355-0.067,0.492-0.203c0.273-0.273,0.273-0.715,0-0.986l-4.711-4.711l4.711-4.711C16.172,4.759,16.172,4.317,15.898,4.045z"></path></svg><i class="absolute top-0 right-0 mt-4 mr-4"></i><ul class="flex flex-col mt-8"><li class=""><a href="/posts" class="font-body font-medium text-lg text-white px-2 block mb-3">Posts</a></li></ul></div></div></div></div><div class="max-w-screen-md mx-auto px-4"><div class="pt-16 lg:pt-20"><div class="border-b border-grey-lighter pb-8 sm:pb-12"><h2 class="font-body font-semibold text-primary dark:text-white text-3xl sm:text-4xl md:text-5xl block leading-tight">The Ultimate Guide to Dealing with High Frequency Data</h2><div class="flex items-center pt-5 sm:pt-8"><p class="font-body font-light text-primary dark:text-white pr-2">2014-05-07 09:10:24 -0400</p></div></div><article class="border-b border-grey-lighter py-8 sm:py-12 prose prose dark:prose-dark max-w-none"><p>I have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!</p>
<p>I'm currently working on a project to replace the <a title="Time Series Database" href="http://andyfiedler.com/projects/time-series-database/">Time Series Database</a> (TSDB) that I wrote a few years ago. By re-writing it, I'm learning a lot about what works and what doesn't when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I've developed while working on this project.</p>
<h2>Partition your data!</h2>
The biggest issue with this much time series data is that you simply cannot index the timestamp column with any normal indexing scheme. In DB-speak, the timestamp column will be a <a title="Cardinality (Wikipedia)" href="http://en.wikipedia.org/wiki/Cardinality_(SQL_statements)" target="_blank">"high cardinality"</a> column, which means it does not lend itself well to indexing. This is a problem because most queries on this kind of high frequency data are to fetch a subset by timestamp, and you do NOT want to make a table scan of a billion plus records to find a few minutes of data.
<p>TSDB attempts to fix this problem by keeping rows in order and creating a <a title="Dense versus Sparse Indexes" href="http://www.cs.sfu.ca/CourseCentral/354/zaiane/material/notes/Chapter11/node5.html" target="_blank">sparse index</a> (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I'm not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.</p>
<p>Another approach is to <em>partition</em> your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union queries across tables.</p>
<p>Partitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I'm working on will use <a title="PyTables" href="http://pytables.github.io" target="_blank">PyTables</a> to partition time series data by date.</p>
<h2>Store timestamps in UTC</h2>
Most of the time, your source data will have timestamps in UTC. If it doesn't, I suggest you convert to UTC before storing. Most libraries use either UTC or local time internally, and because you can never be sure what time zone your users will be in, using UTC is the least common denominator.
<p>UTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.</p>
<h2>Store timestamps as integers, even if your programming language uses floats</h2>
MATLAB, Excel, and R all store timestamps internally as floats by default. This gives their timestamp types a large range and high precision, but I don't think it is appropriate for archiving time series data. Why? Floats are imprecise. You do not know with any accuracy the number of significant digits when using a float, and you cannot make comparisons without worrying about round off errors. Admittedly, even with microsecond data, these systems that use a 64-bit double and 1970-01-01 as the epoch will not loose precision until <span style="color: #000000;">2242-03-16, but why worry about it? I recommend a 64-bit integer as the timestamp column. With one tick equaling one millisecond, you have a time range of <span style="color: #252525;">±</span><span style="color: #252525;">292 million years. This is Java's internal representation. With one tick equaling 100 nanoseconds (0.1 microsecond), you have a time range of ±29,227 years, which is what Win32 does. Should be plenty!</span></span>
<h2>Have a solid ETL procedure</h2>
ETL means "extract, transform, load" and is the term for taking data out of one format or system and loading it into another. The step to make sure is solid when you are dealing with high frequency data is the "load" step. Try to make a process where you can revert a botched load automatically. If you don't do this, guaranteed someone or something will screw up an import, and you will be left wading through millions of rows of data to fix it or re-importing everything from your source system.
<p>The most basic way to make the load step revertible is to just make a copy of your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!</p>
</article></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"the-ultimate-guide-to-dealing-with-high-frequency-data","contentHtml":"\u003cp\u003eI have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!\u003c/p\u003e\n\u003cp\u003eI'm currently working on a project to replace the \u003ca title=\"Time Series Database\" href=\"http://andyfiedler.com/projects/time-series-database/\"\u003eTime Series Database\u003c/a\u003e (TSDB) that I wrote a few years ago. By re-writing it, I'm learning a lot about what works and what doesn't when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I've developed while working on this project.\u003c/p\u003e\n\u003ch2\u003ePartition your data!\u003c/h2\u003e\nThe biggest issue with this much time series data is that you simply cannot index the timestamp column with any normal indexing scheme. In DB-speak, the timestamp column will be a \u003ca title=\"Cardinality (Wikipedia)\" href=\"http://en.wikipedia.org/wiki/Cardinality_(SQL_statements)\" target=\"_blank\"\u003e\"high cardinality\"\u003c/a\u003e column, which means it does not lend itself well to indexing. This is a problem because most queries on this kind of high frequency data are to fetch a subset by timestamp, and you do NOT want to make a table scan of a billion plus records to find a few minutes of data.\n\u003cp\u003eTSDB attempts to fix this problem by keeping rows in order and creating a \u003ca title=\"Dense versus Sparse Indexes\" href=\"http://www.cs.sfu.ca/CourseCentral/354/zaiane/material/notes/Chapter11/node5.html\" target=\"_blank\"\u003esparse index\u003c/a\u003e (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I'm not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.\u003c/p\u003e\n\u003cp\u003eAnother approach is to \u003cem\u003epartition\u003c/em\u003e your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union queries across tables.\u003c/p\u003e\n\u003cp\u003ePartitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I'm working on will use \u003ca title=\"PyTables\" href=\"http://pytables.github.io\" target=\"_blank\"\u003ePyTables\u003c/a\u003e to partition time series data by date.\u003c/p\u003e\n\u003ch2\u003eStore timestamps in UTC\u003c/h2\u003e\nMost of the time, your source data will have timestamps in UTC. If it doesn't, I suggest you convert to UTC before storing. Most libraries use either UTC or local time internally, and because you can never be sure what time zone your users will be in, using UTC is the least common denominator.\n\u003cp\u003eUTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.\u003c/p\u003e\n\u003ch2\u003eStore timestamps as integers, even if your programming language uses floats\u003c/h2\u003e\nMATLAB, Excel, and R all store timestamps internally as floats by default. This gives their timestamp types a large range and high precision, but I don't think it is appropriate for archiving time series data. Why? Floats are imprecise. You do not know with any accuracy the number of significant digits when using a float, and you cannot make comparisons without worrying about round off errors. Admittedly, even with microsecond data, these systems that use a 64-bit double and 1970-01-01 as the epoch will not loose precision until \u003cspan style=\"color: #000000;\"\u003e2242-03-16, but why worry about it? I recommend a 64-bit integer as the timestamp column. With one tick equaling one millisecond, you have a time range of \u003cspan style=\"color: #252525;\"\u003e±\u003c/span\u003e\u003cspan style=\"color: #252525;\"\u003e292 million years. This is Java's internal representation. With one tick equaling 100 nanoseconds (0.1 microsecond), you have a time range of ±29,227 years, which is what Win32 does. Should be plenty!\u003c/span\u003e\u003c/span\u003e\n\u003ch2\u003eHave a solid ETL procedure\u003c/h2\u003e\nETL means \"extract, transform, load\" and is the term for taking data out of one format or system and loading it into another. The step to make sure is solid when you are dealing with high frequency data is the \"load\" step. Try to make a process where you can revert a botched load automatically. If you don't do this, guaranteed someone or something will screw up an import, and you will be left wading through millions of rows of data to fix it or re-importing everything from your source system.\n\u003cp\u003eThe most basic way to make the load step revertible is to just make a copy of your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!\u003c/p\u003e\n","layout":"post","status":"publish","published":true,"title":"The Ultimate Guide to Dealing with High Frequency Data","author":{"display_name":"afiedler","login":"afiedler","email":"andy@andyfiedler.com","url":""},"author_login":"afiedler","author_email":"andy@andyfiedler.com","wordpress_id":267,"wordpress_url":"http://andyfiedler.com/?p=267","date":"2014-05-07 09:10:24 -0400","date_gmt":"2014-05-07 13:10:24 -0400","categories":["Tech Notes"],"tags":["time series","big data"]}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"the-ultimate-guide-to-dealing-with-high-frequency-data"},"buildId":"ECKqL40zNVKWw6Vnv9zpb","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-144e5fa6fafab6397d9c.js"></script><script src="/_next/static/chunks/main-8495f486508069a84016.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.abffcf18e526b7c0dbcd.js" async=""></script><script src="/_next/static/chunks/f6078781a05fe1bcb0902d23dbbb2662c8d200b3.aa021d425006d69b9da8.js" async=""></script><script src="/_next/static/chunks/pages/_app-2f6c726a4c5d3dfd1e8f.js" async=""></script><script src="/_next/static/chunks/0a3463f7b698f32fd7af40216f1292ceb318b3f7.39ce756e4870b72d4174.js" async=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-29dccdc32ec4574aab67.js" async=""></script><script src="/_next/static/ECKqL40zNVKWw6Vnv9zpb/_buildManifest.js" async=""></script><script src="/_next/static/ECKqL40zNVKWw6Vnv9zpb/_ssgManifest.js" async=""></script></body></html>